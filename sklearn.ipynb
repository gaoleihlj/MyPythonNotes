{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scikit-learn学习笔记"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn官方网站：\n",
    "http://scikit-learn.org/stable/index.html\n",
    "\n",
    "参考书1：Python 机器学习（PYTHON MACHINE LEARNING）作者：Sebastian Raschka\n",
    "https://github.com/rasbt/python-machine-learning-book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模式识别算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先导入需要的模块，包括数据模块pandas和numpy，sklearn中的数据集datasets，用于导入鸢尾花（iris）数据。并使用sklearn.model_selection模块中的train_test_split方法，分解训练和测试数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "import numpy as np\n",
    "import sklearn as sl\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# -*- coding: utf-16 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn.datasets in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.datasets\n",
      "\n",
      "DESCRIPTION\n",
      "    The :mod:`sklearn.datasets` module includes utilities to load datasets,\n",
      "    including methods to load and fetch popular reference datasets. It also\n",
      "    features some artificial data generators.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _svmlight_format\n",
      "    base\n",
      "    california_housing\n",
      "    covtype\n",
      "    kddcup99\n",
      "    lfw\n",
      "    mlcomp\n",
      "    mldata\n",
      "    olivetti_faces\n",
      "    rcv1\n",
      "    samples_generator\n",
      "    setup\n",
      "    species_distributions\n",
      "    svmlight_format\n",
      "    tests (package)\n",
      "    twenty_newsgroups\n",
      "\n",
      "FUNCTIONS\n",
      "    clear_data_home(data_home=None)\n",
      "        Delete all the content of the data home cache.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        data_home : str | None\n",
      "            The path to scikit-learn data dir.\n",
      "    \n",
      "    dump_svmlight_file(X, y, f, zero_based=True, comment=None, query_id=None, multilabel=False)\n",
      "        Dump the dataset in svmlight / libsvm file format.\n",
      "        \n",
      "        This format is a text-based format, with one sample per line. It does\n",
      "        not store zero valued features hence is suitable for sparse dataset.\n",
      "        \n",
      "        The first element of each line can be used to store a target variable\n",
      "        to predict.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "            Training vectors, where n_samples is the number of samples and\n",
      "            n_features is the number of features.\n",
      "        \n",
      "        y : {array-like, sparse matrix}, shape = [n_samples (, n_labels)]\n",
      "            Target values. Class labels must be an\n",
      "            integer or float, or array-like objects of integer or float for\n",
      "            multilabel classifications.\n",
      "        \n",
      "        f : string or file-like in binary mode\n",
      "            If string, specifies the path that will contain the data.\n",
      "            If file-like, data will be written to f. f should be opened in binary\n",
      "            mode.\n",
      "        \n",
      "        zero_based : boolean, optional\n",
      "            Whether column indices should be written zero-based (True) or one-based\n",
      "            (False).\n",
      "        \n",
      "        comment : string, optional\n",
      "            Comment to insert at the top of the file. This should be either a\n",
      "            Unicode string, which will be encoded as UTF-8, or an ASCII byte\n",
      "            string.\n",
      "            If a comment is given, then it will be preceded by one that identifies\n",
      "            the file as having been dumped by scikit-learn. Note that not all\n",
      "            tools grok comments in SVMlight files.\n",
      "        \n",
      "        query_id : array-like, shape = [n_samples]\n",
      "            Array containing pairwise preference constraints (qid in svmlight\n",
      "            format).\n",
      "        \n",
      "        multilabel : boolean, optional\n",
      "            Samples may have several labels each (see\n",
      "            http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html)\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "               parameter *multilabel* to support multilabel datasets.\n",
      "    \n",
      "    fetch_20newsgroups(data_home=None, subset='train', categories=None, shuffle=True, random_state=42, remove=(), download_if_missing=True)\n",
      "        Load the filenames and data from the 20 newsgroups dataset.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <20newsgroups>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        data_home : optional, default: None\n",
      "            Specify a download and cache folder for the datasets. If None,\n",
      "            all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
      "        \n",
      "        subset : 'train' or 'test', 'all', optional\n",
      "            Select the dataset to load: 'train' for the training set, 'test'\n",
      "            for the test set, 'all' for both, with shuffled ordering.\n",
      "        \n",
      "        categories : None or collection of string or unicode\n",
      "            If None (default), load all the categories.\n",
      "            If not None, list of category names to load (other categories\n",
      "            ignored).\n",
      "        \n",
      "        shuffle : bool, optional\n",
      "            Whether or not to shuffle the data: might be important for models that\n",
      "            make the assumption that the samples are independent and identically\n",
      "            distributed (i.i.d.), such as stochastic gradient descent.\n",
      "        \n",
      "        random_state : numpy random number generator or seed integer\n",
      "            Used to shuffle the dataset.\n",
      "        \n",
      "        remove : tuple\n",
      "            May contain any subset of ('headers', 'footers', 'quotes'). Each of\n",
      "            these are kinds of text that will be detected and removed from the\n",
      "            newsgroup posts, preventing classifiers from overfitting on\n",
      "            metadata.\n",
      "        \n",
      "            'headers' removes newsgroup headers, 'footers' removes blocks at the\n",
      "            ends of posts that look like signatures, and 'quotes' removes lines\n",
      "            that appear to be quoting another post.\n",
      "        \n",
      "            'headers' follows an exact standard; the other filters are not always\n",
      "            correct.\n",
      "        \n",
      "        download_if_missing : optional, True by default\n",
      "            If False, raise an IOError if the data is not locally available\n",
      "            instead of trying to download the data from the source site.\n",
      "    \n",
      "    fetch_20newsgroups_vectorized(subset='train', remove=(), data_home=None, download_if_missing=True)\n",
      "        Load the 20 newsgroups dataset and transform it into tf-idf vectors.\n",
      "        \n",
      "        This is a convenience function; the tf-idf transformation is done using the\n",
      "        default settings for `sklearn.feature_extraction.text.Vectorizer`. For more\n",
      "        advanced usage (stopword filtering, n-gram extraction, etc.), combine\n",
      "        fetch_20newsgroups with a custom `Vectorizer` or `CountVectorizer`.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <20newsgroups>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        subset : 'train' or 'test', 'all', optional\n",
      "            Select the dataset to load: 'train' for the training set, 'test'\n",
      "            for the test set, 'all' for both, with shuffled ordering.\n",
      "        \n",
      "        remove : tuple\n",
      "            May contain any subset of ('headers', 'footers', 'quotes'). Each of\n",
      "            these are kinds of text that will be detected and removed from the\n",
      "            newsgroup posts, preventing classifiers from overfitting on\n",
      "            metadata.\n",
      "        \n",
      "            'headers' removes newsgroup headers, 'footers' removes blocks at the\n",
      "            ends of posts that look like signatures, and 'quotes' removes lines\n",
      "            that appear to be quoting another post.\n",
      "        \n",
      "        data_home : optional, default: None\n",
      "            Specify an download and cache folder for the datasets. If None,\n",
      "            all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
      "        \n",
      "        download_if_missing : optional, True by default\n",
      "            If False, raise an IOError if the data is not locally available\n",
      "            instead of trying to download the data from the source site.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        bunch : Bunch object\n",
      "            bunch.data: sparse matrix, shape [n_samples, n_features]\n",
      "            bunch.target: array, shape [n_samples]\n",
      "            bunch.target_names: list, length [n_classes]\n",
      "    \n",
      "    fetch_california_housing(data_home=None, download_if_missing=True)\n",
      "        Loader for the California housing dataset from StatLib.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <datasets>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        data_home : optional, default: None\n",
      "            Specify another download and cache folder for the datasets. By default\n",
      "            all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
      "        \n",
      "        download_if_missing : optional, True by default\n",
      "            If False, raise a IOError if the data is not locally available\n",
      "            instead of trying to download the data from the source site.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        dataset : dict-like object with the following attributes:\n",
      "        \n",
      "        dataset.data : ndarray, shape [20640, 8]\n",
      "            Each row corresponding to the 8 feature values in order.\n",
      "        \n",
      "        dataset.target : numpy array of shape (20640,)\n",
      "            Each value corresponds to the average house value in units of 100,000.\n",
      "        \n",
      "        dataset.feature_names : array of length 8\n",
      "            Array of ordered feature names used in the dataset.\n",
      "        \n",
      "        dataset.DESCR : string\n",
      "            Description of the California housing dataset.\n",
      "        \n",
      "        Notes\n",
      "        ------\n",
      "        \n",
      "        This dataset consists of 20,640 samples and 9 features.\n",
      "    \n",
      "    fetch_covtype(data_home=None, download_if_missing=True, random_state=None, shuffle=False)\n",
      "        Load the covertype dataset, downloading it if necessary.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <datasets>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        data_home : string, optional\n",
      "            Specify another download and cache folder for the datasets. By default\n",
      "            all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
      "        \n",
      "        download_if_missing : boolean, default=True\n",
      "            If False, raise a IOError if the data is not locally available\n",
      "            instead of trying to download the data from the source site.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, optional (default=None)\n",
      "            Random state for shuffling the dataset.\n",
      "            If int, random_state is the seed used by the random number generator;\n",
      "            If RandomState instance, random_state is the random number generator;\n",
      "            If None, the random number generator is the RandomState instance used\n",
      "            by `np.random`.\n",
      "        \n",
      "        shuffle : bool, default=False\n",
      "            Whether to shuffle dataset.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        dataset : dict-like object with the following attributes:\n",
      "        \n",
      "        dataset.data : numpy array of shape (581012, 54)\n",
      "            Each row corresponds to the 54 features in the dataset.\n",
      "        \n",
      "        dataset.target : numpy array of shape (581012,)\n",
      "            Each value corresponds to one of the 7 forest covertypes with values\n",
      "            ranging between 1 to 7.\n",
      "        \n",
      "        dataset.DESCR : string\n",
      "            Description of the forest covertype dataset.\n",
      "    \n",
      "    fetch_kddcup99(subset=None, data_home=None, shuffle=False, random_state=None, percent10=True, download_if_missing=True)\n",
      "        Load and return the kddcup 99 dataset (classification).\n",
      "        \n",
      "        The KDD Cup '99 dataset was created by processing the tcpdump portions\n",
      "        of the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset,\n",
      "        created by MIT Lincoln Lab [1]. The artificial data was generated using\n",
      "        a closed network and hand-injected attacks to produce a large number of\n",
      "        different types of attack with normal activity in the background.\n",
      "        As the initial goal was to produce a large training set for supervised\n",
      "        learning algorithms, there is a large proportion (80.1%) of abnormal\n",
      "        data which is unrealistic in real world, and inappropriate for unsupervised\n",
      "        anomaly detection which aims at detecting 'abnormal' data, ie\n",
      "        \n",
      "        1) qualitatively different from normal data.\n",
      "        \n",
      "        2) in large minority among the observations.\n",
      "        \n",
      "        We thus transform the KDD Data set into two different data sets: SA and SF.\n",
      "        \n",
      "        - SA is obtained by simply selecting all the normal data, and a small\n",
      "          proportion of abnormal data to gives an anomaly proportion of 1%.\n",
      "        \n",
      "        - SF is obtained as in [2]\n",
      "          by simply picking up the data whose attribute logged_in is positive, thus\n",
      "          focusing on the intrusion attack, which gives a proportion of 0.3% of\n",
      "          attack.\n",
      "        \n",
      "        - http and smtp are two subsets of SF corresponding with third feature\n",
      "          equal to 'http' (resp. to 'smtp')\n",
      "        \n",
      "        \n",
      "        General KDD structure :\n",
      "        \n",
      "        ================      ==========================================\n",
      "        Samples total         4898431\n",
      "        Dimensionality        41\n",
      "        Features              discrete (int) or continuous (float)\n",
      "        Targets               str, 'normal.' or name of the anomaly type\n",
      "        ================      ==========================================\n",
      "        \n",
      "        SA structure :\n",
      "        \n",
      "        ================      ==========================================\n",
      "        Samples total         976158\n",
      "        Dimensionality        41\n",
      "        Features              discrete (int) or continuous (float)\n",
      "        Targets               str, 'normal.' or name of the anomaly type\n",
      "        ================      ==========================================\n",
      "        \n",
      "        SF structure :\n",
      "        \n",
      "        ================      ==========================================\n",
      "        Samples total         699691\n",
      "        Dimensionality        4\n",
      "        Features              discrete (int) or continuous (float)\n",
      "        Targets               str, 'normal.' or name of the anomaly type\n",
      "        ================      ==========================================\n",
      "        \n",
      "        http structure :\n",
      "        \n",
      "        ================      ==========================================\n",
      "        Samples total         619052\n",
      "        Dimensionality        3\n",
      "        Features              discrete (int) or continuous (float)\n",
      "        Targets               str, 'normal.' or name of the anomaly type\n",
      "        ================      ==========================================\n",
      "        \n",
      "        smtp structure :\n",
      "        \n",
      "        ================      ==========================================\n",
      "        Samples total         95373\n",
      "        Dimensionality        3\n",
      "        Features              discrete (int) or continuous (float)\n",
      "        Targets               str, 'normal.' or name of the anomaly type\n",
      "        ================      ==========================================\n",
      "        \n",
      "        .. versionadded:: 0.18\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        subset : None, 'SA', 'SF', 'http', 'smtp'\n",
      "            To return the corresponding classical subsets of kddcup 99.\n",
      "            If None, return the entire kddcup 99 dataset.\n",
      "        \n",
      "        data_home : string, optional\n",
      "            Specify another download and cache folder for the datasets. By default\n",
      "            all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
      "            .. versionadded:: 0.19\n",
      "        \n",
      "        shuffle : bool, default=False\n",
      "            Whether to shuffle dataset.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, optional (default=None)\n",
      "            Random state for shuffling the dataset.\n",
      "            If int, random_state is the seed used by the random number generator;\n",
      "            If RandomState instance, random_state is the random number generator;\n",
      "            If None, the random number generator is the RandomState instance used\n",
      "            by `np.random`.\n",
      "        \n",
      "        percent10 : bool, default=True\n",
      "            Whether to load only 10 percent of the data.\n",
      "        \n",
      "        download_if_missing : bool, default=True\n",
      "            If False, raise a IOError if the data is not locally available\n",
      "            instead of trying to download the data from the source site.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        data : Bunch\n",
      "            Dictionary-like object, the interesting attributes are:\n",
      "            'data', the data to learn and 'target', the regression target for each\n",
      "            sample.\n",
      "        \n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Analysis and Results of the 1999 DARPA Off-Line Intrusion\n",
      "               Detection Evaluation Richard Lippmann, Joshua W. Haines,\n",
      "               David J. Fried, Jonathan Korba, Kumar Das\n",
      "        \n",
      "        .. [2] K. Yamanishi, J.-I. Takeuchi, G. Williams, and P. Milne. Online\n",
      "               unsupervised outlier detection using finite mixtures with\n",
      "               discounting learning algorithms. In Proceedings of the sixth\n",
      "               ACM SIGKDD international conference on Knowledge discovery\n",
      "               and data mining, pages 320-324. ACM Press, 2000.\n",
      "    \n",
      "    fetch_lfw_pairs(subset='train', data_home=None, funneled=True, resize=0.5, color=False, slice_=(slice(70, 195, None), slice(78, 172, None)), download_if_missing=True)\n",
      "        Loader for the Labeled Faces in the Wild (LFW) pairs dataset\n",
      "        \n",
      "        This dataset is a collection of JPEG pictures of famous people\n",
      "        collected on the internet, all details are available on the\n",
      "        official website:\n",
      "        \n",
      "            http://vis-www.cs.umass.edu/lfw/\n",
      "        \n",
      "        Each picture is centered on a single face. Each pixel of each channel\n",
      "        (color in RGB) is encoded by a float in range 0.0 - 1.0.\n",
      "        \n",
      "        The task is called Face Verification: given a pair of two pictures,\n",
      "        a binary classifier must predict whether the two images are from\n",
      "        the same person.\n",
      "        \n",
      "        In the official `README.txt`_ this task is described as the\n",
      "        \"Restricted\" task.  As I am not sure as to implement the\n",
      "        \"Unrestricted\" variant correctly, I left it as unsupported for now.\n",
      "        \n",
      "          .. _`README.txt`: http://vis-www.cs.umass.edu/lfw/README.txt\n",
      "        \n",
      "        The original images are 250 x 250 pixels, but the default slice and resize\n",
      "        arguments reduce them to 62 x 47.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <labeled_faces_in_the_wild>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        subset : optional, default: 'train'\n",
      "            Select the dataset to load: 'train' for the development training\n",
      "            set, 'test' for the development test set, and '10_folds' for the\n",
      "            official evaluation set that is meant to be used with a 10-folds\n",
      "            cross validation.\n",
      "        \n",
      "        data_home : optional, default: None\n",
      "            Specify another download and cache folder for the datasets. By\n",
      "            default all scikit-learn data is stored in '~/scikit_learn_data'\n",
      "            subfolders.\n",
      "        \n",
      "        funneled : boolean, optional, default: True\n",
      "            Download and use the funneled variant of the dataset.\n",
      "        \n",
      "        resize : float, optional, default 0.5\n",
      "            Ratio used to resize the each face picture.\n",
      "        \n",
      "        color : boolean, optional, default False\n",
      "            Keep the 3 RGB channels instead of averaging them to a single\n",
      "            gray level channel. If color is True the shape of the data has\n",
      "            one more dimension than the shape with color = False.\n",
      "        \n",
      "        slice_ : optional\n",
      "            Provide a custom 2D slice (height, width) to extract the\n",
      "            'interesting' part of the jpeg files and avoid use statistical\n",
      "            correlation from the background\n",
      "        \n",
      "        download_if_missing : optional, True by default\n",
      "            If False, raise a IOError if the data is not locally available\n",
      "            instead of trying to download the data from the source site.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        The data is returned as a Bunch object with the following attributes:\n",
      "        \n",
      "        data : numpy array of shape (2200, 5828). Shape depends on ``subset``.\n",
      "            Each row corresponds to 2 ravel'd face images of original size 62 x 47\n",
      "            pixels. Changing the ``slice_``, ``resize`` or ``subset`` parameters\n",
      "            will change the shape of the output.\n",
      "        \n",
      "        pairs : numpy array of shape (2200, 2, 62, 47). Shape depends on\n",
      "                ``subset``.\n",
      "            Each row has 2 face images corresponding to same or different person\n",
      "            from the dataset containing 5749 people. Changing the ``slice_``,\n",
      "            ``resize`` or ``subset`` parameters will change the shape of the\n",
      "            output.\n",
      "        \n",
      "        target : numpy array of shape (2200,). Shape depends on ``subset``.\n",
      "            Labels associated to each pair of images. The two label values being\n",
      "            different persons or the same person.\n",
      "        \n",
      "        DESCR : string\n",
      "            Description of the Labeled Faces in the Wild (LFW) dataset.\n",
      "    \n",
      "    fetch_lfw_people(data_home=None, funneled=True, resize=0.5, min_faces_per_person=0, color=False, slice_=(slice(70, 195, None), slice(78, 172, None)), download_if_missing=True)\n",
      "        Loader for the Labeled Faces in the Wild (LFW) people dataset\n",
      "        \n",
      "        This dataset is a collection of JPEG pictures of famous people\n",
      "        collected on the internet, all details are available on the\n",
      "        official website:\n",
      "        \n",
      "            http://vis-www.cs.umass.edu/lfw/\n",
      "        \n",
      "        Each picture is centered on a single face. Each pixel of each channel\n",
      "        (color in RGB) is encoded by a float in range 0.0 - 1.0.\n",
      "        \n",
      "        The task is called Face Recognition (or Identification): given the\n",
      "        picture of a face, find the name of the person given a training set\n",
      "        (gallery).\n",
      "        \n",
      "        The original images are 250 x 250 pixels, but the default slice and resize\n",
      "        arguments reduce them to 62 x 47.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        data_home : optional, default: None\n",
      "            Specify another download and cache folder for the datasets. By default\n",
      "            all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
      "        \n",
      "        funneled : boolean, optional, default: True\n",
      "            Download and use the funneled variant of the dataset.\n",
      "        \n",
      "        resize : float, optional, default 0.5\n",
      "            Ratio used to resize the each face picture.\n",
      "        \n",
      "        min_faces_per_person : int, optional, default None\n",
      "            The extracted dataset will only retain pictures of people that have at\n",
      "            least `min_faces_per_person` different pictures.\n",
      "        \n",
      "        color : boolean, optional, default False\n",
      "            Keep the 3 RGB channels instead of averaging them to a single\n",
      "            gray level channel. If color is True the shape of the data has\n",
      "            one more dimension than the shape with color = False.\n",
      "        \n",
      "        slice_ : optional\n",
      "            Provide a custom 2D slice (height, width) to extract the\n",
      "            'interesting' part of the jpeg files and avoid use statistical\n",
      "            correlation from the background\n",
      "        \n",
      "        download_if_missing : optional, True by default\n",
      "            If False, raise a IOError if the data is not locally available\n",
      "            instead of trying to download the data from the source site.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        dataset : dict-like object with the following attributes:\n",
      "        \n",
      "        dataset.data : numpy array of shape (13233, 2914)\n",
      "            Each row corresponds to a ravelled face image of original size 62 x 47\n",
      "            pixels. Changing the ``slice_`` or resize parameters will change the\n",
      "            shape of the output.\n",
      "        \n",
      "        dataset.images : numpy array of shape (13233, 62, 47)\n",
      "            Each row is a face image corresponding to one of the 5749 people in\n",
      "            the dataset. Changing the ``slice_`` or resize parameters will change\n",
      "            the shape of the output.\n",
      "        \n",
      "        dataset.target : numpy array of shape (13233,)\n",
      "            Labels associated to each face image. Those labels range from 0-5748\n",
      "            and correspond to the person IDs.\n",
      "        \n",
      "        dataset.DESCR : string\n",
      "            Description of the Labeled Faces in the Wild (LFW) dataset.\n",
      "    \n",
      "    fetch_mldata(dataname, target_name='label', data_name='data', transpose_data=True, data_home=None)\n",
      "        Fetch an mldata.org data set\n",
      "        \n",
      "        If the file does not exist yet, it is downloaded from mldata.org .\n",
      "        \n",
      "        mldata.org does not have an enforced convention for storing data or\n",
      "        naming the columns in a data set. The default behavior of this function\n",
      "        works well with the most common cases:\n",
      "        \n",
      "          1) data values are stored in the column 'data', and target values in the\n",
      "             column 'label'\n",
      "          2) alternatively, the first column stores target values, and the second\n",
      "             data values\n",
      "          3) the data array is stored as `n_features x n_samples` , and thus needs\n",
      "             to be transposed to match the `sklearn` standard\n",
      "        \n",
      "        Keyword arguments allow to adapt these defaults to specific data sets\n",
      "        (see parameters `target_name`, `data_name`, `transpose_data`, and\n",
      "        the examples below).\n",
      "        \n",
      "        mldata.org data sets may have multiple columns, which are stored in the\n",
      "        Bunch object with their original name.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        \n",
      "        dataname : str\n",
      "            Name of the data set on mldata.org,\n",
      "            e.g.: \"leukemia\", \"Whistler Daily Snowfall\", etc.\n",
      "            The raw name is automatically converted to a mldata.org URL .\n",
      "        \n",
      "        target_name : optional, default: 'label'\n",
      "            Name or index of the column containing the target values.\n",
      "        \n",
      "        data_name : optional, default: 'data'\n",
      "            Name or index of the column containing the data.\n",
      "        \n",
      "        transpose_data : optional, default: True\n",
      "            If True, transpose the downloaded data array.\n",
      "        \n",
      "        data_home : optional, default: None\n",
      "            Specify another download and cache folder for the data sets. By default\n",
      "            all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        \n",
      "        data : Bunch\n",
      "            Dictionary-like object, the interesting attributes are:\n",
      "            'data', the data to learn, 'target', the classification labels,\n",
      "            'DESCR', the full description of the dataset, and\n",
      "            'COL_NAMES', the original names of the dataset columns.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Load the 'iris' dataset from mldata.org:\n",
      "        \n",
      "        >>> from sklearn.datasets.mldata import fetch_mldata\n",
      "        >>> import tempfile\n",
      "        >>> test_data_home = tempfile.mkdtemp()\n",
      "        \n",
      "        >>> iris = fetch_mldata('iris', data_home=test_data_home)\n",
      "        >>> iris.target.shape\n",
      "        (150,)\n",
      "        >>> iris.data.shape\n",
      "        (150, 4)\n",
      "        \n",
      "        Load the 'leukemia' dataset from mldata.org, which needs to be transposed\n",
      "        to respects the scikit-learn axes convention:\n",
      "        \n",
      "        >>> leuk = fetch_mldata('leukemia', transpose_data=True,\n",
      "        ...                     data_home=test_data_home)\n",
      "        >>> leuk.data.shape\n",
      "        (72, 7129)\n",
      "        \n",
      "        Load an alternative 'iris' dataset, which has different names for the\n",
      "        columns:\n",
      "        \n",
      "        >>> iris2 = fetch_mldata('datasets-UCI iris', target_name=1,\n",
      "        ...                      data_name=0, data_home=test_data_home)\n",
      "        >>> iris3 = fetch_mldata('datasets-UCI iris',\n",
      "        ...                      target_name='class', data_name='double0',\n",
      "        ...                      data_home=test_data_home)\n",
      "        \n",
      "        >>> import shutil\n",
      "        >>> shutil.rmtree(test_data_home)\n",
      "    \n",
      "    fetch_olivetti_faces(data_home=None, shuffle=False, random_state=0, download_if_missing=True)\n",
      "        Loader for the Olivetti faces data-set from AT&T.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <olivetti_faces>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        data_home : optional, default: None\n",
      "            Specify another download and cache folder for the datasets. By default\n",
      "            all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
      "        \n",
      "        shuffle : boolean, optional\n",
      "            If True the order of the dataset is shuffled to avoid having\n",
      "            images of the same person grouped.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, optional (default=0)\n",
      "            If int, random_state is the seed used by the random number generator;\n",
      "            If RandomState instance, random_state is the random number generator;\n",
      "            If None, the random number generator is the RandomState instance used\n",
      "            by `np.random`.\n",
      "        \n",
      "        download_if_missing : optional, True by default\n",
      "            If False, raise a IOError if the data is not locally available\n",
      "            instead of trying to download the data from the source site.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        An object with the following attributes:\n",
      "        \n",
      "        data : numpy array of shape (400, 4096)\n",
      "            Each row corresponds to a ravelled face image of original size\n",
      "            64 x 64 pixels.\n",
      "        \n",
      "        images : numpy array of shape (400, 64, 64)\n",
      "            Each row is a face image corresponding to one of the 40 subjects\n",
      "            of the dataset.\n",
      "        \n",
      "        target : numpy array of shape (400, )\n",
      "            Labels associated to each face image. Those labels are ranging from\n",
      "            0-39 and correspond to the Subject IDs.\n",
      "        \n",
      "        DESCR : string\n",
      "            Description of the modified Olivetti Faces Dataset.\n",
      "        \n",
      "        Notes\n",
      "        ------\n",
      "        \n",
      "        This dataset consists of 10 pictures each of 40 individuals. The original\n",
      "        database was available from (now defunct)\n",
      "        \n",
      "            http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html\n",
      "        \n",
      "        The version retrieved here comes in MATLAB format from the personal\n",
      "        web page of Sam Roweis:\n",
      "        \n",
      "            http://www.cs.nyu.edu/~roweis/\n",
      "    \n",
      "    fetch_rcv1(data_home=None, subset='all', download_if_missing=True, random_state=None, shuffle=False)\n",
      "        Load the RCV1 multilabel dataset, downloading it if necessary.\n",
      "        \n",
      "        Version: RCV1-v2, vectors, full sets, topics multilabels.\n",
      "        \n",
      "        ==============     =====================\n",
      "        Classes                              103\n",
      "        Samples total                     804414\n",
      "        Dimensionality                     47236\n",
      "        Features           real, between 0 and 1\n",
      "        ==============     =====================\n",
      "        \n",
      "        Read more in the :ref:`User Guide <datasets>`.\n",
      "        \n",
      "        .. versionadded:: 0.17\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        data_home : string, optional\n",
      "            Specify another download and cache folder for the datasets. By default\n",
      "            all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
      "        \n",
      "        subset : string, 'train', 'test', or 'all', default='all'\n",
      "            Select the dataset to load: 'train' for the training set\n",
      "            (23149 samples), 'test' for the test set (781265 samples),\n",
      "            'all' for both, with the training samples first if shuffle is False.\n",
      "            This follows the official LYRL2004 chronological split.\n",
      "        \n",
      "        download_if_missing : boolean, default=True\n",
      "            If False, raise a IOError if the data is not locally available\n",
      "            instead of trying to download the data from the source site.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, optional (default=None)\n",
      "            Random state for shuffling the dataset.\n",
      "            If int, random_state is the seed used by the random number generator;\n",
      "            If RandomState instance, random_state is the random number generator;\n",
      "            If None, the random number generator is the RandomState instance used\n",
      "            by `np.random`.\n",
      "        \n",
      "        shuffle : bool, default=False\n",
      "            Whether to shuffle dataset.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        dataset : dict-like object with the following attributes:\n",
      "        \n",
      "        dataset.data : scipy csr array, dtype np.float64, shape (804414, 47236)\n",
      "            The array has 0.16% of non zero values.\n",
      "        \n",
      "        dataset.target : scipy csr array, dtype np.uint8, shape (804414, 103)\n",
      "            Each sample has a value of 1 in its categories, and 0 in others.\n",
      "            The array has 3.15% of non zero values.\n",
      "        \n",
      "        dataset.sample_id : numpy array, dtype np.uint32, shape (804414,)\n",
      "            Identification number of each sample, as ordered in dataset.data.\n",
      "        \n",
      "        dataset.target_names : numpy array, dtype object, length (103)\n",
      "            Names of each target (RCV1 topics), as ordered in dataset.target.\n",
      "        \n",
      "        dataset.DESCR : string\n",
      "            Description of the RCV1 dataset.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Lewis, D. D., Yang, Y., Rose, T. G., & Li, F. (2004). RCV1: A new\n",
      "        benchmark collection for text categorization research. The Journal of\n",
      "        Machine Learning Research, 5, 361-397.\n",
      "    \n",
      "    fetch_species_distributions(data_home=None, download_if_missing=True)\n",
      "        Loader for species distribution dataset from Phillips et. al. (2006)\n",
      "        \n",
      "        Read more in the :ref:`User Guide <datasets>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        data_home : optional, default: None\n",
      "            Specify another download and cache folder for the datasets. By default\n",
      "            all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
      "        \n",
      "        download_if_missing : optional, True by default\n",
      "            If False, raise a IOError if the data is not locally available\n",
      "            instead of trying to download the data from the source site.\n",
      "        \n",
      "        Returns\n",
      "        --------\n",
      "        The data is returned as a Bunch object with the following attributes:\n",
      "        \n",
      "        coverages : array, shape = [14, 1592, 1212]\n",
      "            These represent the 14 features measured at each point of the map grid.\n",
      "            The latitude/longitude values for the grid are discussed below.\n",
      "            Missing data is represented by the value -9999.\n",
      "        \n",
      "        train : record array, shape = (1623,)\n",
      "            The training points for the data.  Each point has three fields:\n",
      "        \n",
      "            - train['species'] is the species name\n",
      "            - train['dd long'] is the longitude, in degrees\n",
      "            - train['dd lat'] is the latitude, in degrees\n",
      "        \n",
      "        test : record array, shape = (619,)\n",
      "            The test points for the data.  Same format as the training data.\n",
      "        \n",
      "        Nx, Ny : integers\n",
      "            The number of longitudes (x) and latitudes (y) in the grid\n",
      "        \n",
      "        x_left_lower_corner, y_left_lower_corner : floats\n",
      "            The (x,y) position of the lower-left corner, in degrees\n",
      "        \n",
      "        grid_size : float\n",
      "            The spacing between points of the grid, in degrees\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        * `\"Maximum entropy modeling of species geographic distributions\"\n",
      "          <http://rob.schapire.net/papers/ecolmod.pdf>`_\n",
      "          S. J. Phillips, R. P. Anderson, R. E. Schapire - Ecological Modelling,\n",
      "          190:231-259, 2006.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        This dataset represents the geographic distribution of species.\n",
      "        The dataset is provided by Phillips et. al. (2006).\n",
      "        \n",
      "        The two species are:\n",
      "        \n",
      "        - `\"Bradypus variegatus\"\n",
      "          <http://www.iucnredlist.org/details/3038/0>`_ ,\n",
      "          the Brown-throated Sloth.\n",
      "        \n",
      "        - `\"Microryzomys minutus\"\n",
      "          <http://www.iucnredlist.org/details/13408/0>`_ ,\n",
      "          also known as the Forest Small Rice Rat, a rodent that lives in Peru,\n",
      "          Colombia, Ecuador, Peru, and Venezuela.\n",
      "        \n",
      "        \n",
      "        * For an example of using this dataset with scikit-learn, see\n",
      "          :ref:`examples/applications/plot_species_distribution_modeling.py\n",
      "          <sphx_glr_auto_examples_applications_plot_species_distribution_modeling.py>`.\n",
      "    \n",
      "    get_data_home(data_home=None)\n",
      "        Return the path of the scikit-learn data dir.\n",
      "        \n",
      "        This folder is used by some large dataset loaders to avoid downloading the\n",
      "        data several times.\n",
      "        \n",
      "        By default the data dir is set to a folder named 'scikit_learn_data' in the\n",
      "        user home folder.\n",
      "        \n",
      "        Alternatively, it can be set by the 'SCIKIT_LEARN_DATA' environment\n",
      "        variable or programmatically by giving an explicit folder path. The '~'\n",
      "        symbol is expanded to the user home folder.\n",
      "        \n",
      "        If the folder does not already exist, it is automatically created.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        data_home : str | None\n",
      "            The path to scikit-learn data dir.\n",
      "    \n",
      "    load_boston(return_X_y=False)\n",
      "        Load and return the boston house-prices dataset (regression).\n",
      "        \n",
      "        ==============     ==============\n",
      "        Samples total                 506\n",
      "        Dimensionality                 13\n",
      "        Features           real, positive\n",
      "        Targets             real 5. - 50.\n",
      "        ==============     ==============\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        return_X_y : boolean, default=False.\n",
      "            If True, returns ``(data, target)`` instead of a Bunch object.\n",
      "            See below for more information about the `data` and `target` object.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        data : Bunch\n",
      "            Dictionary-like object, the interesting attributes are:\n",
      "            'data', the data to learn, 'target', the regression targets,\n",
      "            and 'DESCR', the full description of the dataset.\n",
      "        \n",
      "        (data, target) : tuple if ``return_X_y`` is True\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.datasets import load_boston\n",
      "        >>> boston = load_boston()\n",
      "        >>> print(boston.data.shape)\n",
      "        (506, 13)\n",
      "    \n",
      "    load_breast_cancer(return_X_y=False)\n",
      "        Load and return the breast cancer wisconsin dataset (classification).\n",
      "        \n",
      "        The breast cancer dataset is a classic and very easy binary classification\n",
      "        dataset.\n",
      "        \n",
      "        =================   ==============\n",
      "        Classes                          2\n",
      "        Samples per class    212(M),357(B)\n",
      "        Samples total                  569\n",
      "        Dimensionality                  30\n",
      "        Features            real, positive\n",
      "        =================   ==============\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        return_X_y : boolean, default=False\n",
      "            If True, returns ``(data, target)`` instead of a Bunch object.\n",
      "            See below for more information about the `data` and `target` object.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        data : Bunch\n",
      "            Dictionary-like object, the interesting attributes are:\n",
      "            'data', the data to learn, 'target', the classification labels,\n",
      "            'target_names', the meaning of the labels, 'feature_names', the\n",
      "            meaning of the features, and 'DESCR', the\n",
      "            full description of the dataset.\n",
      "        \n",
      "        (data, target) : tuple if ``return_X_y`` is True\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        The copy of UCI ML Breast Cancer Wisconsin (Diagnostic) dataset is\n",
      "        downloaded from:\n",
      "        https://goo.gl/U2Uwz2\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Let's say you are interested in the samples 10, 50, and 85, and want to\n",
      "        know their class name.\n",
      "        \n",
      "        >>> from sklearn.datasets import load_breast_cancer\n",
      "        >>> data = load_breast_cancer()\n",
      "        >>> data.target[[10, 50, 85]]\n",
      "        array([0, 1, 0])\n",
      "        >>> list(data.target_names)\n",
      "        ['malignant', 'benign']\n",
      "    \n",
      "    load_diabetes(return_X_y=False)\n",
      "        Load and return the diabetes dataset (regression).\n",
      "        \n",
      "        ==============      ==================\n",
      "        Samples total       442\n",
      "        Dimensionality      10\n",
      "        Features            real, -.2 < x < .2\n",
      "        Targets             integer 25 - 346\n",
      "        ==============      ==================\n",
      "        \n",
      "        Read more in the :ref:`User Guide <datasets>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        return_X_y : boolean, default=False.\n",
      "            If True, returns ``(data, target)`` instead of a Bunch object.\n",
      "            See below for more information about the `data` and `target` object.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        data : Bunch\n",
      "            Dictionary-like object, the interesting attributes are:\n",
      "            'data', the data to learn and 'target', the regression target for each\n",
      "            sample.\n",
      "        \n",
      "        (data, target) : tuple if ``return_X_y`` is True\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "    \n",
      "    load_digits(n_class=10, return_X_y=False)\n",
      "        Load and return the digits dataset (classification).\n",
      "        \n",
      "        Each datapoint is a 8x8 image of a digit.\n",
      "        \n",
      "        =================   ==============\n",
      "        Classes                         10\n",
      "        Samples per class             ~180\n",
      "        Samples total                 1797\n",
      "        Dimensionality                  64\n",
      "        Features             integers 0-16\n",
      "        =================   ==============\n",
      "        \n",
      "        Read more in the :ref:`User Guide <datasets>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n_class : integer, between 0 and 10, optional (default=10)\n",
      "            The number of classes to return.\n",
      "        \n",
      "        return_X_y : boolean, default=False.\n",
      "            If True, returns ``(data, target)`` instead of a Bunch object.\n",
      "            See below for more information about the `data` and `target` object.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        data : Bunch\n",
      "            Dictionary-like object, the interesting attributes are:\n",
      "            'data', the data to learn, 'images', the images corresponding\n",
      "            to each sample, 'target', the classification labels for each\n",
      "            sample, 'target_names', the meaning of the labels, and 'DESCR',\n",
      "            the full description of the dataset.\n",
      "        \n",
      "        (data, target) : tuple if ``return_X_y`` is True\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        This is a copy of the test set of the UCI ML hand-written digits datasets\n",
      "        http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        To load the data and visualize the images::\n",
      "        \n",
      "            >>> from sklearn.datasets import load_digits\n",
      "            >>> digits = load_digits()\n",
      "            >>> print(digits.data.shape)\n",
      "            (1797, 64)\n",
      "            >>> import matplotlib.pyplot as plt #doctest: +SKIP\n",
      "            >>> plt.gray() #doctest: +SKIP\n",
      "            >>> plt.matshow(digits.images[0]) #doctest: +SKIP\n",
      "            >>> plt.show() #doctest: +SKIP\n",
      "    \n",
      "    load_files(container_path, description=None, categories=None, load_content=True, shuffle=True, encoding=None, decode_error='strict', random_state=0)\n",
      "        Load text files with categories as subfolder names.\n",
      "        \n",
      "        Individual samples are assumed to be files stored a two levels folder\n",
      "        structure such as the following:\n",
      "        \n",
      "            container_folder/\n",
      "                category_1_folder/\n",
      "                    file_1.txt\n",
      "                    file_2.txt\n",
      "                    ...\n",
      "                    file_42.txt\n",
      "                category_2_folder/\n",
      "                    file_43.txt\n",
      "                    file_44.txt\n",
      "                    ...\n",
      "        \n",
      "        The folder names are used as supervised signal label names. The individual\n",
      "        file names are not important.\n",
      "        \n",
      "        This function does not try to extract features into a numpy array or scipy\n",
      "        sparse matrix. In addition, if load_content is false it does not try to\n",
      "        load the files in memory.\n",
      "        \n",
      "        To use text files in a scikit-learn classification or clustering algorithm,\n",
      "        you will need to use the `sklearn.feature_extraction.text` module to build\n",
      "        a feature extraction transformer that suits your problem.\n",
      "        \n",
      "        If you set load_content=True, you should also specify the encoding of the\n",
      "        text using the 'encoding' parameter. For many modern text files, 'utf-8'\n",
      "        will be the correct encoding. If you leave encoding equal to None, then the\n",
      "        content will be made of bytes instead of Unicode, and you will not be able\n",
      "        to use most functions in `sklearn.feature_extraction.text`.\n",
      "        \n",
      "        Similar feature extractors should be built for other kind of unstructured\n",
      "        data input such as images, audio, video, ...\n",
      "        \n",
      "        Read more in the :ref:`User Guide <datasets>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        container_path : string or unicode\n",
      "            Path to the main folder holding one subfolder per category\n",
      "        \n",
      "        description : string or unicode, optional (default=None)\n",
      "            A paragraph describing the characteristic of the dataset: its source,\n",
      "            reference, etc.\n",
      "        \n",
      "        categories : A collection of strings or None, optional (default=None)\n",
      "            If None (default), load all the categories. If not None, list of\n",
      "            category names to load (other categories ignored).\n",
      "        \n",
      "        load_content : boolean, optional (default=True)\n",
      "            Whether to load or not the content of the different files. If true a\n",
      "            'data' attribute containing the text information is present in the data\n",
      "            structure returned. If not, a filenames attribute gives the path to the\n",
      "            files.\n",
      "        \n",
      "        shuffle : bool, optional (default=True)\n",
      "            Whether or not to shuffle the data: might be important for models that\n",
      "            make the assumption that the samples are independent and identically\n",
      "            distributed (i.i.d.), such as stochastic gradient descent.\n",
      "        \n",
      "        encoding : string or None (default is None)\n",
      "            If None, do not try to decode the content of the files (e.g. for images\n",
      "            or other non-text content). If not None, encoding to use to decode text\n",
      "            files to Unicode if load_content is True.\n",
      "        \n",
      "        decode_error : {'strict', 'ignore', 'replace'}, optional\n",
      "            Instruction on what to do if a byte sequence is given to analyze that\n",
      "            contains characters not of the given `encoding`. Passed as keyword\n",
      "            argument 'errors' to bytes.decode.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, optional (default=0)\n",
      "            If int, random_state is the seed used by the random number generator;\n",
      "            If RandomState instance, random_state is the random number generator;\n",
      "            If None, the random number generator is the RandomState instance used\n",
      "            by `np.random`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        data : Bunch\n",
      "            Dictionary-like object, the interesting attributes are: either\n",
      "            data, the raw text data to learn, or 'filenames', the files\n",
      "            holding it, 'target', the classification labels (integer index),\n",
      "            'target_names', the meaning of the labels, and 'DESCR', the full\n",
      "            description of the dataset.\n",
      "    \n",
      "    load_iris(return_X_y=False)\n",
      "        Load and return the iris dataset (classification).\n",
      "        \n",
      "        The iris dataset is a classic and very easy multi-class classification\n",
      "        dataset.\n",
      "        \n",
      "        =================   ==============\n",
      "        Classes                          3\n",
      "        Samples per class               50\n",
      "        Samples total                  150\n",
      "        Dimensionality                   4\n",
      "        Features            real, positive\n",
      "        =================   ==============\n",
      "        \n",
      "        Read more in the :ref:`User Guide <datasets>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        return_X_y : boolean, default=False.\n",
      "            If True, returns ``(data, target)`` instead of a Bunch object. See\n",
      "            below for more information about the `data` and `target` object.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        data : Bunch\n",
      "            Dictionary-like object, the interesting attributes are:\n",
      "            'data', the data to learn, 'target', the classification labels,\n",
      "            'target_names', the meaning of the labels, 'feature_names', the\n",
      "            meaning of the features, and 'DESCR', the\n",
      "            full description of the dataset.\n",
      "        \n",
      "        (data, target) : tuple if ``return_X_y`` is True\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Let's say you are interested in the samples 10, 25, and 50, and want to\n",
      "        know their class name.\n",
      "        \n",
      "        >>> from sklearn.datasets import load_iris\n",
      "        >>> data = load_iris()\n",
      "        >>> data.target[[10, 25, 50]]\n",
      "        array([0, 0, 1])\n",
      "        >>> list(data.target_names)\n",
      "        ['setosa', 'versicolor', 'virginica']\n",
      "    \n",
      "    load_linnerud(return_X_y=False)\n",
      "        Load and return the linnerud dataset (multivariate regression).\n",
      "        \n",
      "        ==============    ============================\n",
      "        Samples total     20\n",
      "        Dimensionality    3 (for both data and target)\n",
      "        Features          integer\n",
      "        Targets           integer\n",
      "        ==============    ============================\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        return_X_y : boolean, default=False.\n",
      "            If True, returns ``(data, target)`` instead of a Bunch object.\n",
      "            See below for more information about the `data` and `target` object.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        data : Bunch\n",
      "            Dictionary-like object, the interesting attributes are: 'data' and\n",
      "            'targets', the two multivariate datasets, with 'data' corresponding to\n",
      "            the exercise and 'targets' corresponding to the physiological\n",
      "            measurements, as well as 'feature_names' and 'target_names'.\n",
      "        \n",
      "        (data, target) : tuple if ``return_X_y`` is True\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "    \n",
      "    load_mlcomp(name_or_id, set_='raw', mlcomp_root=None, **kwargs)\n",
      "        DEPRECATED: since the http://mlcomp.org/ website will shut down in March 2017, the load_mlcomp function was deprecated in version 0.19 and will be removed in 0.21.\n",
      "        \n",
      "        Load a datasets as downloaded from http://mlcomp.org\n",
      "        \n",
      "            Parameters\n",
      "            ----------\n",
      "        \n",
      "            name_or_id : the integer id or the string name metadata of the MLComp\n",
      "                         dataset to load\n",
      "        \n",
      "            set_ : select the portion to load: 'train', 'test' or 'raw'\n",
      "        \n",
      "            mlcomp_root : the filesystem path to the root folder where MLComp datasets\n",
      "                          are stored, if mlcomp_root is None, the MLCOMP_DATASETS_HOME\n",
      "                          environment variable is looked up instead.\n",
      "        \n",
      "            **kwargs : domain specific kwargs to be passed to the dataset loader.\n",
      "        \n",
      "            Read more in the :ref:`User Guide <datasets>`.\n",
      "        \n",
      "            Returns\n",
      "            -------\n",
      "        \n",
      "            data : Bunch\n",
      "                Dictionary-like object, the interesting attributes are:\n",
      "                'filenames', the files holding the raw to learn, 'target', the\n",
      "                classification labels (integer index), 'target_names',\n",
      "                the meaning of the labels, and 'DESCR', the full description of the\n",
      "                dataset.\n",
      "        \n",
      "            Note on the lookup process: depending on the type of name_or_id,\n",
      "            will choose between integer id lookup or metadata name lookup by\n",
      "            looking at the unzipped archives and metadata file.\n",
      "        \n",
      "            TODO: implement zip dataset loading too\n",
      "    \n",
      "    load_sample_image(image_name)\n",
      "        Load the numpy array of a single sample image\n",
      "        \n",
      "        Parameters\n",
      "        -----------\n",
      "        image_name : {`china.jpg`, `flower.jpg`}\n",
      "            The name of the sample image loaded\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        img : 3D array\n",
      "            The image as a numpy array: height x width x color\n",
      "        \n",
      "        Examples\n",
      "        ---------\n",
      "        \n",
      "        >>> from sklearn.datasets import load_sample_image\n",
      "        >>> china = load_sample_image('china.jpg')   # doctest: +SKIP\n",
      "        >>> china.dtype                              # doctest: +SKIP\n",
      "        dtype('uint8')\n",
      "        >>> china.shape                              # doctest: +SKIP\n",
      "        (427, 640, 3)\n",
      "        >>> flower = load_sample_image('flower.jpg') # doctest: +SKIP\n",
      "        >>> flower.dtype                             # doctest: +SKIP\n",
      "        dtype('uint8')\n",
      "        >>> flower.shape                             # doctest: +SKIP\n",
      "        (427, 640, 3)\n",
      "    \n",
      "    load_sample_images()\n",
      "        Load sample images for image manipulation.\n",
      "        \n",
      "        Loads both, ``china`` and ``flower``.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        data : Bunch\n",
      "            Dictionary-like object with the following attributes : 'images', the\n",
      "            two sample images, 'filenames', the file names for the images, and\n",
      "            'DESCR' the full description of the dataset.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        To load the data and visualize the images:\n",
      "        \n",
      "        >>> from sklearn.datasets import load_sample_images\n",
      "        >>> dataset = load_sample_images()     #doctest: +SKIP\n",
      "        >>> len(dataset.images)                #doctest: +SKIP\n",
      "        2\n",
      "        >>> first_img_data = dataset.images[0] #doctest: +SKIP\n",
      "        >>> first_img_data.shape               #doctest: +SKIP\n",
      "        (427, 640, 3)\n",
      "        >>> first_img_data.dtype               #doctest: +SKIP\n",
      "        dtype('uint8')\n",
      "    \n",
      "    load_svmlight_file(f, n_features=None, dtype=<class 'numpy.float64'>, multilabel=False, zero_based='auto', query_id=False, offset=0, length=-1)\n",
      "        Load datasets in the svmlight / libsvm format into sparse CSR matrix\n",
      "        \n",
      "        This format is a text-based format, with one sample per line. It does\n",
      "        not store zero valued features hence is suitable for sparse dataset.\n",
      "        \n",
      "        The first element of each line can be used to store a target variable\n",
      "        to predict.\n",
      "        \n",
      "        This format is used as the default format for both svmlight and the\n",
      "        libsvm command line programs.\n",
      "        \n",
      "        Parsing a text based source can be expensive. When working on\n",
      "        repeatedly on the same dataset, it is recommended to wrap this\n",
      "        loader with joblib.Memory.cache to store a memmapped backup of the\n",
      "        CSR results of the first call and benefit from the near instantaneous\n",
      "        loading of memmapped structures for the subsequent calls.\n",
      "        \n",
      "        In case the file contains a pairwise preference constraint (known\n",
      "        as \"qid\" in the svmlight format) these are ignored unless the\n",
      "        query_id parameter is set to True. These pairwise preference\n",
      "        constraints can be used to constraint the combination of samples\n",
      "        when using pairwise loss functions (as is the case in some\n",
      "        learning to rank problems) so that only pairs with the same\n",
      "        query_id value are considered.\n",
      "        \n",
      "        This implementation is written in Cython and is reasonably fast.\n",
      "        However, a faster API-compatible loader is also available at:\n",
      "        \n",
      "          https://github.com/mblondel/svmlight-loader\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : {str, file-like, int}\n",
      "            (Path to) a file to load. If a path ends in \".gz\" or \".bz2\", it will\n",
      "            be uncompressed on the fly. If an integer is passed, it is assumed to\n",
      "            be a file descriptor. A file-like or file descriptor will not be closed\n",
      "            by this function. A file-like object must be opened in binary mode.\n",
      "        \n",
      "        n_features : int or None\n",
      "            The number of features to use. If None, it will be inferred. This\n",
      "            argument is useful to load several files that are subsets of a\n",
      "            bigger sliced dataset: each subset might not have examples of\n",
      "            every feature, hence the inferred shape might vary from one\n",
      "            slice to another.\n",
      "            n_features is only required if ``offset`` or ``length`` are passed a\n",
      "            non-default value.\n",
      "        \n",
      "        dtype : numpy data type, default np.float64\n",
      "            Data type of dataset to be loaded. This will be the data type of the\n",
      "            output numpy arrays ``X`` and ``y``.\n",
      "        \n",
      "        multilabel : boolean, optional, default False\n",
      "            Samples may have several labels each (see\n",
      "            http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html)\n",
      "        \n",
      "        zero_based : boolean or \"auto\", optional, default \"auto\"\n",
      "            Whether column indices in f are zero-based (True) or one-based\n",
      "            (False). If column indices are one-based, they are transformed to\n",
      "            zero-based to match Python/NumPy conventions.\n",
      "            If set to \"auto\", a heuristic check is applied to determine this from\n",
      "            the file contents. Both kinds of files occur \"in the wild\", but they\n",
      "            are unfortunately not self-identifying. Using \"auto\" or True should\n",
      "            always be safe when no ``offset`` or ``length`` is passed.\n",
      "            If ``offset`` or ``length`` are passed, the \"auto\" mode falls back\n",
      "            to ``zero_based=True`` to avoid having the heuristic check yield\n",
      "            inconsistent results on different segments of the file.\n",
      "        \n",
      "        query_id : boolean, default False\n",
      "            If True, will return the query_id array for each file.\n",
      "        \n",
      "        offset : integer, optional, default 0\n",
      "            Ignore the offset first bytes by seeking forward, then\n",
      "            discarding the following bytes up until the next new line\n",
      "            character.\n",
      "        \n",
      "        length : integer, optional, default -1\n",
      "            If strictly positive, stop reading any new line of data once the\n",
      "            position in the file has reached the (offset + length) bytes threshold.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : scipy.sparse matrix of shape (n_samples, n_features)\n",
      "        \n",
      "        y : ndarray of shape (n_samples,), or, in the multilabel a list of\n",
      "            tuples of length n_samples.\n",
      "        \n",
      "        query_id : array of shape (n_samples,)\n",
      "           query_id for each sample. Only returned when query_id is set to\n",
      "           True.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        load_svmlight_files: similar function for loading multiple files in this\n",
      "        format, enforcing the same number of features/columns on all of them.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        To use joblib.Memory to cache the svmlight file::\n",
      "        \n",
      "            from sklearn.externals.joblib import Memory\n",
      "            from sklearn.datasets import load_svmlight_file\n",
      "            mem = Memory(\"./mycache\")\n",
      "        \n",
      "            @mem.cache\n",
      "            def get_data():\n",
      "                data = load_svmlight_file(\"mysvmlightfile\")\n",
      "                return data[0], data[1]\n",
      "        \n",
      "            X, y = get_data()\n",
      "    \n",
      "    load_svmlight_files(files, n_features=None, dtype=<class 'numpy.float64'>, multilabel=False, zero_based='auto', query_id=False, offset=0, length=-1)\n",
      "        Load dataset from multiple files in SVMlight format\n",
      "        \n",
      "        This function is equivalent to mapping load_svmlight_file over a list of\n",
      "        files, except that the results are concatenated into a single, flat list\n",
      "        and the samples vectors are constrained to all have the same number of\n",
      "        features.\n",
      "        \n",
      "        In case the file contains a pairwise preference constraint (known\n",
      "        as \"qid\" in the svmlight format) these are ignored unless the\n",
      "        query_id parameter is set to True. These pairwise preference\n",
      "        constraints can be used to constraint the combination of samples\n",
      "        when using pairwise loss functions (as is the case in some\n",
      "        learning to rank problems) so that only pairs with the same\n",
      "        query_id value are considered.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        files : iterable over {str, file-like, int}\n",
      "            (Paths of) files to load. If a path ends in \".gz\" or \".bz2\", it will\n",
      "            be uncompressed on the fly. If an integer is passed, it is assumed to\n",
      "            be a file descriptor. File-likes and file descriptors will not be\n",
      "            closed by this function. File-like objects must be opened in binary\n",
      "            mode.\n",
      "        \n",
      "        n_features : int or None\n",
      "            The number of features to use. If None, it will be inferred from the\n",
      "            maximum column index occurring in any of the files.\n",
      "        \n",
      "            This can be set to a higher value than the actual number of features\n",
      "            in any of the input files, but setting it to a lower value will cause\n",
      "            an exception to be raised.\n",
      "        \n",
      "        dtype : numpy data type, default np.float64\n",
      "            Data type of dataset to be loaded. This will be the data type of the\n",
      "            output numpy arrays ``X`` and ``y``.\n",
      "        \n",
      "        multilabel : boolean, optional\n",
      "            Samples may have several labels each (see\n",
      "            http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html)\n",
      "        \n",
      "        zero_based : boolean or \"auto\", optional\n",
      "            Whether column indices in f are zero-based (True) or one-based\n",
      "            (False). If column indices are one-based, they are transformed to\n",
      "            zero-based to match Python/NumPy conventions.\n",
      "            If set to \"auto\", a heuristic check is applied to determine this from\n",
      "            the file contents. Both kinds of files occur \"in the wild\", but they\n",
      "            are unfortunately not self-identifying. Using \"auto\" or True should\n",
      "            always be safe when no offset or length is passed.\n",
      "            If offset or length are passed, the \"auto\" mode falls back\n",
      "            to zero_based=True to avoid having the heuristic check yield\n",
      "            inconsistent results on different segments of the file.\n",
      "        \n",
      "        query_id : boolean, defaults to False\n",
      "            If True, will return the query_id array for each file.\n",
      "        \n",
      "        offset : integer, optional, default 0\n",
      "            Ignore the offset first bytes by seeking forward, then\n",
      "            discarding the following bytes up until the next new line\n",
      "            character.\n",
      "        \n",
      "        length : integer, optional, default -1\n",
      "            If strictly positive, stop reading any new line of data once the\n",
      "            position in the file has reached the (offset + length) bytes threshold.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        [X1, y1, ..., Xn, yn]\n",
      "        where each (Xi, yi) pair is the result from load_svmlight_file(files[i]).\n",
      "        \n",
      "        If query_id is set to True, this will return instead [X1, y1, q1,\n",
      "        ..., Xn, yn, qn] where (Xi, yi, qi) is the result from\n",
      "        load_svmlight_file(files[i])\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When fitting a model to a matrix X_train and evaluating it against a\n",
      "        matrix X_test, it is essential that X_train and X_test have the same\n",
      "        number of features (X_train.shape[1] == X_test.shape[1]). This may not\n",
      "        be the case if you load the files individually with load_svmlight_file.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        load_svmlight_file\n",
      "    \n",
      "    load_wine(return_X_y=False)\n",
      "        Load and return the wine dataset (classification).\n",
      "        \n",
      "        .. versionadded:: 0.18\n",
      "        \n",
      "        The wine dataset is a classic and very easy multi-class classification\n",
      "        dataset.\n",
      "        \n",
      "        =================   ==============\n",
      "        Classes                          3\n",
      "        Samples per class        [59,71,48]\n",
      "        Samples total                  178\n",
      "        Dimensionality                  13\n",
      "        Features            real, positive\n",
      "        =================   ==============\n",
      "        \n",
      "        Read more in the :ref:`User Guide <datasets>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        return_X_y : boolean, default=False.\n",
      "            If True, returns ``(data, target)`` instead of a Bunch object.\n",
      "            See below for more information about the `data` and `target` object.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        data : Bunch\n",
      "            Dictionary-like object, the interesting attributes are: 'data', the\n",
      "            data to learn, 'target', the classification labels, 'target_names', the\n",
      "            meaning of the labels, 'feature_names', the meaning of the features,\n",
      "            and 'DESCR', the full description of the dataset.\n",
      "        \n",
      "        (data, target) : tuple if ``return_X_y`` is True\n",
      "        \n",
      "        The copy of UCI ML Wine Data Set dataset is downloaded and modified to fit\n",
      "        standard format from:\n",
      "        https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Let's say you are interested in the samples 10, 80, and 140, and want to\n",
      "        know their class name.\n",
      "        \n",
      "        >>> from sklearn.datasets import load_wine\n",
      "        >>> data = load_wine()\n",
      "        >>> data.target[[10, 80, 140]]\n",
      "        array([0, 1, 2])\n",
      "        >>> list(data.target_names)\n",
      "        ['class_0', 'class_1', 'class_2']\n",
      "    \n",
      "    make_biclusters(shape, n_clusters, noise=0.0, minval=10, maxval=100, shuffle=True, random_state=None)\n",
      "        Generate an array with constant block diagonal structure for\n",
      "        biclustering.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        shape : iterable (n_rows, n_cols)\n",
      "            The shape of the result.\n",
      "        \n",
      "        n_clusters : integer\n",
      "            The number of biclusters.\n",
      "        \n",
      "        noise : float, optional (default=0.0)\n",
      "            The standard deviation of the gaussian noise.\n",
      "        \n",
      "        minval : int, optional (default=10)\n",
      "            Minimum value of a bicluster.\n",
      "        \n",
      "        maxval : int, optional (default=100)\n",
      "            Maximum value of a bicluster.\n",
      "        \n",
      "        shuffle : boolean, optional (default=True)\n",
      "            Shuffle the samples.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, optional (default=None)\n",
      "            If int, random_state is the seed used by the random number generator;\n",
      "            If RandomState instance, random_state is the random number generator;\n",
      "            If None, the random number generator is the RandomState instance used\n",
      "            by `np.random`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : array of shape `shape`\n",
      "            The generated array.\n",
      "        \n",
      "        rows : array of shape (n_clusters, X.shape[0],)\n",
      "            The indicators for cluster membership of each row.\n",
      "        \n",
      "        cols : array of shape (n_clusters, X.shape[1],)\n",
      "            The indicators for cluster membership of each column.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] Dhillon, I. S. (2001, August). Co-clustering documents and\n",
      "            words using bipartite spectral graph partitioning. In Proceedings\n",
      "            of the seventh ACM SIGKDD international conference on Knowledge\n",
      "            discovery and data mining (pp. 269-274). ACM.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        make_checkerboard\n",
      "    \n",
      "    make_blobs(n_samples=100, n_features=2, centers=3, cluster_std=1.0, center_box=(-10.0, 10.0), shuffle=True, random_state=None)\n",
      "        Generate isotropic Gaussian blobs for clustering.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n_samples : int, optional (default=100)\n",
      "            The total number of points equally divided among clusters.\n",
      "        \n",
      "        n_features : int, optional (default=2)\n",
      "            The number of features for each sample.\n",
      "        \n",
      "        centers : int or array of shape [n_centers, n_features], optional\n",
      "            (default=3)\n",
      "            The number of centers to generate, or the fixed center locations.\n",
      "        \n",
      "        cluster_std : float or sequence of floats, optional (default=1.0)\n",
      "            The standard deviation of the clusters.\n",
      "        \n",
      "        center_box : pair of floats (min, max), optional (default=(-10.0, 10.0))\n",
      "            The bounding box for each cluster center when centers are\n",
      "            generated at random.\n",
      "        \n",
      "        shuffle : boolean, optional (default=True)\n",
      "            Shuffle the samples.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, optional (default=None)\n",
      "            If int, random_state is the seed used by the random number generator;\n",
      "            If RandomState instance, random_state is the random number generator;\n",
      "            If None, the random number generator is the RandomState instance used\n",
      "            by `np.random`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : array of shape [n_samples, n_features]\n",
      "            The generated samples.\n",
      "        \n",
      "        y : array of shape [n_samples]\n",
      "            The integer labels for cluster membership of each sample.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.datasets.samples_generator import make_blobs\n",
      "        >>> X, y = make_blobs(n_samples=10, centers=3, n_features=2,\n",
      "        ...                   random_state=0)\n",
      "        >>> print(X.shape)\n",
      "        (10, 2)\n",
      "        >>> y\n",
      "        array([0, 0, 1, 0, 2, 2, 2, 1, 1, 0])\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        make_classification: a more intricate variant\n",
      "    \n",
      "    make_checkerboard(shape, n_clusters, noise=0.0, minval=10, maxval=100, shuffle=True, random_state=None)\n",
      "        Generate an array with block checkerboard structure for\n",
      "        biclustering.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        shape : iterable (n_rows, n_cols)\n",
      "            The shape of the result.\n",
      "        \n",
      "        n_clusters : integer or iterable (n_row_clusters, n_column_clusters)\n",
      "            The number of row and column clusters.\n",
      "        \n",
      "        noise : float, optional (default=0.0)\n",
      "            The standard deviation of the gaussian noise.\n",
      "        \n",
      "        minval : int, optional (default=10)\n",
      "            Minimum value of a bicluster.\n",
      "        \n",
      "        maxval : int, optional (default=100)\n",
      "            Maximum value of a bicluster.\n",
      "        \n",
      "        shuffle : boolean, optional (default=True)\n",
      "            Shuffle the samples.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, optional (default=None)\n",
      "            If int, random_state is the seed used by the random number generator;\n",
      "            If RandomState instance, random_state is the random number generator;\n",
      "            If None, the random number generator is the RandomState instance used\n",
      "            by `np.random`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : array of shape `shape`\n",
      "            The generated array.\n",
      "        \n",
      "        rows : array of shape (n_clusters, X.shape[0],)\n",
      "            The indicators for cluster membership of each row.\n",
      "        \n",
      "        cols : array of shape (n_clusters, X.shape[1],)\n",
      "            The indicators for cluster membership of each column.\n",
      "        \n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] Kluger, Y., Basri, R., Chang, J. T., & Gerstein, M. (2003).\n",
      "            Spectral biclustering of microarray data: coclustering genes\n",
      "            and conditions. Genome research, 13(4), 703-716.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        make_biclusters\n",
      "    \n",
      "    make_circles(n_samples=100, shuffle=True, noise=None, random_state=None, factor=0.8)\n",
      "        Make a large circle containing a smaller circle in 2d.\n",
      "        \n",
      "        A simple toy dataset to visualize clustering and classification\n",
      "        algorithms.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n_samples : int, optional (default=100)\n",
      "            The total number of points generated.\n",
      "        \n",
      "        shuffle : bool, optional (default=True)\n",
      "            Whether to shuffle the samples.\n",
      "        \n",
      "        noise : double or None (default=None)\n",
      "            Standard deviation of Gaussian noise added to the data.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, optional (default=None)\n",
      "            If int, random_state is the seed used by the random number generator;\n",
      "            If RandomState instance, random_state is the random number generator;\n",
      "            If None, the random number generator is the RandomState instance used\n",
      "            by `np.random`.\n",
      "        \n",
      "        factor : double < 1 (default=.8)\n",
      "            Scale factor between inner and outer circle.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : array of shape [n_samples, 2]\n",
      "            The generated samples.\n",
      "        \n",
      "        y : array of shape [n_samples]\n",
      "            The integer labels (0 or 1) for class membership of each sample.\n",
      "    \n",
      "    make_classification(n_samples=100, n_features=20, n_informative=2, n_redundant=2, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None)\n",
      "        Generate a random n-class classification problem.\n",
      "        \n",
      "        This initially creates clusters of points normally distributed (std=1)\n",
      "        about vertices of an `n_informative`-dimensional hypercube with sides of\n",
      "        length `2*class_sep` and assigns an equal number of clusters to each\n",
      "        class. It introduces interdependence between these features and adds\n",
      "        various types of further noise to the data.\n",
      "        \n",
      "        Prior to shuffling, `X` stacks a number of these primary \"informative\"\n",
      "        features, \"redundant\" linear combinations of these, \"repeated\" duplicates\n",
      "        of sampled features, and arbitrary noise for and remaining features.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n_samples : int, optional (default=100)\n",
      "            The number of samples.\n",
      "        \n",
      "        n_features : int, optional (default=20)\n",
      "            The total number of features. These comprise `n_informative`\n",
      "            informative features, `n_redundant` redundant features, `n_repeated`\n",
      "            duplicated features and `n_features-n_informative-n_redundant-\n",
      "            n_repeated` useless features drawn at random.\n",
      "        \n",
      "        n_informative : int, optional (default=2)\n",
      "            The number of informative features. Each class is composed of a number\n",
      "            of gaussian clusters each located around the vertices of a hypercube\n",
      "            in a subspace of dimension `n_informative`. For each cluster,\n",
      "            informative features are drawn independently from  N(0, 1) and then\n",
      "            randomly linearly combined within each cluster in order to add\n",
      "            covariance. The clusters are then placed on the vertices of the\n",
      "            hypercube.\n",
      "        \n",
      "        n_redundant : int, optional (default=2)\n",
      "            The number of redundant features. These features are generated as\n",
      "            random linear combinations of the informative features.\n",
      "        \n",
      "        n_repeated : int, optional (default=0)\n",
      "            The number of duplicated features, drawn randomly from the informative\n",
      "            and the redundant features.\n",
      "        \n",
      "        n_classes : int, optional (default=2)\n",
      "            The number of classes (or labels) of the classification problem.\n",
      "        \n",
      "        n_clusters_per_class : int, optional (default=2)\n",
      "            The number of clusters per class.\n",
      "        \n",
      "        weights : list of floats or None (default=None)\n",
      "            The proportions of samples assigned to each class. If None, then\n",
      "            classes are balanced. Note that if `len(weights) == n_classes - 1`,\n",
      "            then the last class weight is automatically inferred.\n",
      "            More than `n_samples` samples may be returned if the sum of `weights`\n",
      "            exceeds 1.\n",
      "        \n",
      "        flip_y : float, optional (default=0.01)\n",
      "            The fraction of samples whose class are randomly exchanged. Larger\n",
      "            values introduce noise in the labels and make the classification\n",
      "            task harder.\n",
      "        \n",
      "        class_sep : float, optional (default=1.0)\n",
      "            The factor multiplying the hypercube size.  Larger values spread\n",
      "            out the clusters/classes and make the classification task easier.\n",
      "        \n",
      "        hypercube : boolean, optional (default=True)\n",
      "            If True, the clusters are put on the vertices of a hypercube. If\n",
      "            False, the clusters are put on the vertices of a random polytope.\n",
      "        \n",
      "        shift : float, array of shape [n_features] or None, optional (default=0.0)\n",
      "            Shift features by the specified value. If None, then features\n",
      "            are shifted by a random value drawn in [-class_sep, class_sep].\n",
      "        \n",
      "        scale : float, array of shape [n_features] or None, optional (default=1.0)\n",
      "            Multiply features by the specified value. If None, then features\n",
      "            are scaled by a random value drawn in [1, 100]. Note that scaling\n",
      "            happens after shifting.\n",
      "        \n",
      "        shuffle : boolean, optional (default=True)\n",
      "            Shuffle the samples and the features.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, optional (default=None)\n",
      "            If int, random_state is the seed used by the random number generator;\n",
      "            If RandomState instance, random_state is the random number generator;\n",
      "            If None, the random number generator is the RandomState instance used\n",
      "            by `np.random`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : array of shape [n_samples, n_features]\n",
      "            The generated samples.\n",
      "        \n",
      "        y : array of shape [n_samples]\n",
      "            The integer labels for class membership of each sample.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The algorithm is adapted from Guyon [1] and was designed to generate\n",
      "        the \"Madelon\" dataset.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] I. Guyon, \"Design of experiments for the NIPS 2003 variable\n",
      "               selection benchmark\", 2003.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        make_blobs: simplified variant\n",
      "        make_multilabel_classification: unrelated generator for multilabel tasks\n",
      "    \n",
      "    make_friedman1(n_samples=100, n_features=10, noise=0.0, random_state=None)\n",
      "        Generate the \"Friedman \\#1\" regression problem\n",
      "        \n",
      "        This dataset is described in Friedman [1] and Breiman [2].\n",
      "        \n",
      "        Inputs `X` are independent features uniformly distributed on the interval\n",
      "        [0, 1]. The output `y` is created according to the formula::\n",
      "        \n",
      "            y(X) = 10 * sin(pi * X[:, 0] * X[:, 1]) + 20 * (X[:, 2] - 0.5) ** 2 + 10 * X[:, 3] + 5 * X[:, 4] + noise * N(0, 1).\n",
      "        \n",
      "        Out of the `n_features` features, only 5 are actually used to compute\n",
      "        `y`. The remaining features are independent of `y`.\n",
      "        \n",
      "        The number of features has to be >= 5.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n_samples : int, optional (default=100)\n",
      "            The number of samples.\n",
      "        \n",
      "        n_features : int, optional (default=10)\n",
      "            The number of features. Should be at least 5.\n",
      "        \n",
      "        noise : float, optional (default=0.0)\n",
      "            The standard deviation of the gaussian noise applied to the output.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, optional (default=None)\n",
      "            If int, random_state is the seed used by the random number generator;\n",
      "            If RandomState instance, random_state is the random number generator;\n",
      "            If None, the random number generator is the RandomState instance used\n",
      "            by `np.random`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : array of shape [n_samples, n_features]\n",
      "            The input samples.\n",
      "        \n",
      "        y : array of shape [n_samples]\n",
      "            The output values.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] J. Friedman, \"Multivariate adaptive regression splines\", The Annals\n",
      "               of Statistics 19 (1), pages 1-67, 1991.\n",
      "        \n",
      "        .. [2] L. Breiman, \"Bagging predictors\", Machine Learning 24,\n",
      "               pages 123-140, 1996.\n",
      "    \n",
      "    make_friedman2(n_samples=100, noise=0.0, random_state=None)\n",
      "        Generate the \"Friedman \\#2\" regression problem\n",
      "        \n",
      "        This dataset is described in Friedman [1] and Breiman [2].\n",
      "        \n",
      "        Inputs `X` are 4 independent features uniformly distributed on the\n",
      "        intervals::\n",
      "        \n",
      "            0 <= X[:, 0] <= 100,\n",
      "            40 * pi <= X[:, 1] <= 560 * pi,\n",
      "            0 <= X[:, 2] <= 1,\n",
      "            1 <= X[:, 3] <= 11.\n",
      "        \n",
      "        The output `y` is created according to the formula::\n",
      "        \n",
      "            y(X) = (X[:, 0] ** 2 + (X[:, 1] * X[:, 2]  - 1 / (X[:, 1] * X[:, 3])) ** 2) ** 0.5 + noise * N(0, 1).\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n_samples : int, optional (default=100)\n",
      "            The number of samples.\n",
      "        \n",
      "        noise : float, optional (default=0.0)\n",
      "            The standard deviation of the gaussian noise applied to the output.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, optional (default=None)\n",
      "            If int, random_state is the seed used by the random number generator;\n",
      "            If RandomState instance, random_state is the random number generator;\n",
      "            If None, the random number generator is the RandomState instance used\n",
      "            by `np.random`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : array of shape [n_samples, 4]\n",
      "            The input samples.\n",
      "        \n",
      "        y : array of shape [n_samples]\n",
      "            The output values.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] J. Friedman, \"Multivariate adaptive regression splines\", The Annals\n",
      "               of Statistics 19 (1), pages 1-67, 1991.\n",
      "        \n",
      "        .. [2] L. Breiman, \"Bagging predictors\", Machine Learning 24,\n",
      "               pages 123-140, 1996.\n",
      "    \n",
      "    make_friedman3(n_samples=100, noise=0.0, random_state=None)\n",
      "        Generate the \"Friedman \\#3\" regression problem\n",
      "        \n",
      "        This dataset is described in Friedman [1] and Breiman [2].\n",
      "        \n",
      "        Inputs `X` are 4 independent features uniformly distributed on the\n",
      "        intervals::\n",
      "        \n",
      "            0 <= X[:, 0] <= 100,\n",
      "            40 * pi <= X[:, 1] <= 560 * pi,\n",
      "            0 <= X[:, 2] <= 1,\n",
      "            1 <= X[:, 3] <= 11.\n",
      "        \n",
      "        The output `y` is created according to the formula::\n",
      "        \n",
      "            y(X) = arctan((X[:, 1] * X[:, 2] - 1 / (X[:, 1] * X[:, 3])) / X[:, 0]) + noise * N(0, 1).\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n_samples : int, optional (default=100)\n",
      "            The number of samples.\n",
      "        \n",
      "        noise : float, optional (default=0.0)\n",
      "            The standard deviation of the gaussian noise applied to the output.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, optional (default=None)\n",
      "            If int, random_state is the seed used by the random number generator;\n",
      "            If RandomState instance, random_state is the random number generator;\n",
      "            If None, the random number generator is the RandomState instance used\n",
      "            by `np.random`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : array of shape [n_samples, 4]\n",
      "            The input samples.\n",
      "        \n",
      "        y : array of shape [n_samples]\n",
      "            The output values.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] J. Friedman, \"Multivariate adaptive regression splines\", The Annals\n",
      "               of Statistics 19 (1), pages 1-67, 1991.\n",
      "        \n",
      "        .. [2] L. Breiman, \"Bagging predictors\", Machine Learning 24,\n",
      "               pages 123-140, 1996.\n",
      "    \n",
      "    make_gaussian_quantiles(mean=None, cov=1.0, n_samples=100, n_features=2, n_classes=3, shuffle=True, random_state=None)\n",
      "        Generate isotropic Gaussian and label samples by quantile\n",
      "        \n",
      "        This classification dataset is constructed by taking a multi-dimensional\n",
      "        standard normal distribution and defining classes separated by nested\n",
      "        concentric multi-dimensional spheres such that roughly equal numbers of\n",
      "        samples are in each class (quantiles of the :math:`\\chi^2` distribution).\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        mean : array of shape [n_features], optional (default=None)\n",
      "            The mean of the multi-dimensional normal distribution.\n",
      "            If None then use the origin (0, 0, ...).\n",
      "        \n",
      "        cov : float, optional (default=1.)\n",
      "            The covariance matrix will be this value times the unit matrix. This\n",
      "            dataset only produces symmetric normal distributions.\n",
      "        \n",
      "        n_samples : int, optional (default=100)\n",
      "            The total number of points equally divided among classes.\n",
      "        \n",
      "        n_features : int, optional (default=2)\n",
      "            The number of features for each sample.\n",
      "        \n",
      "        n_classes : int, optional (default=3)\n",
      "            The number of classes\n",
      "        \n",
      "        shuffle : boolean, optional (default=True)\n",
      "            Shuffle the samples.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, optional (default=None)\n",
      "            If int, random_state is the seed used by the random number generator;\n",
      "            If RandomState instance, random_state is the random number generator;\n",
      "            If None, the random number generator is the RandomState instance used\n",
      "            by `np.random`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : array of shape [n_samples, n_features]\n",
      "            The generated samples.\n",
      "        \n",
      "        y : array of shape [n_samples]\n",
      "            The integer labels for quantile membership of each sample.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The dataset is from Zhu et al [1].\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.\n",
      "    \n",
      "    make_hastie_10_2(n_samples=12000, random_state=None)\n",
      "        Generates data for binary classification used in\n",
      "        Hastie et al. 2009, Example 10.2.\n",
      "        \n",
      "        The ten features are standard independent Gaussian and\n",
      "        the target ``y`` is defined by::\n",
      "        \n",
      "          y[i] = 1 if np.sum(X[i] ** 2) > 9.34 else -1\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n_samples : int, optional (default=12000)\n",
      "            The number of samples.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, optional (default=None)\n",
      "            If int, random_state is the seed used by the random number generator;\n",
      "            If RandomState instance, random_state is the random number generator;\n",
      "            If None, the random number generator is the RandomState instance used\n",
      "            by `np.random`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : array of shape [n_samples, 10]\n",
      "            The input samples.\n",
      "        \n",
      "        y : array of shape [n_samples]\n",
      "            The output values.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] T. Hastie, R. Tibshirani and J. Friedman, \"Elements of Statistical\n",
      "               Learning Ed. 2\", Springer, 2009.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        make_gaussian_quantiles: a generalization of this dataset approach\n",
      "    \n",
      "    make_low_rank_matrix(n_samples=100, n_features=100, effective_rank=10, tail_strength=0.5, random_state=None)\n",
      "        Generate a mostly low rank matrix with bell-shaped singular values\n",
      "        \n",
      "        Most of the variance can be explained by a bell-shaped curve of width\n",
      "        effective_rank: the low rank part of the singular values profile is::\n",
      "        \n",
      "            (1 - tail_strength) * exp(-1.0 * (i / effective_rank) ** 2)\n",
      "        \n",
      "        The remaining singular values' tail is fat, decreasing as::\n",
      "        \n",
      "            tail_strength * exp(-0.1 * i / effective_rank).\n",
      "        \n",
      "        The low rank part of the profile can be considered the structured\n",
      "        signal part of the data while the tail can be considered the noisy\n",
      "        part of the data that cannot be summarized by a low number of linear\n",
      "        components (singular vectors).\n",
      "        \n",
      "        This kind of singular profiles is often seen in practice, for instance:\n",
      "         - gray level pictures of faces\n",
      "         - TF-IDF vectors of text documents crawled from the web\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n_samples : int, optional (default=100)\n",
      "            The number of samples.\n",
      "        \n",
      "        n_features : int, optional (default=100)\n",
      "            The number of features.\n",
      "        \n",
      "        effective_rank : int, optional (default=10)\n",
      "            The approximate number of singular vectors required to explain most of\n",
      "            the data by linear combinations.\n",
      "        \n",
      "        tail_strength : float between 0.0 and 1.0, optional (default=0.5)\n",
      "            The relative importance of the fat noisy tail of the singular values\n",
      "            profile.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, optional (default=None)\n",
      "            If int, random_state is the seed used by the random number generator;\n",
      "            If RandomState instance, random_state is the random number generator;\n",
      "            If None, the random number generator is the RandomState instance used\n",
      "            by `np.random`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : array of shape [n_samples, n_features]\n",
      "            The matrix.\n",
      "    \n",
      "    make_moons(n_samples=100, shuffle=True, noise=None, random_state=None)\n",
      "        Make two interleaving half circles\n",
      "        \n",
      "        A simple toy dataset to visualize clustering and classification\n",
      "        algorithms. Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n_samples : int, optional (default=100)\n",
      "            The total number of points generated.\n",
      "        \n",
      "        shuffle : bool, optional (default=True)\n",
      "            Whether to shuffle the samples.\n",
      "        \n",
      "        noise : double or None (default=None)\n",
      "            Standard deviation of Gaussian noise added to the data.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, optional (default=None)\n",
      "            If int, random_state is the seed used by the random number generator;\n",
      "            If RandomState instance, random_state is the random number generator;\n",
      "            If None, the random number generator is the RandomState instance used\n",
      "            by `np.random`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : array of shape [n_samples, 2]\n",
      "            The generated samples.\n",
      "        \n",
      "        y : array of shape [n_samples]\n",
      "            The integer labels (0 or 1) for class membership of each sample.\n",
      "    \n",
      "    make_multilabel_classification(n_samples=100, n_features=20, n_classes=5, n_labels=2, length=50, allow_unlabeled=True, sparse=False, return_indicator='dense', return_distributions=False, random_state=None)\n",
      "        Generate a random multilabel classification problem.\n",
      "        \n",
      "        For each sample, the generative process is:\n",
      "            - pick the number of labels: n ~ Poisson(n_labels)\n",
      "            - n times, choose a class c: c ~ Multinomial(theta)\n",
      "            - pick the document length: k ~ Poisson(length)\n",
      "            - k times, choose a word: w ~ Multinomial(theta_c)\n",
      "        \n",
      "        In the above process, rejection sampling is used to make sure that\n",
      "        n is never zero or more than `n_classes`, and that the document length\n",
      "        is never zero. Likewise, we reject classes which have already been chosen.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n_samples : int, optional (default=100)\n",
      "            The number of samples.\n",
      "        \n",
      "        n_features : int, optional (default=20)\n",
      "            The total number of features.\n",
      "        \n",
      "        n_classes : int, optional (default=5)\n",
      "            The number of classes of the classification problem.\n",
      "        \n",
      "        n_labels : int, optional (default=2)\n",
      "            The average number of labels per instance. More precisely, the number\n",
      "            of labels per sample is drawn from a Poisson distribution with\n",
      "            ``n_labels`` as its expected value, but samples are bounded (using\n",
      "            rejection sampling) by ``n_classes``, and must be nonzero if\n",
      "            ``allow_unlabeled`` is False.\n",
      "        \n",
      "        length : int, optional (default=50)\n",
      "            The sum of the features (number of words if documents) is drawn from\n",
      "            a Poisson distribution with this expected value.\n",
      "        \n",
      "        allow_unlabeled : bool, optional (default=True)\n",
      "            If ``True``, some instances might not belong to any class.\n",
      "        \n",
      "        sparse : bool, optional (default=False)\n",
      "            If ``True``, return a sparse feature matrix\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "               parameter to allow *sparse* output.\n",
      "        \n",
      "        return_indicator : 'dense' (default) | 'sparse' | False\n",
      "            If ``dense`` return ``Y`` in the dense binary indicator format. If\n",
      "            ``'sparse'`` return ``Y`` in the sparse binary indicator format.\n",
      "            ``False`` returns a list of lists of labels.\n",
      "        \n",
      "        return_distributions : bool, optional (default=False)\n",
      "            If ``True``, return the prior class probability and conditional\n",
      "            probabilities of features given classes, from which the data was\n",
      "            drawn.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, optional (default=None)\n",
      "            If int, random_state is the seed used by the random number generator;\n",
      "            If RandomState instance, random_state is the random number generator;\n",
      "            If None, the random number generator is the RandomState instance used\n",
      "            by `np.random`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : array of shape [n_samples, n_features]\n",
      "            The generated samples.\n",
      "        \n",
      "        Y : array or sparse CSR matrix of shape [n_samples, n_classes]\n",
      "            The label sets.\n",
      "        \n",
      "        p_c : array, shape [n_classes]\n",
      "            The probability of each class being drawn. Only returned if\n",
      "            ``return_distributions=True``.\n",
      "        \n",
      "        p_w_c : array, shape [n_features, n_classes]\n",
      "            The probability of each feature being drawn given each class.\n",
      "            Only returned if ``return_distributions=True``.\n",
      "    \n",
      "    make_regression(n_samples=100, n_features=100, n_informative=10, n_targets=1, bias=0.0, effective_rank=None, tail_strength=0.5, noise=0.0, shuffle=True, coef=False, random_state=None)\n",
      "        Generate a random regression problem.\n",
      "        \n",
      "        The input set can either be well conditioned (by default) or have a low\n",
      "        rank-fat tail singular profile. See :func:`make_low_rank_matrix` for\n",
      "        more details.\n",
      "        \n",
      "        The output is generated by applying a (potentially biased) random linear\n",
      "        regression model with `n_informative` nonzero regressors to the previously\n",
      "        generated input and some gaussian centered noise with some adjustable\n",
      "        scale.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n_samples : int, optional (default=100)\n",
      "            The number of samples.\n",
      "        \n",
      "        n_features : int, optional (default=100)\n",
      "            The number of features.\n",
      "        \n",
      "        n_informative : int, optional (default=10)\n",
      "            The number of informative features, i.e., the number of features used\n",
      "            to build the linear model used to generate the output.\n",
      "        \n",
      "        n_targets : int, optional (default=1)\n",
      "            The number of regression targets, i.e., the dimension of the y output\n",
      "            vector associated with a sample. By default, the output is a scalar.\n",
      "        \n",
      "        bias : float, optional (default=0.0)\n",
      "            The bias term in the underlying linear model.\n",
      "        \n",
      "        effective_rank : int or None, optional (default=None)\n",
      "            if not None:\n",
      "                The approximate number of singular vectors required to explain most\n",
      "                of the input data by linear combinations. Using this kind of\n",
      "                singular spectrum in the input allows the generator to reproduce\n",
      "                the correlations often observed in practice.\n",
      "            if None:\n",
      "                The input set is well conditioned, centered and gaussian with\n",
      "                unit variance.\n",
      "        \n",
      "        tail_strength : float between 0.0 and 1.0, optional (default=0.5)\n",
      "            The relative importance of the fat noisy tail of the singular values\n",
      "            profile if `effective_rank` is not None.\n",
      "        \n",
      "        noise : float, optional (default=0.0)\n",
      "            The standard deviation of the gaussian noise applied to the output.\n",
      "        \n",
      "        shuffle : boolean, optional (default=True)\n",
      "            Shuffle the samples and the features.\n",
      "        \n",
      "        coef : boolean, optional (default=False)\n",
      "            If True, the coefficients of the underlying linear model are returned.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, optional (default=None)\n",
      "            If int, random_state is the seed used by the random number generator;\n",
      "            If RandomState instance, random_state is the random number generator;\n",
      "            If None, the random number generator is the RandomState instance used\n",
      "            by `np.random`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : array of shape [n_samples, n_features]\n",
      "            The input samples.\n",
      "        \n",
      "        y : array of shape [n_samples] or [n_samples, n_targets]\n",
      "            The output values.\n",
      "        \n",
      "        coef : array of shape [n_features] or [n_features, n_targets], optional\n",
      "            The coefficient of the underlying linear model. It is returned only if\n",
      "            coef is True.\n",
      "    \n",
      "    make_s_curve(n_samples=100, noise=0.0, random_state=None)\n",
      "        Generate an S curve dataset.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n_samples : int, optional (default=100)\n",
      "            The number of sample points on the S curve.\n",
      "        \n",
      "        noise : float, optional (default=0.0)\n",
      "            The standard deviation of the gaussian noise.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, optional (default=None)\n",
      "            If int, random_state is the seed used by the random number generator;\n",
      "            If RandomState instance, random_state is the random number generator;\n",
      "            If None, the random number generator is the RandomState instance used\n",
      "            by `np.random`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : array of shape [n_samples, 3]\n",
      "            The points.\n",
      "        \n",
      "        t : array of shape [n_samples]\n",
      "            The univariate position of the sample according to the main dimension\n",
      "            of the points in the manifold.\n",
      "    \n",
      "    make_sparse_coded_signal(n_samples, n_components, n_features, n_nonzero_coefs, random_state=None)\n",
      "        Generate a signal as a sparse combination of dictionary elements.\n",
      "        \n",
      "        Returns a matrix Y = DX, such as D is (n_features, n_components),\n",
      "        X is (n_components, n_samples) and each column of X has exactly\n",
      "        n_nonzero_coefs non-zero elements.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n_samples : int\n",
      "            number of samples to generate\n",
      "        \n",
      "        n_components :  int,\n",
      "            number of components in the dictionary\n",
      "        \n",
      "        n_features : int\n",
      "            number of features of the dataset to generate\n",
      "        \n",
      "        n_nonzero_coefs : int\n",
      "            number of active (non-zero) coefficients in each sample\n",
      "        \n",
      "        random_state : int, RandomState instance or None, optional (default=None)\n",
      "            If int, random_state is the seed used by the random number generator;\n",
      "            If RandomState instance, random_state is the random number generator;\n",
      "            If None, the random number generator is the RandomState instance used\n",
      "            by `np.random`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        data : array of shape [n_features, n_samples]\n",
      "            The encoded signal (Y).\n",
      "        \n",
      "        dictionary : array of shape [n_features, n_components]\n",
      "            The dictionary with normalized components (D).\n",
      "        \n",
      "        code : array of shape [n_components, n_samples]\n",
      "            The sparse code such that each column of this matrix has exactly\n",
      "            n_nonzero_coefs non-zero items (X).\n",
      "    \n",
      "    make_sparse_spd_matrix(dim=1, alpha=0.95, norm_diag=False, smallest_coef=0.1, largest_coef=0.9, random_state=None)\n",
      "        Generate a sparse symmetric definite positive matrix.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        dim : integer, optional (default=1)\n",
      "            The size of the random matrix to generate.\n",
      "        \n",
      "        alpha : float between 0 and 1, optional (default=0.95)\n",
      "            The probability that a coefficient is zero (see notes). Larger values\n",
      "            enforce more sparsity.\n",
      "        \n",
      "        norm_diag : boolean, optional (default=False)\n",
      "            Whether to normalize the output matrix to make the leading diagonal\n",
      "            elements all 1\n",
      "        \n",
      "        smallest_coef : float between 0 and 1, optional (default=0.1)\n",
      "            The value of the smallest coefficient.\n",
      "        \n",
      "        largest_coef : float between 0 and 1, optional (default=0.9)\n",
      "            The value of the largest coefficient.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, optional (default=None)\n",
      "            If int, random_state is the seed used by the random number generator;\n",
      "            If RandomState instance, random_state is the random number generator;\n",
      "            If None, the random number generator is the RandomState instance used\n",
      "            by `np.random`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        prec : sparse matrix of shape (dim, dim)\n",
      "            The generated matrix.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The sparsity is actually imposed on the cholesky factor of the matrix.\n",
      "        Thus alpha does not translate directly into the filling fraction of\n",
      "        the matrix itself.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        make_spd_matrix\n",
      "    \n",
      "    make_sparse_uncorrelated(n_samples=100, n_features=10, random_state=None)\n",
      "        Generate a random regression problem with sparse uncorrelated design\n",
      "        \n",
      "        This dataset is described in Celeux et al [1]. as::\n",
      "        \n",
      "            X ~ N(0, 1)\n",
      "            y(X) = X[:, 0] + 2 * X[:, 1] - 2 * X[:, 2] - 1.5 * X[:, 3]\n",
      "        \n",
      "        Only the first 4 features are informative. The remaining features are\n",
      "        useless.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n_samples : int, optional (default=100)\n",
      "            The number of samples.\n",
      "        \n",
      "        n_features : int, optional (default=10)\n",
      "            The number of features.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, optional (default=None)\n",
      "            If int, random_state is the seed used by the random number generator;\n",
      "            If RandomState instance, random_state is the random number generator;\n",
      "            If None, the random number generator is the RandomState instance used\n",
      "            by `np.random`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : array of shape [n_samples, n_features]\n",
      "            The input samples.\n",
      "        \n",
      "        y : array of shape [n_samples]\n",
      "            The output values.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] G. Celeux, M. El Anbari, J.-M. Marin, C. P. Robert,\n",
      "               \"Regularization in regression: comparing Bayesian and frequentist\n",
      "               methods in a poorly informative situation\", 2009.\n",
      "    \n",
      "    make_spd_matrix(n_dim, random_state=None)\n",
      "        Generate a random symmetric, positive-definite matrix.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n_dim : int\n",
      "            The matrix dimension.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, optional (default=None)\n",
      "            If int, random_state is the seed used by the random number generator;\n",
      "            If RandomState instance, random_state is the random number generator;\n",
      "            If None, the random number generator is the RandomState instance used\n",
      "            by `np.random`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : array of shape [n_dim, n_dim]\n",
      "            The random symmetric, positive-definite matrix.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        make_sparse_spd_matrix\n",
      "    \n",
      "    make_swiss_roll(n_samples=100, noise=0.0, random_state=None)\n",
      "        Generate a swiss roll dataset.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n_samples : int, optional (default=100)\n",
      "            The number of sample points on the S curve.\n",
      "        \n",
      "        noise : float, optional (default=0.0)\n",
      "            The standard deviation of the gaussian noise.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, optional (default=None)\n",
      "            If int, random_state is the seed used by the random number generator;\n",
      "            If RandomState instance, random_state is the random number generator;\n",
      "            If None, the random number generator is the RandomState instance used\n",
      "            by `np.random`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : array of shape [n_samples, 3]\n",
      "            The points.\n",
      "        \n",
      "        t : array of shape [n_samples]\n",
      "            The univariate position of the sample according to the main dimension\n",
      "            of the points in the manifold.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The algorithm is from Marsland [1].\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] S. Marsland, \"Machine Learning: An Algorithmic Perspective\",\n",
      "               Chapter 10, 2009.\n",
      "               http://seat.massey.ac.nz/personal/s.r.marsland/Code/10/lle.py\n",
      "    \n",
      "    mldata_filename(dataname)\n",
      "        Convert a raw name for a data set in a mldata.org filename.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        dataname : str\n",
      "            Name of dataset\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        fname : str\n",
      "            The converted dataname.\n",
      "\n",
      "DATA\n",
      "    __all__ = ['clear_data_home', 'dump_svmlight_file', 'fetch_20newsgroup...\n",
      "\n",
      "FILE\n",
      "    /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/datasets/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "help(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train_test_split in module sklearn.model_selection._split:\n",
      "\n",
      "train_test_split(*arrays, **options)\n",
      "    Split arrays or matrices into random train and test subsets\n",
      "    \n",
      "    Quick utility that wraps input validation and\n",
      "    ``next(ShuffleSplit().split(X, y))`` and application to input data\n",
      "    into a single call for splitting (and optionally subsampling) data in a\n",
      "    oneliner.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <cross_validation>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    *arrays : sequence of indexables with same length / shape[0]\n",
      "        Allowed inputs are lists, numpy arrays, scipy-sparse\n",
      "        matrices or pandas dataframes.\n",
      "    \n",
      "    test_size : float, int, None, optional\n",
      "        If float, should be between 0.0 and 1.0 and represent the proportion\n",
      "        of the dataset to include in the test split. If int, represents the\n",
      "        absolute number of test samples. If None, the value is set to the\n",
      "        complement of the train size. By default, the value is set to 0.25.\n",
      "        The default will change in version 0.21. It will remain 0.25 only\n",
      "        if ``train_size`` is unspecified, otherwise it will complement\n",
      "        the specified ``train_size``.\n",
      "    \n",
      "    train_size : float, int, or None, default None\n",
      "        If float, should be between 0.0 and 1.0 and represent the\n",
      "        proportion of the dataset to include in the train split. If\n",
      "        int, represents the absolute number of train samples. If None,\n",
      "        the value is automatically set to the complement of the test size.\n",
      "    \n",
      "    random_state : int, RandomState instance or None, optional (default=None)\n",
      "        If int, random_state is the seed used by the random number generator;\n",
      "        If RandomState instance, random_state is the random number generator;\n",
      "        If None, the random number generator is the RandomState instance used\n",
      "        by `np.random`.\n",
      "    \n",
      "    shuffle : boolean, optional (default=True)\n",
      "        Whether or not to shuffle the data before splitting. If shuffle=False\n",
      "        then stratify must be None.\n",
      "    \n",
      "    stratify : array-like or None (default is None)\n",
      "        If not None, data is split in a stratified fashion, using this as\n",
      "        the class labels.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    splitting : list, length=2 * len(arrays)\n",
      "        List containing train-test split of inputs.\n",
      "    \n",
      "        .. versionadded:: 0.16\n",
      "            If the input is sparse, the output will be a\n",
      "            ``scipy.sparse.csr_matrix``. Else, output type is the same as the\n",
      "            input type.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> import numpy as np\n",
      "    >>> from sklearn.model_selection import train_test_split\n",
      "    >>> X, y = np.arange(10).reshape((5, 2)), range(5)\n",
      "    >>> X\n",
      "    array([[0, 1],\n",
      "           [2, 3],\n",
      "           [4, 5],\n",
      "           [6, 7],\n",
      "           [8, 9]])\n",
      "    >>> list(y)\n",
      "    [0, 1, 2, 3, 4]\n",
      "    \n",
      "    >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "    ...     X, y, test_size=0.33, random_state=42)\n",
      "    ...\n",
      "    >>> X_train\n",
      "    array([[4, 5],\n",
      "           [0, 1],\n",
      "           [6, 7]])\n",
      "    >>> y_train\n",
      "    [2, 0, 3]\n",
      "    >>> X_test\n",
      "    array([[2, 3],\n",
      "           [8, 9]])\n",
      "    >>> y_test\n",
      "    [1, 4]\n",
      "    \n",
      "    >>> train_test_split(y, shuffle=False)\n",
      "    [[0, 1, 2], [3, 4]]\n",
      "\n",
      "(105, 2)\n"
     ]
    }
   ],
   "source": [
    "X = iris.data[:,[2,3]]\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "help(train_test_split)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据标准化过程，从结果上看，其方法是每列数据减均值除以标准差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class StandardScaler in module sklearn.preprocessing.data:\n",
      "\n",
      "class StandardScaler(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin)\n",
      " |  Standardize features by removing the mean and scaling to unit variance\n",
      " |  \n",
      " |  Centering and scaling happen independently on each feature by computing\n",
      " |  the relevant statistics on the samples in the training set. Mean and\n",
      " |  standard deviation are then stored to be used on later data using the\n",
      " |  `transform` method.\n",
      " |  \n",
      " |  Standardization of a dataset is a common requirement for many\n",
      " |  machine learning estimators: they might behave badly if the\n",
      " |  individual feature do not more or less look like standard normally\n",
      " |  distributed data (e.g. Gaussian with 0 mean and unit variance).\n",
      " |  \n",
      " |  For instance many elements used in the objective function of\n",
      " |  a learning algorithm (such as the RBF kernel of Support Vector\n",
      " |  Machines or the L1 and L2 regularizers of linear models) assume that\n",
      " |  all features are centered around 0 and have variance in the same\n",
      " |  order. If a feature has a variance that is orders of magnitude larger\n",
      " |  that others, it might dominate the objective function and make the\n",
      " |  estimator unable to learn from other features correctly as expected.\n",
      " |  \n",
      " |  This scaler can also be applied to sparse CSR or CSC matrices by passing\n",
      " |  `with_mean=False` to avoid breaking the sparsity structure of the data.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  copy : boolean, optional, default True\n",
      " |      If False, try to avoid a copy and do inplace scaling instead.\n",
      " |      This is not guaranteed to always work inplace; e.g. if the data is\n",
      " |      not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n",
      " |      returned.\n",
      " |  \n",
      " |  with_mean : boolean, True by default\n",
      " |      If True, center the data before scaling.\n",
      " |      This does not work (and will raise an exception) when attempted on\n",
      " |      sparse matrices, because centering them entails building a dense\n",
      " |      matrix which in common use cases is likely to be too large to fit in\n",
      " |      memory.\n",
      " |  \n",
      " |  with_std : boolean, True by default\n",
      " |      If True, scale the data to unit variance (or equivalently,\n",
      " |      unit standard deviation).\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  scale_ : ndarray, shape (n_features,)\n",
      " |      Per feature relative scaling of the data.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *scale_*\n",
      " |  \n",
      " |  mean_ : array of floats with shape [n_features]\n",
      " |      The mean value for each feature in the training set.\n",
      " |  \n",
      " |  var_ : array of floats with shape [n_features]\n",
      " |      The variance for each feature in the training set. Used to compute\n",
      " |      `scale_`\n",
      " |  \n",
      " |  n_samples_seen_ : int\n",
      " |      The number of samples processed by the estimator. Will be reset on\n",
      " |      new calls to fit, but increments across ``partial_fit`` calls.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.preprocessing import StandardScaler\n",
      " |  >>>\n",
      " |  >>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n",
      " |  >>> scaler = StandardScaler()\n",
      " |  >>> print(scaler.fit(data))\n",
      " |  StandardScaler(copy=True, with_mean=True, with_std=True)\n",
      " |  >>> print(scaler.mean_)\n",
      " |  [ 0.5  0.5]\n",
      " |  >>> print(scaler.transform(data))\n",
      " |  [[-1. -1.]\n",
      " |   [-1. -1.]\n",
      " |   [ 1.  1.]\n",
      " |   [ 1.  1.]]\n",
      " |  >>> print(scaler.transform([[2, 2]]))\n",
      " |  [[ 3.  3.]]\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  scale: Equivalent function without the estimator API.\n",
      " |  \n",
      " |  :class:`sklearn.decomposition.PCA`\n",
      " |      Further removes the linear correlation across features with 'whiten=True'.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  For a comparison of the different scalers, transformers, and normalizers,\n",
      " |  see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      " |  <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      StandardScaler\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, copy=True, with_mean=True, with_std=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Compute the mean and std to be used for later scaling.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape [n_samples, n_features]\n",
      " |          The data used to compute the mean and standard deviation\n",
      " |          used for later scaling along the features axis.\n",
      " |      \n",
      " |      y : Passthrough for ``Pipeline`` compatibility.\n",
      " |  \n",
      " |  inverse_transform(self, X, copy=None)\n",
      " |      Scale back the data to the original representation\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape [n_samples, n_features]\n",
      " |          The data used to scale along the features axis.\n",
      " |      copy : bool, optional (default: None)\n",
      " |          Copy the input X or not.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_tr : array-like, shape [n_samples, n_features]\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  partial_fit(self, X, y=None)\n",
      " |      Online computation of mean and std on X for later scaling.\n",
      " |      All of X is processed as a single batch. This is intended for cases\n",
      " |      when `fit` is not feasible due to very large number of `n_samples`\n",
      " |      or because X is read from a continuous stream.\n",
      " |      \n",
      " |      The algorithm for incremental mean and std is given in Equation 1.5a,b\n",
      " |      in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. \"Algorithms\n",
      " |      for computing the sample variance: Analysis and recommendations.\"\n",
      " |      The American Statistician 37.3 (1983): 242-247:\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape [n_samples, n_features]\n",
      " |          The data used to compute the mean and standard deviation\n",
      " |          used for later scaling along the features axis.\n",
      " |      \n",
      " |      y : Passthrough for ``Pipeline`` compatibility.\n",
      " |  \n",
      " |  transform(self, X, y='deprecated', copy=None)\n",
      " |      Perform standardization by centering and scaling\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape [n_samples, n_features]\n",
      " |          The data used to scale along the features axis.\n",
      " |      y : (ignored)\n",
      " |          .. deprecated:: 0.19\n",
      " |             This parameter will be removed in 0.21.\n",
      " |      copy : bool, optional (default: None)\n",
      " |          Copy the input X or not.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to X and y with optional parameters fit_params\n",
      " |      and returns a transformed version of X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : numpy array of shape [n_samples, n_features]\n",
      " |          Training set.\n",
      " |      \n",
      " |      y : numpy array of shape [n_samples]\n",
      " |          Target values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : numpy array of shape [n_samples, n_features_new]\n",
      " |          Transformed array.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "help(StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
      "[ 0.5  0.5]\n",
      "[[-1. -1.]\n",
      " [-1. -1.]\n",
      " [ 1.  1.]\n",
      " [ 1.  1.]]\n",
      "[[ 3.  3.]]\n"
     ]
    }
   ],
   "source": [
    "data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n",
    "scaler = StandardScaler()\n",
    "print(scaler.fit(data))\n",
    "print(scaler.mean_)\n",
    "print(scaler.transform(data))\n",
    "print(scaler.transform([[2, 2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "X_combined_std = np.vstack((X_train_std, X_test_std))\n",
    "y_combined = np.hstack((y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot_decision_regions函数是用于显示分类结果的绘图程序，来自参考书1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "\n",
    "def versiontuple(v):\n",
    "    return tuple(map(int, (v.split(\".\"))))\n",
    "\n",
    "\n",
    "def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n",
    "\n",
    "    # setup marker generator and color map\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], \n",
    "                    y=X[y == cl, 1],\n",
    "                    alpha=0.6, \n",
    "                    c=cmap(idx),\n",
    "                    edgecolor='black',\n",
    "                    marker=markers[idx], \n",
    "                    label=cl)\n",
    "\n",
    "    # highlight test samples\n",
    "    if test_idx:\n",
    "        # plot all samples\n",
    "        if not versiontuple(np.__version__) >= versiontuple('1.9.0'):\n",
    "            X_test, y_test = X[list(test_idx), :], y[list(test_idx)]\n",
    "            warnings.warn('Please update to NumPy 1.9.0 or newer')\n",
    "        else:\n",
    "            X_test, y_test = X[test_idx, :], y[test_idx]\n",
    "\n",
    "        plt.scatter(X_test[:, 0],\n",
    "                    X_test[:, 1],\n",
    "                    c='',\n",
    "                    alpha=1.0,\n",
    "                    edgecolor='black',\n",
    "                    linewidths=1,\n",
    "                    marker='o',\n",
    "                    s=55, label='test set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 感知器 Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "错分样本数： 4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "ppn = Perceptron(max_iter=40, eta0=0.1, random_state=0)\n",
    "ppn.fit(X_train_std, y_train)\n",
    "\n",
    "y_pred =ppn.predict(X_test_std)\n",
    "print('错分样本数： %d' % (y_test!= y_pred).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11350bd68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plot_decision_regions(X=X_combined_std,\n",
    "                      y=y_combined,\n",
    "                      classifier=ppn,\n",
    "                      test_idx=range(105,150))\n",
    "plt.xlabel('petal length(standardized)')\n",
    "plt.ylabel('petal width(standardized)')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn.linear_model in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.linear_model\n",
      "\n",
      "DESCRIPTION\n",
      "    The :mod:`sklearn.linear_model` module implements generalized linear models. It\n",
      "    includes Ridge regression, Bayesian Regression, Lasso and Elastic Net\n",
      "    estimators computed with Least Angle Regression and coordinate descent. It also\n",
      "    implements Stochastic Gradient Descent related algorithms.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    base\n",
      "    bayes\n",
      "    cd_fast\n",
      "    coordinate_descent\n",
      "    huber\n",
      "    least_angle\n",
      "    logistic\n",
      "    omp\n",
      "    passive_aggressive\n",
      "    perceptron\n",
      "    randomized_l1\n",
      "    ransac\n",
      "    ridge\n",
      "    sag\n",
      "    sag_fast\n",
      "    setup\n",
      "    sgd_fast\n",
      "    stochastic_gradient\n",
      "    tests (package)\n",
      "    theil_sen\n",
      "\n",
      "CLASSES\n",
      "    sklearn.base.BaseEstimator(builtins.object)\n",
      "        sklearn.linear_model.huber.HuberRegressor(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model.logistic.LogisticRegression(sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin, sklearn.linear_model.base.SparseCoefMixin)\n",
      "            sklearn.linear_model.logistic.LogisticRegressionCV(sklearn.linear_model.logistic.LogisticRegression, sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin)\n",
      "        sklearn.linear_model.ransac.RANSACRegressor(sklearn.base.BaseEstimator, sklearn.base.MetaEstimatorMixin, sklearn.base.RegressorMixin)\n",
      "    sklearn.base.MetaEstimatorMixin(builtins.object)\n",
      "        sklearn.linear_model.ransac.RANSACRegressor(sklearn.base.BaseEstimator, sklearn.base.MetaEstimatorMixin, sklearn.base.RegressorMixin)\n",
      "    sklearn.base.RegressorMixin(builtins.object)\n",
      "        sklearn.linear_model.base.LinearRegression(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.bayes.ARDRegression(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.bayes.BayesianRidge(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.coordinate_descent.ElasticNet(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "            sklearn.linear_model.coordinate_descent.Lasso\n",
      "                sklearn.linear_model.coordinate_descent.MultiTaskElasticNet\n",
      "                    sklearn.linear_model.coordinate_descent.MultiTaskLasso\n",
      "        sklearn.linear_model.coordinate_descent.ElasticNetCV(sklearn.linear_model.coordinate_descent.LinearModelCV, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.coordinate_descent.LassoCV(sklearn.linear_model.coordinate_descent.LinearModelCV, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.coordinate_descent.MultiTaskElasticNetCV(sklearn.linear_model.coordinate_descent.LinearModelCV, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.coordinate_descent.MultiTaskLassoCV(sklearn.linear_model.coordinate_descent.LinearModelCV, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.huber.HuberRegressor(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model.least_angle.Lars(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "            sklearn.linear_model.least_angle.LarsCV\n",
      "                sklearn.linear_model.least_angle.LassoLarsCV\n",
      "            sklearn.linear_model.least_angle.LassoLars\n",
      "                sklearn.linear_model.least_angle.LassoLarsIC\n",
      "        sklearn.linear_model.omp.OrthogonalMatchingPursuit(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.omp.OrthogonalMatchingPursuitCV(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.ransac.RANSACRegressor(sklearn.base.BaseEstimator, sklearn.base.MetaEstimatorMixin, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.ridge.Ridge(sklearn.linear_model.ridge._BaseRidge, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.ridge.RidgeCV(sklearn.linear_model.ridge._BaseRidgeCV, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.theil_sen.TheilSenRegressor(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "    sklearn.linear_model.base.LinearClassifierMixin(sklearn.base.ClassifierMixin)\n",
      "        sklearn.linear_model.logistic.LogisticRegression(sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin, sklearn.linear_model.base.SparseCoefMixin)\n",
      "            sklearn.linear_model.logistic.LogisticRegressionCV(sklearn.linear_model.logistic.LogisticRegression, sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin)\n",
      "        sklearn.linear_model.ridge.RidgeClassifier(sklearn.linear_model.base.LinearClassifierMixin, sklearn.linear_model.ridge._BaseRidge)\n",
      "        sklearn.linear_model.ridge.RidgeClassifierCV(sklearn.linear_model.base.LinearClassifierMixin, sklearn.linear_model.ridge._BaseRidgeCV)\n",
      "    sklearn.linear_model.base.LinearModel(abc.NewBase)\n",
      "        sklearn.linear_model.base.LinearRegression(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.bayes.ARDRegression(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.bayes.BayesianRidge(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.coordinate_descent.ElasticNet(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "            sklearn.linear_model.coordinate_descent.Lasso\n",
      "                sklearn.linear_model.coordinate_descent.MultiTaskElasticNet\n",
      "                    sklearn.linear_model.coordinate_descent.MultiTaskLasso\n",
      "        sklearn.linear_model.huber.HuberRegressor(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model.least_angle.Lars(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "            sklearn.linear_model.least_angle.LarsCV\n",
      "                sklearn.linear_model.least_angle.LassoLarsCV\n",
      "            sklearn.linear_model.least_angle.LassoLars\n",
      "                sklearn.linear_model.least_angle.LassoLarsIC\n",
      "        sklearn.linear_model.omp.OrthogonalMatchingPursuit(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.omp.OrthogonalMatchingPursuitCV(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.theil_sen.TheilSenRegressor(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "    sklearn.linear_model.base.SparseCoefMixin(builtins.object)\n",
      "        sklearn.linear_model.logistic.LogisticRegression(sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin, sklearn.linear_model.base.SparseCoefMixin)\n",
      "            sklearn.linear_model.logistic.LogisticRegressionCV(sklearn.linear_model.logistic.LogisticRegression, sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin)\n",
      "    sklearn.linear_model.coordinate_descent.LinearModelCV(abc.NewBase)\n",
      "        sklearn.linear_model.coordinate_descent.ElasticNetCV(sklearn.linear_model.coordinate_descent.LinearModelCV, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.coordinate_descent.LassoCV(sklearn.linear_model.coordinate_descent.LinearModelCV, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.coordinate_descent.MultiTaskElasticNetCV(sklearn.linear_model.coordinate_descent.LinearModelCV, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.coordinate_descent.MultiTaskLassoCV(sklearn.linear_model.coordinate_descent.LinearModelCV, sklearn.base.RegressorMixin)\n",
      "    sklearn.linear_model.randomized_l1.BaseRandomizedLinearModel(abc.NewBase)\n",
      "        sklearn.linear_model.randomized_l1.RandomizedLasso\n",
      "        sklearn.linear_model.randomized_l1.RandomizedLogisticRegression\n",
      "    sklearn.linear_model.ridge._BaseRidge(abc.NewBase)\n",
      "        sklearn.linear_model.ridge.Ridge(sklearn.linear_model.ridge._BaseRidge, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.ridge.RidgeClassifier(sklearn.linear_model.base.LinearClassifierMixin, sklearn.linear_model.ridge._BaseRidge)\n",
      "    sklearn.linear_model.ridge._BaseRidgeCV(sklearn.linear_model.base.LinearModel)\n",
      "        sklearn.linear_model.ridge.RidgeCV(sklearn.linear_model.ridge._BaseRidgeCV, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.ridge.RidgeClassifierCV(sklearn.linear_model.base.LinearClassifierMixin, sklearn.linear_model.ridge._BaseRidgeCV)\n",
      "    sklearn.linear_model.sgd_fast.Classification(sklearn.linear_model.sgd_fast.LossFunction)\n",
      "        sklearn.linear_model.sgd_fast.Hinge\n",
      "        sklearn.linear_model.sgd_fast.Log\n",
      "        sklearn.linear_model.sgd_fast.ModifiedHuber\n",
      "    sklearn.linear_model.sgd_fast.Regression(sklearn.linear_model.sgd_fast.LossFunction)\n",
      "        sklearn.linear_model.sgd_fast.Huber\n",
      "        sklearn.linear_model.sgd_fast.SquaredLoss\n",
      "    sklearn.linear_model.stochastic_gradient.BaseSGDClassifier(abc.NewBase)\n",
      "        sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier\n",
      "        sklearn.linear_model.perceptron.Perceptron\n",
      "        sklearn.linear_model.stochastic_gradient.SGDClassifier\n",
      "    sklearn.linear_model.stochastic_gradient.BaseSGDRegressor(sklearn.linear_model.stochastic_gradient.BaseSGD, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.passive_aggressive.PassiveAggressiveRegressor\n",
      "        sklearn.linear_model.stochastic_gradient.SGDRegressor\n",
      "    \n",
      "    class ARDRegression(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "     |  Bayesian ARD regression.\n",
      "     |  \n",
      "     |  Fit the weights of a regression model, using an ARD prior. The weights of\n",
      "     |  the regression model are assumed to be in Gaussian distributions.\n",
      "     |  Also estimate the parameters lambda (precisions of the distributions of the\n",
      "     |  weights) and alpha (precision of the distribution of the noise).\n",
      "     |  The estimation is done by an iterative procedures (Evidence Maximization)\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <bayesian_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_iter : int, optional\n",
      "     |      Maximum number of iterations. Default is 300\n",
      "     |  \n",
      "     |  tol : float, optional\n",
      "     |      Stop the algorithm if w has converged. Default is 1.e-3.\n",
      "     |  \n",
      "     |  alpha_1 : float, optional\n",
      "     |      Hyper-parameter : shape parameter for the Gamma distribution prior\n",
      "     |      over the alpha parameter. Default is 1.e-6.\n",
      "     |  \n",
      "     |  alpha_2 : float, optional\n",
      "     |      Hyper-parameter : inverse scale parameter (rate parameter) for the\n",
      "     |      Gamma distribution prior over the alpha parameter. Default is 1.e-6.\n",
      "     |  \n",
      "     |  lambda_1 : float, optional\n",
      "     |      Hyper-parameter : shape parameter for the Gamma distribution prior\n",
      "     |      over the lambda parameter. Default is 1.e-6.\n",
      "     |  \n",
      "     |  lambda_2 : float, optional\n",
      "     |      Hyper-parameter : inverse scale parameter (rate parameter) for the\n",
      "     |      Gamma distribution prior over the lambda parameter. Default is 1.e-6.\n",
      "     |  \n",
      "     |  compute_score : boolean, optional\n",
      "     |      If True, compute the objective function at each step of the model.\n",
      "     |      Default is False.\n",
      "     |  \n",
      "     |  threshold_lambda : float, optional\n",
      "     |      threshold for removing (pruning) weights with high precision from\n",
      "     |      the computation. Default is 1.e+4.\n",
      "     |  \n",
      "     |  fit_intercept : boolean, optional\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |      Default is True.\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True.\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  verbose : boolean, optional, default False\n",
      "     |      Verbose mode when fitting the model.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape = (n_features)\n",
      "     |      Coefficients of the regression model (mean of distribution)\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |     estimated precision of the noise.\n",
      "     |  \n",
      "     |  lambda_ : array, shape = (n_features)\n",
      "     |     estimated precisions of the weights.\n",
      "     |  \n",
      "     |  sigma_ : array, shape = (n_features, n_features)\n",
      "     |      estimated variance-covariance matrix of the weights\n",
      "     |  \n",
      "     |  scores_ : float\n",
      "     |      if computed, value of the objective function (to be maximized)\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.ARDRegression()\n",
      "     |  >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n",
      "     |  ... # doctest: +NORMALIZE_WHITESPACE\n",
      "     |  ARDRegression(alpha_1=1e-06, alpha_2=1e-06, compute_score=False,\n",
      "     |          copy_X=True, fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06,\n",
      "     |          n_iter=300, normalize=False, threshold_lambda=10000.0, tol=0.001,\n",
      "     |          verbose=False)\n",
      "     |  >>> clf.predict([[1, 1]])\n",
      "     |  array([ 1.])\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  For an example, see :ref:`examples/linear_model/plot_ard.py\n",
      "     |  <sphx_glr_auto_examples_linear_model_plot_ard.py>`.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  D. J. C. MacKay, Bayesian nonlinear modeling for the prediction\n",
      "     |  competition, ASHRAE Transactions, 1994.\n",
      "     |  \n",
      "     |  R. Salakhutdinov, Lecture notes on Statistical Machine Learning,\n",
      "     |  http://www.utstat.toronto.edu/~rsalakhu/sta4273/notes/Lecture2.pdf#page=15\n",
      "     |  Their beta is our ``self.alpha_``\n",
      "     |  Their alpha is our ``self.lambda_``\n",
      "     |  ARD is a little different than the slide: only dimensions/features for\n",
      "     |  which ``self.lambda_ < self.threshold_lambda`` are kept and the rest are\n",
      "     |  discarded.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ARDRegression\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_iter=300, tol=0.001, alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06, compute_score=False, threshold_lambda=10000.0, fit_intercept=True, normalize=False, copy_X=True, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the ARDRegression model according to the given training data\n",
      "     |      and parameters.\n",
      "     |      \n",
      "     |      Iterative procedure to maximize the evidence\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = [n_samples, n_features]\n",
      "     |          Training vector, where n_samples in the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      y : array, shape = [n_samples]\n",
      "     |          Target values (integers). Will be cast to X's dtype if necessary\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  predict(self, X, return_std=False)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      In addition to the mean of the predictive distribution, also its\n",
      "     |      standard deviation can be returned.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      return_std : boolean, optional\n",
      "     |          Whether to return the standard deviation of posterior prediction.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_mean : array, shape = (n_samples,)\n",
      "     |          Mean of predictive distribution of query points.\n",
      "     |      \n",
      "     |      y_std : array, shape = (n_samples,)\n",
      "     |          Standard deviation of predictive distribution of query points.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class BayesianRidge(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "     |  Bayesian ridge regression\n",
      "     |  \n",
      "     |  Fit a Bayesian ridge model and optimize the regularization parameters\n",
      "     |  lambda (precision of the weights) and alpha (precision of the noise).\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <bayesian_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_iter : int, optional\n",
      "     |      Maximum number of iterations.  Default is 300.\n",
      "     |  \n",
      "     |  tol : float, optional\n",
      "     |      Stop the algorithm if w has converged. Default is 1.e-3.\n",
      "     |  \n",
      "     |  alpha_1 : float, optional\n",
      "     |      Hyper-parameter : shape parameter for the Gamma distribution prior\n",
      "     |      over the alpha parameter. Default is 1.e-6\n",
      "     |  \n",
      "     |  alpha_2 : float, optional\n",
      "     |      Hyper-parameter : inverse scale parameter (rate parameter) for the\n",
      "     |      Gamma distribution prior over the alpha parameter.\n",
      "     |      Default is 1.e-6.\n",
      "     |  \n",
      "     |  lambda_1 : float, optional\n",
      "     |      Hyper-parameter : shape parameter for the Gamma distribution prior\n",
      "     |      over the lambda parameter. Default is 1.e-6.\n",
      "     |  \n",
      "     |  lambda_2 : float, optional\n",
      "     |      Hyper-parameter : inverse scale parameter (rate parameter) for the\n",
      "     |      Gamma distribution prior over the lambda parameter.\n",
      "     |      Default is 1.e-6\n",
      "     |  \n",
      "     |  compute_score : boolean, optional\n",
      "     |      If True, compute the objective function at each step of the model.\n",
      "     |      Default is False\n",
      "     |  \n",
      "     |  fit_intercept : boolean, optional\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |      Default is True.\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  verbose : boolean, optional, default False\n",
      "     |      Verbose mode when fitting the model.\n",
      "     |  \n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape = (n_features)\n",
      "     |      Coefficients of the regression model (mean of distribution)\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |     estimated precision of the noise.\n",
      "     |  \n",
      "     |  lambda_ : float\n",
      "     |     estimated precision of the weights.\n",
      "     |  \n",
      "     |  sigma_ : array, shape = (n_features, n_features)\n",
      "     |      estimated variance-covariance matrix of the weights\n",
      "     |  \n",
      "     |  scores_ : float\n",
      "     |      if computed, value of the objective function (to be maximized)\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.BayesianRidge()\n",
      "     |  >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n",
      "     |  ... # doctest: +NORMALIZE_WHITESPACE\n",
      "     |  BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False,\n",
      "     |          copy_X=True, fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06,\n",
      "     |          n_iter=300, normalize=False, tol=0.001, verbose=False)\n",
      "     |  >>> clf.predict([[1, 1]])\n",
      "     |  array([ 1.])\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  For an example, see :ref:`examples/linear_model/plot_bayesian_ridge.py\n",
      "     |  <sphx_glr_auto_examples_linear_model_plot_bayesian_ridge.py>`.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  D. J. C. MacKay, Bayesian Interpolation, Computation and Neural Systems,\n",
      "     |  Vol. 4, No. 3, 1992.\n",
      "     |  \n",
      "     |  R. Salakhutdinov, Lecture notes on Statistical Machine Learning,\n",
      "     |  http://www.utstat.toronto.edu/~rsalakhu/sta4273/notes/Lecture2.pdf#page=15\n",
      "     |  Their beta is our ``self.alpha_``\n",
      "     |  Their alpha is our ``self.lambda_``\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BayesianRidge\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_iter=300, tol=0.001, alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06, compute_score=False, fit_intercept=True, normalize=False, copy_X=True, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : numpy array of shape [n_samples,n_features]\n",
      "     |          Training data\n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Target values. Will be cast to X's dtype if necessary\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  predict(self, X, return_std=False)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      In addition to the mean of the predictive distribution, also its\n",
      "     |      standard deviation can be returned.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      return_std : boolean, optional\n",
      "     |          Whether to return the standard deviation of posterior prediction.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_mean : array, shape = (n_samples,)\n",
      "     |          Mean of predictive distribution of query points.\n",
      "     |      \n",
      "     |      y_std : array, shape = (n_samples,)\n",
      "     |          Standard deviation of predictive distribution of query points.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class ElasticNet(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "     |  Linear regression with combined L1 and L2 priors as regularizer.\n",
      "     |  \n",
      "     |  Minimizes the objective function::\n",
      "     |  \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |  \n",
      "     |  If you are interested in controlling the L1 and L2 penalty\n",
      "     |  separately, keep in mind that this is equivalent to::\n",
      "     |  \n",
      "     |          a * L1 + b * L2\n",
      "     |  \n",
      "     |  where::\n",
      "     |  \n",
      "     |          alpha = a + b and l1_ratio = a / (a + b)\n",
      "     |  \n",
      "     |  The parameter l1_ratio corresponds to alpha in the glmnet R package while\n",
      "     |  alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio\n",
      "     |  = 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not reliable,\n",
      "     |  unless you supply your own sequence of alpha.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, optional\n",
      "     |      Constant that multiplies the penalty terms. Defaults to 1.0.\n",
      "     |      See the notes for the exact mathematical meaning of this\n",
      "     |      parameter.``alpha = 0`` is equivalent to an ordinary least square,\n",
      "     |      solved by the :class:`LinearRegression` object. For numerical\n",
      "     |      reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.\n",
      "     |      Given this, you should use the :class:`LinearRegression` object.\n",
      "     |  \n",
      "     |  l1_ratio : float\n",
      "     |      The ElasticNet mixing parameter, with ``0 <= l1_ratio <= 1``. For\n",
      "     |      ``l1_ratio = 0`` the penalty is an L2 penalty. ``For l1_ratio = 1`` it\n",
      "     |      is an L1 penalty.  For ``0 < l1_ratio < 1``, the penalty is a\n",
      "     |      combination of L1 and L2.\n",
      "     |  \n",
      "     |  fit_intercept : bool\n",
      "     |      Whether the intercept should be estimated or not. If ``False``, the\n",
      "     |      data is assumed to be already centered.\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  precompute : True | False | array-like\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. The Gram matrix can also be passed as argument.\n",
      "     |      For sparse input this option is always ``True`` to preserve sparsity.\n",
      "     |  \n",
      "     |  max_iter : int, optional\n",
      "     |      The maximum number of iterations\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  tol : float, optional\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  warm_start : bool, optional\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |  \n",
      "     |  positive : bool, optional\n",
      "     |      When set to ``True``, forces the coefficients to be positive.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional, default None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update.  If int, random_state is the seed used by the random\n",
      "     |      number generator; If RandomState instance, random_state is the random\n",
      "     |      number generator; If None, the random number generator is the\n",
      "     |      RandomState instance used by `np.random`. Used when ``selection`` ==\n",
      "     |      'random'.\n",
      "     |  \n",
      "     |  selection : str, default 'cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape (n_features,) | (n_targets, n_features)\n",
      "     |      parameter vector (w in the cost function formula)\n",
      "     |  \n",
      "     |  sparse_coef_ : scipy.sparse matrix, shape (n_features, 1) |             (n_targets, n_features)\n",
      "     |      ``sparse_coef_`` is a readonly property derived from ``coef_``\n",
      "     |  \n",
      "     |  intercept_ : float | array, shape (n_targets,)\n",
      "     |      independent term in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : array-like, shape (n_targets,)\n",
      "     |      number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import ElasticNet\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>>\n",
      "     |  >>> X, y = make_regression(n_features=2, random_state=0)\n",
      "     |  >>> regr = ElasticNet(random_state=0)\n",
      "     |  >>> regr.fit(X, y)\n",
      "     |  ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
      "     |        max_iter=1000, normalize=False, positive=False, precompute=False,\n",
      "     |        random_state=0, selection='cyclic', tol=0.0001, warm_start=False)\n",
      "     |  >>> print(regr.coef_) # doctest: +ELLIPSIS\n",
      "     |  [ 18.83816048  64.55968825]\n",
      "     |  >>> print(regr.intercept_) # doctest: +ELLIPSIS\n",
      "     |  1.45126075617\n",
      "     |  >>> print(regr.predict([[0, 0]])) # doctest: +ELLIPSIS\n",
      "     |  [ 1.45126076]\n",
      "     |  \n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |  should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  SGDRegressor: implements elastic net regression with incremental training.\n",
      "     |  SGDClassifier: implements logistic regression with elastic net penalty\n",
      "     |      (``SGDClassifier(loss=\"log\", penalty=\"elasticnet\")``).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ElasticNet\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, l1_ratio=0.5, fit_intercept=True, normalize=False, precompute=False, max_iter=1000, copy_X=True, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, check_input=True)\n",
      "     |      Fit model with coordinate descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      -----------\n",
      "     |      X : ndarray or scipy.sparse matrix, (n_samples, n_features)\n",
      "     |          Data\n",
      "     |      \n",
      "     |      y : ndarray, shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target. Will be cast to X's dtype if necessary\n",
      "     |      \n",
      "     |      check_input : boolean, (default=True)\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      \n",
      "     |      Coordinate descent is an algorithm that considers each column of\n",
      "     |      data at a time hence it will automatically convert the X input\n",
      "     |      as a Fortran-contiguous numpy array if necessary.\n",
      "     |      \n",
      "     |      To avoid memory re-allocation it is advised to allocate the\n",
      "     |      initial data in memory directly using that format.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  path = enet_path(X, y, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      "     |      Compute elastic net path with coordinate descent\n",
      "     |      \n",
      "     |      The elastic net optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n",
      "     |          + alpha * l1_ratio * ||W||_21\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like}, shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : ndarray, shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      l1_ratio : float, optional\n",
      "     |          float between 0 and 1 passed to elastic net (scaling between\n",
      "     |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso\n",
      "     |      \n",
      "     |      eps : float\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``\n",
      "     |      \n",
      "     |      n_alphas : int, optional\n",
      "     |          Number of alphas along the regularization path\n",
      "     |      \n",
      "     |      alphas : ndarray, optional\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If None alphas are set automatically\n",
      "     |      \n",
      "     |      precompute : True | False | 'auto' | array-like\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like, optional\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : boolean, optional, default True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : array, shape (n_features, ) | None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or integer\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool\n",
      "     |          whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      check_input : bool, default True\n",
      "     |          Skip input validation checks, including the Gram matrix when provided\n",
      "     |          assuming there are handled by the caller when check_input=False.\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : array, shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : array, shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : array, shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : array-like, shape (n_alphas,)\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |          (Is returned when ``return_n_iter`` is set to True).\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "     |      \n",
      "     |      See also\n",
      "     |      --------\n",
      "     |      MultiTaskElasticNet\n",
      "     |      MultiTaskElasticNetCV\n",
      "     |      ElasticNet\n",
      "     |      ElasticNetCV\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  sparse_coef_\n",
      "     |      sparse representation of the fitted ``coef_``\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class ElasticNetCV(LinearModelCV, sklearn.base.RegressorMixin)\n",
      "     |  Elastic Net model with iterative fitting along a regularization path\n",
      "     |  \n",
      "     |  The best model is selected by cross-validation.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  l1_ratio : float or array of floats, optional\n",
      "     |      float between 0 and 1 passed to ElasticNet (scaling between\n",
      "     |      l1 and l2 penalties). For ``l1_ratio = 0``\n",
      "     |      the penalty is an L2 penalty. For ``l1_ratio = 1`` it is an L1 penalty.\n",
      "     |      For ``0 < l1_ratio < 1``, the penalty is a combination of L1 and L2\n",
      "     |      This parameter can be a list, in which case the different\n",
      "     |      values are tested by cross-validation and the one giving the best\n",
      "     |      prediction score is used. Note that a good choice of list of\n",
      "     |      values for l1_ratio is often to put more values close to 1\n",
      "     |      (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7,\n",
      "     |      .9, .95, .99, 1]``\n",
      "     |  \n",
      "     |  eps : float, optional\n",
      "     |      Length of the path. ``eps=1e-3`` means that\n",
      "     |      ``alpha_min / alpha_max = 1e-3``.\n",
      "     |  \n",
      "     |  n_alphas : int, optional\n",
      "     |      Number of alphas along the regularization path, used for each l1_ratio.\n",
      "     |  \n",
      "     |  alphas : numpy array, optional\n",
      "     |      List of alphas where to compute the models.\n",
      "     |      If None alphas are set automatically\n",
      "     |  \n",
      "     |  fit_intercept : boolean\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  precompute : True | False | 'auto' | array-like\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |      matrix can also be passed as argument.\n",
      "     |  \n",
      "     |  max_iter : int, optional\n",
      "     |      The maximum number of iterations\n",
      "     |  \n",
      "     |  tol : float, optional\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, optional\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 3-fold cross-validation,\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - An object to be used as a cross-validation generator.\n",
      "     |      - An iterable yielding train/test splits.\n",
      "     |  \n",
      "     |      For integer/None inputs, :class:`KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  verbose : bool or integer\n",
      "     |      Amount of verbosity.\n",
      "     |  \n",
      "     |  n_jobs : integer, optional\n",
      "     |      Number of CPUs to use during the cross validation. If ``-1``, use\n",
      "     |      all the CPUs.\n",
      "     |  \n",
      "     |  positive : bool, optional\n",
      "     |      When set to ``True``, forces the coefficients to be positive.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional, default None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update.  If int, random_state is the seed used by the random\n",
      "     |      number generator; If RandomState instance, random_state is the random\n",
      "     |      number generator; If None, the random number generator is the\n",
      "     |      RandomState instance used by `np.random`. Used when ``selection`` ==\n",
      "     |      'random'.\n",
      "     |  \n",
      "     |  selection : str, default 'cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  alpha_ : float\n",
      "     |      The amount of penalization chosen by cross validation\n",
      "     |  \n",
      "     |  l1_ratio_ : float\n",
      "     |      The compromise between l1 and l2 penalization chosen by\n",
      "     |      cross validation\n",
      "     |  \n",
      "     |  coef_ : array, shape (n_features,) | (n_targets, n_features)\n",
      "     |      Parameter vector (w in the cost function formula),\n",
      "     |  \n",
      "     |  intercept_ : float | array, shape (n_targets, n_features)\n",
      "     |      Independent term in the decision function.\n",
      "     |  \n",
      "     |  mse_path_ : array, shape (n_l1_ratio, n_alpha, n_folds)\n",
      "     |      Mean square error for the test set on each fold, varying l1_ratio and\n",
      "     |      alpha.\n",
      "     |  \n",
      "     |  alphas_ : numpy array, shape (n_alphas,) or (n_l1_ratio, n_alphas)\n",
      "     |      The grid of alphas used for fitting, for each l1_ratio.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance for the optimal alpha.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import ElasticNetCV\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>>\n",
      "     |  >>> X, y = make_regression(n_features=2, random_state=0)\n",
      "     |  >>> regr = ElasticNetCV(cv=5, random_state=0)\n",
      "     |  >>> regr.fit(X, y)\n",
      "     |  ElasticNetCV(alphas=None, copy_X=True, cv=5, eps=0.001, fit_intercept=True,\n",
      "     |         l1_ratio=0.5, max_iter=1000, n_alphas=100, n_jobs=1,\n",
      "     |         normalize=False, positive=False, precompute='auto', random_state=0,\n",
      "     |         selection='cyclic', tol=0.0001, verbose=0)\n",
      "     |  >>> print(regr.alpha_) # doctest: +ELLIPSIS\n",
      "     |  0.19947279427\n",
      "     |  >>> print(regr.intercept_) # doctest: +ELLIPSIS\n",
      "     |  0.398882965428\n",
      "     |  >>> print(regr.predict([[0, 0]])) # doctest: +ELLIPSIS\n",
      "     |  [ 0.39888297]\n",
      "     |  \n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  For an example, see\n",
      "     |  :ref:`examples/linear_model/plot_lasso_model_selection.py\n",
      "     |  <sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py>`.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |  should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |  \n",
      "     |  The parameter l1_ratio corresponds to alpha in the glmnet R package\n",
      "     |  while alpha corresponds to the lambda parameter in glmnet.\n",
      "     |  More specifically, the optimization objective is::\n",
      "     |  \n",
      "     |      1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |      + alpha * l1_ratio * ||w||_1\n",
      "     |      + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |  \n",
      "     |  If you are interested in controlling the L1 and L2 penalty\n",
      "     |  separately, keep in mind that this is equivalent to::\n",
      "     |  \n",
      "     |      a * L1 + b * L2\n",
      "     |  \n",
      "     |  for::\n",
      "     |  \n",
      "     |      alpha = a + b and l1_ratio = a / (a + b).\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  enet_path\n",
      "     |  ElasticNet\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ElasticNetCV\n",
      "     |      LinearModelCV\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize=False, precompute='auto', max_iter=1000, tol=0.0001, cv=None, copy_X=True, verbose=0, n_jobs=1, positive=False, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  path = enet_path(X, y, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      "     |      Compute elastic net path with coordinate descent\n",
      "     |      \n",
      "     |      The elastic net optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n",
      "     |          + alpha * l1_ratio * ||W||_21\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like}, shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : ndarray, shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      l1_ratio : float, optional\n",
      "     |          float between 0 and 1 passed to elastic net (scaling between\n",
      "     |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso\n",
      "     |      \n",
      "     |      eps : float\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``\n",
      "     |      \n",
      "     |      n_alphas : int, optional\n",
      "     |          Number of alphas along the regularization path\n",
      "     |      \n",
      "     |      alphas : ndarray, optional\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If None alphas are set automatically\n",
      "     |      \n",
      "     |      precompute : True | False | 'auto' | array-like\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like, optional\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : boolean, optional, default True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : array, shape (n_features, ) | None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or integer\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool\n",
      "     |          whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      check_input : bool, default True\n",
      "     |          Skip input validation checks, including the Gram matrix when provided\n",
      "     |          assuming there are handled by the caller when check_input=False.\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : array, shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : array, shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : array, shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : array-like, shape (n_alphas,)\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |          (Is returned when ``return_n_iter`` is set to True).\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "     |      \n",
      "     |      See also\n",
      "     |      --------\n",
      "     |      MultiTaskElasticNet\n",
      "     |      MultiTaskElasticNetCV\n",
      "     |      ElasticNet\n",
      "     |      ElasticNetCV\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LinearModelCV:\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit linear model with coordinate descent\n",
      "     |      \n",
      "     |      Fit is on grid of alphas and best alpha estimated by cross-validation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like}, shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data\n",
      "     |          to avoid unnecessary memory duplication. If y is mono-output,\n",
      "     |          X can be sparse.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class Hinge(Classification)\n",
      "     |  Hinge loss for binary classification tasks with y in {-1,1}\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  threshold : float > 0.0\n",
      "     |      Margin threshold. When threshold=1.0, one gets the loss used by SVM.\n",
      "     |      When threshold=0.0, one gets the loss used by the Perceptron.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Hinge\n",
      "     |      Classification\n",
      "     |      LossFunction\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      helper for pickle\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __pyx_vtable__ = <capsule object NULL>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Classification:\n",
      "     |  \n",
      "     |  __setstate__ = __setstate_cython__(...)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunction:\n",
      "     |  \n",
      "     |  dloss(...)\n",
      "     |      Evaluate the derivative of the loss function with respect to\n",
      "     |      the prediction `p`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : double\n",
      "     |          The prediction, p = w^T x\n",
      "     |      y : double\n",
      "     |          The true value (aka target)\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      double\n",
      "     |          The derivative of the loss function with regards to `p`.\n",
      "    \n",
      "    class Huber(Regression)\n",
      "     |  Huber regression loss\n",
      "     |  \n",
      "     |  Variant of the SquaredLoss that is robust to outliers (quadratic near zero,\n",
      "     |  linear in for large errors).\n",
      "     |  \n",
      "     |  https://en.wikipedia.org/wiki/Huber_Loss_Function\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Huber\n",
      "     |      Regression\n",
      "     |      LossFunction\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      helper for pickle\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __pyx_vtable__ = <capsule object NULL>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Regression:\n",
      "     |  \n",
      "     |  __setstate__ = __setstate_cython__(...)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunction:\n",
      "     |  \n",
      "     |  dloss(...)\n",
      "     |      Evaluate the derivative of the loss function with respect to\n",
      "     |      the prediction `p`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : double\n",
      "     |          The prediction, p = w^T x\n",
      "     |      y : double\n",
      "     |          The true value (aka target)\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      double\n",
      "     |          The derivative of the loss function with regards to `p`.\n",
      "    \n",
      "    class HuberRegressor(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin, sklearn.base.BaseEstimator)\n",
      "     |  Linear regression model that is robust to outliers.\n",
      "     |  \n",
      "     |  The Huber Regressor optimizes the squared loss for the samples where\n",
      "     |  ``|(y - X'w) / sigma| < epsilon`` and the absolute loss for the samples\n",
      "     |  where ``|(y - X'w) / sigma| > epsilon``, where w and sigma are parameters\n",
      "     |  to be optimized. The parameter sigma makes sure that if y is scaled up\n",
      "     |  or down by a certain factor, one does not need to rescale epsilon to\n",
      "     |  achieve the same robustness. Note that this does not take into account\n",
      "     |  the fact that the different features of X may be of different scales.\n",
      "     |  \n",
      "     |  This makes sure that the loss function is not heavily influenced by the\n",
      "     |  outliers while not completely ignoring their effect.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <huber_regression>`\n",
      "     |  \n",
      "     |  .. versionadded:: 0.18\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  epsilon : float, greater than 1.0, default 1.35\n",
      "     |      The parameter epsilon controls the number of samples that should be\n",
      "     |      classified as outliers. The smaller the epsilon, the more robust it is\n",
      "     |      to outliers.\n",
      "     |  \n",
      "     |  max_iter : int, default 100\n",
      "     |      Maximum number of iterations that scipy.optimize.fmin_l_bfgs_b\n",
      "     |      should run for.\n",
      "     |  \n",
      "     |  alpha : float, default 0.0001\n",
      "     |      Regularization parameter.\n",
      "     |  \n",
      "     |  warm_start : bool, default False\n",
      "     |      This is useful if the stored attributes of a previously used model\n",
      "     |      has to be reused. If set to False, then the coefficients will\n",
      "     |      be rewritten for every call to fit.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default True\n",
      "     |      Whether or not to fit the intercept. This can be set to False\n",
      "     |      if the data is already centered around the origin.\n",
      "     |  \n",
      "     |  tol : float, default 1e-5\n",
      "     |      The iteration will stop when\n",
      "     |      ``max{|proj g_i | i = 1, ..., n}`` <= ``tol``\n",
      "     |      where pg_i is the i-th component of the projected gradient.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape (n_features,)\n",
      "     |      Features got by optimizing the Huber loss.\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      Bias.\n",
      "     |  \n",
      "     |  scale_ : float\n",
      "     |      The value by which ``|y - X'w - c|`` is scaled down.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Number of iterations that fmin_l_bfgs_b has run for.\n",
      "     |      Not available if SciPy version is 0.9 and below.\n",
      "     |  \n",
      "     |  outliers_ : array, shape (n_samples,)\n",
      "     |      A boolean mask which is set to True where the samples are identified\n",
      "     |      as outliers.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Peter J. Huber, Elvezio M. Ronchetti, Robust Statistics\n",
      "     |         Concomitant scale estimates, pg 172\n",
      "     |  .. [2] Art B. Owen (2006), A robust hybrid of lasso and ridge regression.\n",
      "     |         http://statweb.stanford.edu/~owen/reports/hhu.pdf\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      HuberRegressor\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, epsilon=1.35, max_iter=100, alpha=0.0001, warm_start=False, fit_intercept=True, tol=1e-05)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Training vector, where n_samples in the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,)\n",
      "     |          Target vector relative to X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,)\n",
      "     |          Weight given to each sample.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "    \n",
      "    class Lars(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "     |  Least Angle Regression model a.k.a. LAR\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <least_angle_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fit_intercept : boolean\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  verbose : boolean or integer, optional\n",
      "     |      Sets the verbosity amount\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default True\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  precompute : True | False | 'auto' | array-like\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |      matrix can also be passed as argument.\n",
      "     |  \n",
      "     |  n_nonzero_coefs : int, optional\n",
      "     |      Target number of non-zero coefficients. Use ``np.inf`` for no limit.\n",
      "     |  \n",
      "     |  eps : float, optional\n",
      "     |      The machine-precision regularization in the computation of the\n",
      "     |      Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "     |      systems. Unlike the ``tol`` parameter in some iterative\n",
      "     |      optimization-based algorithms, this parameter does not control\n",
      "     |      the tolerance of the optimization.\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  fit_path : boolean\n",
      "     |      If True the full path is stored in the ``coef_path_`` attribute.\n",
      "     |      If you compute the solution for a large problem or many targets,\n",
      "     |      setting ``fit_path`` to ``False`` will lead to a speedup, especially\n",
      "     |      with a small alpha.\n",
      "     |  \n",
      "     |  positive : boolean (default=False)\n",
      "     |      Restrict coefficients to be >= 0. Be aware that you might want to\n",
      "     |      remove fit_intercept which is set True by default.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  alphas_ : array, shape (n_alphas + 1,) | list of n_targets such arrays\n",
      "     |      Maximum of covariances (in absolute value) at each iteration.         ``n_alphas`` is either ``n_nonzero_coefs`` or ``n_features``,         whichever is smaller.\n",
      "     |  \n",
      "     |  active_ : list, length = n_alphas | list of n_targets such lists\n",
      "     |      Indices of active variables at the end of the path.\n",
      "     |  \n",
      "     |  coef_path_ : array, shape (n_features, n_alphas + 1)         | list of n_targets such arrays\n",
      "     |      The varying values of the coefficients along the path. It is not\n",
      "     |      present if the ``fit_path`` parameter is ``False``.\n",
      "     |  \n",
      "     |  coef_ : array, shape (n_features,) or (n_targets, n_features)\n",
      "     |      Parameter vector (w in the formulation formula).\n",
      "     |  \n",
      "     |  intercept_ : float | array, shape (n_targets,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : array-like or int\n",
      "     |      The number of iterations taken by lars_path to find the\n",
      "     |      grid of alphas for each target.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> reg = linear_model.Lars(n_nonzero_coefs=1)\n",
      "     |  >>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1.1111, 0, -1.1111])\n",
      "     |  ... # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE\n",
      "     |  Lars(copy_X=True, eps=..., fit_intercept=True, fit_path=True,\n",
      "     |     n_nonzero_coefs=1, normalize=True, positive=False, precompute='auto',\n",
      "     |     verbose=False)\n",
      "     |  >>> print(reg.coef_) # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE\n",
      "     |  [ 0. -1.11...]\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  lars_path, LarsCV\n",
      "     |  sklearn.decomposition.sparse_encode\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Lars\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, fit_intercept=True, verbose=False, normalize=True, precompute='auto', n_nonzero_coefs=500, eps=2.2204460492503131e-16, copy_X=True, fit_path=True, positive=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, Xy=None)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Xy : array-like, shape (n_samples,) or (n_samples, n_targets),                 optional\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  method = 'lar'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class LarsCV(Lars)\n",
      "     |  Cross-validated Least Angle Regression model\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <least_angle_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fit_intercept : boolean\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  verbose : boolean or integer, optional\n",
      "     |      Sets the verbosity amount\n",
      "     |  \n",
      "     |  max_iter : integer, optional\n",
      "     |      Maximum number of iterations to perform.\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default True\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  precompute : True | False | 'auto' | array-like\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram matrix\n",
      "     |      cannot be passed as argument since we will use only subsets of X.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, optional\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 3-fold cross-validation,\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - An object to be used as a cross-validation generator.\n",
      "     |      - An iterable yielding train/test splits.\n",
      "     |  \n",
      "     |      For integer/None inputs, :class:`KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |  max_n_alphas : integer, optional\n",
      "     |      The maximum number of points on the path used to compute the\n",
      "     |      residuals in the cross-validation\n",
      "     |  \n",
      "     |  n_jobs : integer, optional\n",
      "     |      Number of CPUs to use during the cross validation. If ``-1``, use\n",
      "     |      all the CPUs\n",
      "     |  \n",
      "     |  eps : float, optional\n",
      "     |      The machine-precision regularization in the computation of the\n",
      "     |      Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "     |      systems.\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  positive : boolean (default=False)\n",
      "     |      Restrict coefficients to be >= 0. Be aware that you might want to\n",
      "     |      remove fit_intercept which is set True by default.\n",
      "     |  \n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape (n_features,)\n",
      "     |      parameter vector (w in the formulation formula)\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      independent term in decision function\n",
      "     |  \n",
      "     |  coef_path_ : array, shape (n_features, n_alphas)\n",
      "     |      the varying values of the coefficients along the path\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      the estimated regularization parameter alpha\n",
      "     |  \n",
      "     |  alphas_ : array, shape (n_alphas,)\n",
      "     |      the different values of alpha along the path\n",
      "     |  \n",
      "     |  cv_alphas_ : array, shape (n_cv_alphas,)\n",
      "     |      all the values of alpha along the path for the different folds\n",
      "     |  \n",
      "     |  mse_path_ : array, shape (n_folds, n_cv_alphas)\n",
      "     |      the mean square error on left-out for each fold along the path\n",
      "     |      (alpha values given by ``cv_alphas``)\n",
      "     |  \n",
      "     |  n_iter_ : array-like or int\n",
      "     |      the number of iterations run by Lars with the optimal alpha.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  lars_path, LassoLars, LassoLarsCV\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LarsCV\n",
      "     |      Lars\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, fit_intercept=True, verbose=False, max_iter=500, normalize=True, precompute='auto', cv=None, max_n_alphas=1000, n_jobs=1, eps=2.2204460492503131e-16, copy_X=True, positive=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  alpha\n",
      "     |      DEPRECATED: Attribute alpha is deprecated in 0.19 and will be removed in 0.21. See ``alpha_`` instead\n",
      "     |  \n",
      "     |  cv_mse_path_\n",
      "     |      DEPRECATED: Attribute ``cv_mse_path_`` is deprecated in 0.18 and will be removed in 0.20. Use ``mse_path_`` instead\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  method = 'lar'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class Lasso(ElasticNet)\n",
      "     |  Linear Model trained with L1 prior as regularizer (aka the Lasso)\n",
      "     |  \n",
      "     |  The optimization objective for Lasso is::\n",
      "     |  \n",
      "     |      (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |  \n",
      "     |  Technically the Lasso model is optimizing the same objective function as\n",
      "     |  the Elastic Net with ``l1_ratio=1.0`` (no L2 penalty).\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <lasso>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, optional\n",
      "     |      Constant that multiplies the L1 term. Defaults to 1.0.\n",
      "     |      ``alpha = 0`` is equivalent to an ordinary least square, solved\n",
      "     |      by the :class:`LinearRegression` object. For numerical\n",
      "     |      reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.\n",
      "     |      Given this, you should use the :class:`LinearRegression` object.\n",
      "     |  \n",
      "     |  fit_intercept : boolean\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  precompute : True | False | array-like, default=False\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |      matrix can also be passed as argument. For sparse input\n",
      "     |      this option is always ``True`` to preserve sparsity.\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  max_iter : int, optional\n",
      "     |      The maximum number of iterations\n",
      "     |  \n",
      "     |  tol : float, optional\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  warm_start : bool, optional\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |  \n",
      "     |  positive : bool, optional\n",
      "     |      When set to ``True``, forces the coefficients to be positive.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional, default None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update.  If int, random_state is the seed used by the random\n",
      "     |      number generator; If RandomState instance, random_state is the random\n",
      "     |      number generator; If None, the random number generator is the\n",
      "     |      RandomState instance used by `np.random`. Used when ``selection`` ==\n",
      "     |      'random'.\n",
      "     |  \n",
      "     |  selection : str, default 'cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape (n_features,) | (n_targets, n_features)\n",
      "     |      parameter vector (w in the cost function formula)\n",
      "     |  \n",
      "     |  sparse_coef_ : scipy.sparse matrix, shape (n_features, 1) |             (n_targets, n_features)\n",
      "     |      ``sparse_coef_`` is a readonly property derived from ``coef_``\n",
      "     |  \n",
      "     |  intercept_ : float | array, shape (n_targets,)\n",
      "     |      independent term in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : int | array-like, shape (n_targets,)\n",
      "     |      number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.Lasso(alpha=0.1)\n",
      "     |  >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n",
      "     |  Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "     |     normalize=False, positive=False, precompute=False, random_state=None,\n",
      "     |     selection='cyclic', tol=0.0001, warm_start=False)\n",
      "     |  >>> print(clf.coef_)\n",
      "     |  [ 0.85  0.  ]\n",
      "     |  >>> print(clf.intercept_)\n",
      "     |  0.15\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  lars_path\n",
      "     |  lasso_path\n",
      "     |  LassoLars\n",
      "     |  LassoCV\n",
      "     |  LassoLarsCV\n",
      "     |  sklearn.decomposition.sparse_encode\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The algorithm used to fit the model is coordinate descent.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |  should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Lasso\n",
      "     |      ElasticNet\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  path = enet_path(X, y, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      "     |      Compute elastic net path with coordinate descent\n",
      "     |      \n",
      "     |      The elastic net optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n",
      "     |          + alpha * l1_ratio * ||W||_21\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like}, shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : ndarray, shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      l1_ratio : float, optional\n",
      "     |          float between 0 and 1 passed to elastic net (scaling between\n",
      "     |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso\n",
      "     |      \n",
      "     |      eps : float\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``\n",
      "     |      \n",
      "     |      n_alphas : int, optional\n",
      "     |          Number of alphas along the regularization path\n",
      "     |      \n",
      "     |      alphas : ndarray, optional\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If None alphas are set automatically\n",
      "     |      \n",
      "     |      precompute : True | False | 'auto' | array-like\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like, optional\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : boolean, optional, default True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : array, shape (n_features, ) | None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or integer\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool\n",
      "     |          whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      check_input : bool, default True\n",
      "     |          Skip input validation checks, including the Gram matrix when provided\n",
      "     |          assuming there are handled by the caller when check_input=False.\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : array, shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : array, shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : array, shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : array-like, shape (n_alphas,)\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |          (Is returned when ``return_n_iter`` is set to True).\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "     |      \n",
      "     |      See also\n",
      "     |      --------\n",
      "     |      MultiTaskElasticNet\n",
      "     |      MultiTaskElasticNetCV\n",
      "     |      ElasticNet\n",
      "     |      ElasticNetCV\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ElasticNet:\n",
      "     |  \n",
      "     |  fit(self, X, y, check_input=True)\n",
      "     |      Fit model with coordinate descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      -----------\n",
      "     |      X : ndarray or scipy.sparse matrix, (n_samples, n_features)\n",
      "     |          Data\n",
      "     |      \n",
      "     |      y : ndarray, shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target. Will be cast to X's dtype if necessary\n",
      "     |      \n",
      "     |      check_input : boolean, (default=True)\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      \n",
      "     |      Coordinate descent is an algorithm that considers each column of\n",
      "     |      data at a time hence it will automatically convert the X input\n",
      "     |      as a Fortran-contiguous numpy array if necessary.\n",
      "     |      \n",
      "     |      To avoid memory re-allocation it is advised to allocate the\n",
      "     |      initial data in memory directly using that format.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from ElasticNet:\n",
      "     |  \n",
      "     |  sparse_coef_\n",
      "     |      sparse representation of the fitted ``coef_``\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class LassoCV(LinearModelCV, sklearn.base.RegressorMixin)\n",
      "     |  Lasso linear model with iterative fitting along a regularization path\n",
      "     |  \n",
      "     |  The best model is selected by cross-validation.\n",
      "     |  \n",
      "     |  The optimization objective for Lasso is::\n",
      "     |  \n",
      "     |      (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <lasso>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  eps : float, optional\n",
      "     |      Length of the path. ``eps=1e-3`` means that\n",
      "     |      ``alpha_min / alpha_max = 1e-3``.\n",
      "     |  \n",
      "     |  n_alphas : int, optional\n",
      "     |      Number of alphas along the regularization path\n",
      "     |  \n",
      "     |  alphas : numpy array, optional\n",
      "     |      List of alphas where to compute the models.\n",
      "     |      If ``None`` alphas are set automatically\n",
      "     |  \n",
      "     |  fit_intercept : boolean, default True\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  precompute : True | False | 'auto' | array-like\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |      matrix can also be passed as argument.\n",
      "     |  \n",
      "     |  max_iter : int, optional\n",
      "     |      The maximum number of iterations\n",
      "     |  \n",
      "     |  tol : float, optional\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, optional\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 3-fold cross-validation,\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - An object to be used as a cross-validation generator.\n",
      "     |      - An iterable yielding train/test splits.\n",
      "     |  \n",
      "     |      For integer/None inputs, :class:`KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |  verbose : bool or integer\n",
      "     |      Amount of verbosity.\n",
      "     |  \n",
      "     |  n_jobs : integer, optional\n",
      "     |      Number of CPUs to use during the cross validation. If ``-1``, use\n",
      "     |      all the CPUs.\n",
      "     |  \n",
      "     |  positive : bool, optional\n",
      "     |      If positive, restrict regression coefficients to be positive\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional, default None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update.  If int, random_state is the seed used by the random\n",
      "     |      number generator; If RandomState instance, random_state is the random\n",
      "     |      number generator; If None, the random number generator is the\n",
      "     |      RandomState instance used by `np.random`. Used when ``selection`` ==\n",
      "     |      'random'.\n",
      "     |  \n",
      "     |  selection : str, default 'cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  alpha_ : float\n",
      "     |      The amount of penalization chosen by cross validation\n",
      "     |  \n",
      "     |  coef_ : array, shape (n_features,) | (n_targets, n_features)\n",
      "     |      parameter vector (w in the cost function formula)\n",
      "     |  \n",
      "     |  intercept_ : float | array, shape (n_targets,)\n",
      "     |      independent term in decision function.\n",
      "     |  \n",
      "     |  mse_path_ : array, shape (n_alphas, n_folds)\n",
      "     |      mean square error for the test set on each fold, varying alpha\n",
      "     |  \n",
      "     |  alphas_ : numpy array, shape (n_alphas,)\n",
      "     |      The grid of alphas used for fitting\n",
      "     |  \n",
      "     |  dual_gap_ : ndarray, shape ()\n",
      "     |      The dual gap at the end of the optimization for the optimal alpha\n",
      "     |      (``alpha_``).\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance for the optimal alpha.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  For an example, see\n",
      "     |  :ref:`examples/linear_model/plot_lasso_model_selection.py\n",
      "     |  <sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py>`.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |  should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  lars_path\n",
      "     |  lasso_path\n",
      "     |  LassoLars\n",
      "     |  Lasso\n",
      "     |  LassoLarsCV\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LassoCV\n",
      "     |      LinearModelCV\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize=False, precompute='auto', max_iter=1000, tol=0.0001, copy_X=True, cv=None, verbose=False, n_jobs=1, positive=False, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  path = lasso_path(X, y, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, **params)\n",
      "     |      Compute Lasso path with coordinate descent\n",
      "     |      \n",
      "     |      The Lasso optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <lasso>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : ndarray, shape (n_samples,), or (n_samples, n_outputs)\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      eps : float, optional\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``\n",
      "     |      \n",
      "     |      n_alphas : int, optional\n",
      "     |          Number of alphas along the regularization path\n",
      "     |      \n",
      "     |      alphas : ndarray, optional\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If ``None`` alphas are set automatically\n",
      "     |      \n",
      "     |      precompute : True | False | 'auto' | array-like\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like, optional\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : boolean, optional, default True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : array, shape (n_features, ) | None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or integer\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool\n",
      "     |          whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : array, shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : array, shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : array, shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : array-like, shape (n_alphas,)\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "     |      \n",
      "     |      To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |      should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |      \n",
      "     |      Note that in certain cases, the Lars solver may be significantly\n",
      "     |      faster to implement this functionality. In particular, linear\n",
      "     |      interpolation can be used to retrieve model coefficients between the\n",
      "     |      values output by lars_path\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      ---------\n",
      "     |      \n",
      "     |      Comparing lasso_path and lars_path with interpolation:\n",
      "     |      \n",
      "     |      >>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n",
      "     |      >>> y = np.array([1, 2, 3.1])\n",
      "     |      >>> # Use lasso_path to compute a coefficient path\n",
      "     |      >>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])\n",
      "     |      >>> print(coef_path)\n",
      "     |      [[ 0.          0.          0.46874778]\n",
      "     |       [ 0.2159048   0.4425765   0.23689075]]\n",
      "     |      \n",
      "     |      >>> # Now use lars_path and 1D linear interpolation to compute the\n",
      "     |      >>> # same path\n",
      "     |      >>> from sklearn.linear_model import lars_path\n",
      "     |      >>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')\n",
      "     |      >>> from scipy import interpolate\n",
      "     |      >>> coef_path_continuous = interpolate.interp1d(alphas[::-1],\n",
      "     |      ...                                             coef_path_lars[:, ::-1])\n",
      "     |      >>> print(coef_path_continuous([5., 1., .5]))\n",
      "     |      [[ 0.          0.          0.46915237]\n",
      "     |       [ 0.2159048   0.4425765   0.23668876]]\n",
      "     |      \n",
      "     |      \n",
      "     |      See also\n",
      "     |      --------\n",
      "     |      lars_path\n",
      "     |      Lasso\n",
      "     |      LassoLars\n",
      "     |      LassoCV\n",
      "     |      LassoLarsCV\n",
      "     |      sklearn.decomposition.sparse_encode\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LinearModelCV:\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit linear model with coordinate descent\n",
      "     |      \n",
      "     |      Fit is on grid of alphas and best alpha estimated by cross-validation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like}, shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data\n",
      "     |          to avoid unnecessary memory duplication. If y is mono-output,\n",
      "     |          X can be sparse.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class LassoLars(Lars)\n",
      "     |  Lasso model fit with Least Angle Regression a.k.a. Lars\n",
      "     |  \n",
      "     |  It is a Linear Model trained with an L1 prior as regularizer.\n",
      "     |  \n",
      "     |  The optimization objective for Lasso is::\n",
      "     |  \n",
      "     |  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <least_angle_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float\n",
      "     |      Constant that multiplies the penalty term. Defaults to 1.0.\n",
      "     |      ``alpha = 0`` is equivalent to an ordinary least square, solved\n",
      "     |      by :class:`LinearRegression`. For numerical reasons, using\n",
      "     |      ``alpha = 0`` with the LassoLars object is not advised and you\n",
      "     |      should prefer the LinearRegression object.\n",
      "     |  \n",
      "     |  fit_intercept : boolean\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  verbose : boolean or integer, optional\n",
      "     |      Sets the verbosity amount\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default True\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  precompute : True | False | 'auto' | array-like\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |      matrix can also be passed as argument.\n",
      "     |  \n",
      "     |  max_iter : integer, optional\n",
      "     |      Maximum number of iterations to perform.\n",
      "     |  \n",
      "     |  eps : float, optional\n",
      "     |      The machine-precision regularization in the computation of the\n",
      "     |      Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "     |      systems. Unlike the ``tol`` parameter in some iterative\n",
      "     |      optimization-based algorithms, this parameter does not control\n",
      "     |      the tolerance of the optimization.\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  fit_path : boolean\n",
      "     |      If ``True`` the full path is stored in the ``coef_path_`` attribute.\n",
      "     |      If you compute the solution for a large problem or many targets,\n",
      "     |      setting ``fit_path`` to ``False`` will lead to a speedup, especially\n",
      "     |      with a small alpha.\n",
      "     |  \n",
      "     |  positive : boolean (default=False)\n",
      "     |      Restrict coefficients to be >= 0. Be aware that you might want to\n",
      "     |      remove fit_intercept which is set True by default.\n",
      "     |      Under the positive restriction the model coefficients will not converge\n",
      "     |      to the ordinary-least-squares solution for small values of alpha.\n",
      "     |      Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\n",
      "     |      0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\n",
      "     |      algorithm are typically in congruence with the solution of the\n",
      "     |      coordinate descent Lasso estimator.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  alphas_ : array, shape (n_alphas + 1,) | list of n_targets such arrays\n",
      "     |      Maximum of covariances (in absolute value) at each iteration.         ``n_alphas`` is either ``max_iter``, ``n_features``, or the number of         nodes in the path with correlation greater than ``alpha``, whichever         is smaller.\n",
      "     |  \n",
      "     |  active_ : list, length = n_alphas | list of n_targets such lists\n",
      "     |      Indices of active variables at the end of the path.\n",
      "     |  \n",
      "     |  coef_path_ : array, shape (n_features, n_alphas + 1) or list\n",
      "     |      If a list is passed it's expected to be one of n_targets such arrays.\n",
      "     |      The varying values of the coefficients along the path. It is not\n",
      "     |      present if the ``fit_path`` parameter is ``False``.\n",
      "     |  \n",
      "     |  coef_ : array, shape (n_features,) or (n_targets, n_features)\n",
      "     |      Parameter vector (w in the formulation formula).\n",
      "     |  \n",
      "     |  intercept_ : float | array, shape (n_targets,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : array-like or int.\n",
      "     |      The number of iterations taken by lars_path to find the\n",
      "     |      grid of alphas for each target.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> reg = linear_model.LassoLars(alpha=0.01)\n",
      "     |  >>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1, 0, -1])\n",
      "     |  ... # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE\n",
      "     |  LassoLars(alpha=0.01, copy_X=True, eps=..., fit_intercept=True,\n",
      "     |       fit_path=True, max_iter=500, normalize=True, positive=False,\n",
      "     |       precompute='auto', verbose=False)\n",
      "     |  >>> print(reg.coef_) # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE\n",
      "     |  [ 0.         -0.963257...]\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  lars_path\n",
      "     |  lasso_path\n",
      "     |  Lasso\n",
      "     |  LassoCV\n",
      "     |  LassoLarsCV\n",
      "     |  sklearn.decomposition.sparse_encode\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LassoLars\n",
      "     |      Lars\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, fit_intercept=True, verbose=False, normalize=True, precompute='auto', max_iter=500, eps=2.2204460492503131e-16, copy_X=True, fit_path=True, positive=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  method = 'lasso'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Lars:\n",
      "     |  \n",
      "     |  fit(self, X, y, Xy=None)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Xy : array-like, shape (n_samples,) or (n_samples, n_targets),                 optional\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class LassoLarsCV(LarsCV)\n",
      "     |  Cross-validated Lasso, using the LARS algorithm\n",
      "     |  \n",
      "     |  The optimization objective for Lasso is::\n",
      "     |  \n",
      "     |  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <least_angle_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fit_intercept : boolean\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  verbose : boolean or integer, optional\n",
      "     |      Sets the verbosity amount\n",
      "     |  \n",
      "     |  max_iter : integer, optional\n",
      "     |      Maximum number of iterations to perform.\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default True\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  precompute : True | False | 'auto'\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram matrix\n",
      "     |      cannot be passed as argument since we will use only subsets of X.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, optional\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 3-fold cross-validation,\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - An object to be used as a cross-validation generator.\n",
      "     |      - An iterable yielding train/test splits.\n",
      "     |  \n",
      "     |      For integer/None inputs, :class:`KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |  max_n_alphas : integer, optional\n",
      "     |      The maximum number of points on the path used to compute the\n",
      "     |      residuals in the cross-validation\n",
      "     |  \n",
      "     |  n_jobs : integer, optional\n",
      "     |      Number of CPUs to use during the cross validation. If ``-1``, use\n",
      "     |      all the CPUs\n",
      "     |  \n",
      "     |  eps : float, optional\n",
      "     |      The machine-precision regularization in the computation of the\n",
      "     |      Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "     |      systems.\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  positive : boolean (default=False)\n",
      "     |      Restrict coefficients to be >= 0. Be aware that you might want to\n",
      "     |      remove fit_intercept which is set True by default.\n",
      "     |      Under the positive restriction the model coefficients do not converge\n",
      "     |      to the ordinary-least-squares solution for small values of alpha.\n",
      "     |      Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\n",
      "     |      0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\n",
      "     |      algorithm are typically in congruence with the solution of the\n",
      "     |      coordinate descent Lasso estimator.\n",
      "     |      As a consequence using LassoLarsCV only makes sense for problems where\n",
      "     |      a sparse solution is expected and/or reached.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape (n_features,)\n",
      "     |      parameter vector (w in the formulation formula)\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      independent term in decision function.\n",
      "     |  \n",
      "     |  coef_path_ : array, shape (n_features, n_alphas)\n",
      "     |      the varying values of the coefficients along the path\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      the estimated regularization parameter alpha\n",
      "     |  \n",
      "     |  alphas_ : array, shape (n_alphas,)\n",
      "     |      the different values of alpha along the path\n",
      "     |  \n",
      "     |  cv_alphas_ : array, shape (n_cv_alphas,)\n",
      "     |      all the values of alpha along the path for the different folds\n",
      "     |  \n",
      "     |  mse_path_ : array, shape (n_folds, n_cv_alphas)\n",
      "     |      the mean square error on left-out for each fold along the path\n",
      "     |      (alpha values given by ``cv_alphas``)\n",
      "     |  \n",
      "     |  n_iter_ : array-like or int\n",
      "     |      the number of iterations run by Lars with the optimal alpha.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  \n",
      "     |  The object solves the same problem as the LassoCV object. However,\n",
      "     |  unlike the LassoCV, it find the relevant alphas values by itself.\n",
      "     |  In general, because of this property, it will be more stable.\n",
      "     |  However, it is more fragile to heavily multicollinear datasets.\n",
      "     |  \n",
      "     |  It is more efficient than the LassoCV if only a small number of\n",
      "     |  features are selected compared to the total number, for instance if\n",
      "     |  there are very few samples compared to the number of features.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  lars_path, LassoLars, LarsCV, LassoCV\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LassoLarsCV\n",
      "     |      LarsCV\n",
      "     |      Lars\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, fit_intercept=True, verbose=False, max_iter=500, normalize=True, precompute='auto', cv=None, max_n_alphas=1000, n_jobs=1, eps=2.2204460492503131e-16, copy_X=True, positive=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  method = 'lasso'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LarsCV:\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LarsCV:\n",
      "     |  \n",
      "     |  alpha\n",
      "     |      DEPRECATED: Attribute alpha is deprecated in 0.19 and will be removed in 0.21. See ``alpha_`` instead\n",
      "     |  \n",
      "     |  cv_mse_path_\n",
      "     |      DEPRECATED: Attribute ``cv_mse_path_`` is deprecated in 0.18 and will be removed in 0.20. Use ``mse_path_`` instead\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class LassoLarsIC(LassoLars)\n",
      "     |  Lasso model fit with Lars using BIC or AIC for model selection\n",
      "     |  \n",
      "     |  The optimization objective for Lasso is::\n",
      "     |  \n",
      "     |  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |  \n",
      "     |  AIC is the Akaike information criterion and BIC is the Bayes\n",
      "     |  Information criterion. Such criteria are useful to select the value\n",
      "     |  of the regularization parameter by making a trade-off between the\n",
      "     |  goodness of fit and the complexity of the model. A good model should\n",
      "     |  explain well the data while being simple.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <least_angle_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  criterion : 'bic' | 'aic'\n",
      "     |      The type of criterion to use.\n",
      "     |  \n",
      "     |  fit_intercept : boolean\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  verbose : boolean or integer, optional\n",
      "     |      Sets the verbosity amount\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default True\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  precompute : True | False | 'auto' | array-like\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |      matrix can also be passed as argument.\n",
      "     |  \n",
      "     |  max_iter : integer, optional\n",
      "     |      Maximum number of iterations to perform. Can be used for\n",
      "     |      early stopping.\n",
      "     |  \n",
      "     |  eps : float, optional\n",
      "     |      The machine-precision regularization in the computation of the\n",
      "     |      Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "     |      systems. Unlike the ``tol`` parameter in some iterative\n",
      "     |      optimization-based algorithms, this parameter does not control\n",
      "     |      the tolerance of the optimization.\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  positive : boolean (default=False)\n",
      "     |      Restrict coefficients to be >= 0. Be aware that you might want to\n",
      "     |      remove fit_intercept which is set True by default.\n",
      "     |      Under the positive restriction the model coefficients do not converge\n",
      "     |      to the ordinary-least-squares solution for small values of alpha.\n",
      "     |      Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\n",
      "     |      0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\n",
      "     |      algorithm are typically in congruence with the solution of the\n",
      "     |      coordinate descent Lasso estimator.\n",
      "     |      As a consequence using LassoLarsIC only makes sense for problems where\n",
      "     |      a sparse solution is expected and/or reached.\n",
      "     |  \n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape (n_features,)\n",
      "     |      parameter vector (w in the formulation formula)\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      independent term in decision function.\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      the alpha parameter chosen by the information criterion\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      number of iterations run by lars_path to find the grid of\n",
      "     |      alphas.\n",
      "     |  \n",
      "     |  criterion_ : array, shape (n_alphas,)\n",
      "     |      The value of the information criteria ('aic', 'bic') across all\n",
      "     |      alphas. The alpha which has the smallest information criterion is\n",
      "     |      chosen. This value is larger by a factor of ``n_samples`` compared to\n",
      "     |      Eqns. 2.15 and 2.16 in (Zou et al, 2007).\n",
      "     |  \n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> reg = linear_model.LassoLarsIC(criterion='bic')\n",
      "     |  >>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1.1111, 0, -1.1111])\n",
      "     |  ... # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE\n",
      "     |  LassoLarsIC(copy_X=True, criterion='bic', eps=..., fit_intercept=True,\n",
      "     |        max_iter=500, normalize=True, positive=False, precompute='auto',\n",
      "     |        verbose=False)\n",
      "     |  >>> print(reg.coef_) # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE\n",
      "     |  [ 0.  -1.11...]\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The estimation of the number of degrees of freedom is given by:\n",
      "     |  \n",
      "     |  \"On the degrees of freedom of the lasso\"\n",
      "     |  Hui Zou, Trevor Hastie, and Robert Tibshirani\n",
      "     |  Ann. Statist. Volume 35, Number 5 (2007), 2173-2192.\n",
      "     |  \n",
      "     |  https://en.wikipedia.org/wiki/Akaike_information_criterion\n",
      "     |  https://en.wikipedia.org/wiki/Bayesian_information_criterion\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  lars_path, LassoLars, LassoLarsCV\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LassoLarsIC\n",
      "     |      LassoLars\n",
      "     |      Lars\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, criterion='aic', fit_intercept=True, verbose=False, normalize=True, precompute='auto', max_iter=500, eps=2.2204460492503131e-16, copy_X=True, positive=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, copy_X=True)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          training data.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,)\n",
      "     |          target values. Will be cast to X's dtype if necessary\n",
      "     |      \n",
      "     |      copy_X : boolean, optional, default True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from LassoLars:\n",
      "     |  \n",
      "     |  method = 'lasso'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class LinearRegression(LinearModel, sklearn.base.RegressorMixin)\n",
      "     |  Ordinary least squares Linear Regression.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fit_intercept : boolean, optional, default True\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to False, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on\n",
      "     |      an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  n_jobs : int, optional, default 1\n",
      "     |      The number of jobs to use for the computation.\n",
      "     |      If -1 all CPUs are used. This will only provide speedup for\n",
      "     |      n_targets > 1 and sufficient large problems.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape (n_features, ) or (n_targets, n_features)\n",
      "     |      Estimated coefficients for the linear regression problem.\n",
      "     |      If multiple targets are passed during the fit (y 2D), this\n",
      "     |      is a 2D array of shape (n_targets, n_features), while if only\n",
      "     |      one target is passed, this is a 1D array of length n_features.\n",
      "     |  \n",
      "     |  intercept_ : array\n",
      "     |      Independent term in the linear model.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  From the implementation point of view, this is just plain Ordinary\n",
      "     |  Least Squares (scipy.linalg.lstsq) wrapped as a predictor object.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LinearRegression\n",
      "     |      LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, fit_intercept=True, normalize=False, copy_X=True, n_jobs=1)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : numpy array or sparse matrix of shape [n_samples,n_features]\n",
      "     |          Training data\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples, n_targets]\n",
      "     |          Target values. Will be cast to X's dtype if necessary\n",
      "     |      \n",
      "     |      sample_weight : numpy array of shape [n_samples]\n",
      "     |          Individual weights for each sample\n",
      "     |      \n",
      "     |          .. versionadded:: 0.17\n",
      "     |             parameter *sample_weight* support to LinearRegression.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class Log(Classification)\n",
      "     |  Logistic regression loss for binary classification with y in {-1, 1}\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Log\n",
      "     |      Classification\n",
      "     |      LossFunction\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      helper for pickle\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __pyx_vtable__ = <capsule object NULL>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Classification:\n",
      "     |  \n",
      "     |  __setstate__ = __setstate_cython__(...)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunction:\n",
      "     |  \n",
      "     |  dloss(...)\n",
      "     |      Evaluate the derivative of the loss function with respect to\n",
      "     |      the prediction `p`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : double\n",
      "     |          The prediction, p = w^T x\n",
      "     |      y : double\n",
      "     |          The true value (aka target)\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      double\n",
      "     |          The derivative of the loss function with regards to `p`.\n",
      "    \n",
      "    class LogisticRegression(sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin, sklearn.linear_model.base.SparseCoefMixin)\n",
      "     |  Logistic Regression (aka logit, MaxEnt) classifier.\n",
      "     |  \n",
      "     |  In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
      "     |  scheme if the 'multi_class' option is set to 'ovr', and uses the cross-\n",
      "     |  entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
      "     |  (Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
      "     |  'sag' and 'newton-cg' solvers.)\n",
      "     |  \n",
      "     |  This class implements regularized logistic regression using the\n",
      "     |  'liblinear' library, 'newton-cg', 'sag' and 'lbfgs' solvers. It can handle\n",
      "     |  both dense and sparse input. Use C-ordered arrays or CSR matrices\n",
      "     |  containing 64-bit floats for optimal performance; any other input format\n",
      "     |  will be converted (and copied).\n",
      "     |  \n",
      "     |  The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
      "     |  with primal formulation. The 'liblinear' solver supports both L1 and L2\n",
      "     |  regularization, with a dual formulation only for the L2 penalty.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <logistic_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  penalty : str, 'l1' or 'l2', default: 'l2'\n",
      "     |      Used to specify the norm used in the penalization. The 'newton-cg',\n",
      "     |      'sag' and 'lbfgs' solvers support only l2 penalties.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |         l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
      "     |  \n",
      "     |  dual : bool, default: False\n",
      "     |      Dual or primal formulation. Dual formulation is only implemented for\n",
      "     |      l2 penalty with liblinear solver. Prefer dual=False when\n",
      "     |      n_samples > n_features.\n",
      "     |  \n",
      "     |  tol : float, default: 1e-4\n",
      "     |      Tolerance for stopping criteria.\n",
      "     |  \n",
      "     |  C : float, default: 1.0\n",
      "     |      Inverse of regularization strength; must be a positive float.\n",
      "     |      Like in support vector machines, smaller values specify stronger\n",
      "     |      regularization.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default: True\n",
      "     |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      "     |      added to the decision function.\n",
      "     |  \n",
      "     |  intercept_scaling : float, default 1.\n",
      "     |      Useful only when the solver 'liblinear' is used\n",
      "     |      and self.fit_intercept is set to True. In this case, x becomes\n",
      "     |      [x, self.intercept_scaling],\n",
      "     |      i.e. a \"synthetic\" feature with constant value equal to\n",
      "     |      intercept_scaling is appended to the instance vector.\n",
      "     |      The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
      "     |  \n",
      "     |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      "     |      as all other features.\n",
      "     |      To lessen the effect of regularization on synthetic feature weight\n",
      "     |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
      "     |  \n",
      "     |  class_weight : dict or 'balanced', default: None\n",
      "     |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |      If not given, all classes are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "     |  \n",
      "     |      Note that these weights will be multiplied with sample_weight (passed\n",
      "     |      through the fit method) if sample_weight is specified.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         *class_weight='balanced'*\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional, default: None\n",
      "     |      The seed of the pseudo random number generator to use when shuffling\n",
      "     |      the data.  If int, random_state is the seed used by the random number\n",
      "     |      generator; If RandomState instance, random_state is the random number\n",
      "     |      generator; If None, the random number generator is the RandomState\n",
      "     |      instance used by `np.random`. Used when ``solver`` == 'sag' or\n",
      "     |      'liblinear'.\n",
      "     |  \n",
      "     |  solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},\n",
      "     |      default: 'liblinear'\n",
      "     |      Algorithm to use in the optimization problem.\n",
      "     |  \n",
      "     |      - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n",
      "     |          'saga' are faster for large ones.\n",
      "     |      - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
      "     |          handle multinomial loss; 'liblinear' is limited to one-versus-rest\n",
      "     |          schemes.\n",
      "     |      - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas\n",
      "     |          'liblinear' and 'saga' handle L1 penalty.\n",
      "     |  \n",
      "     |      Note that 'sag' and 'saga' fast convergence is only guaranteed on\n",
      "     |      features with approximately the same scale. You can\n",
      "     |      preprocess the data with a scaler from sklearn.preprocessing.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         Stochastic Average Gradient descent solver.\n",
      "     |      .. versionadded:: 0.19\n",
      "     |         SAGA solver.\n",
      "     |  \n",
      "     |  max_iter : int, default: 100\n",
      "     |      Useful only for the newton-cg, sag and lbfgs solvers.\n",
      "     |      Maximum number of iterations taken for the solvers to converge.\n",
      "     |  \n",
      "     |  multi_class : str, {'ovr', 'multinomial'}, default: 'ovr'\n",
      "     |      Multiclass option can be either 'ovr' or 'multinomial'. If the option\n",
      "     |      chosen is 'ovr', then a binary problem is fit for each label. Else\n",
      "     |      the loss minimised is the multinomial loss fit across\n",
      "     |      the entire probability distribution. Does not work for liblinear\n",
      "     |      solver.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |         Stochastic Average Gradient descent solver for 'multinomial' case.\n",
      "     |  \n",
      "     |  verbose : int, default: 0\n",
      "     |      For the liblinear and lbfgs solvers set verbose to any positive\n",
      "     |      number for verbosity.\n",
      "     |  \n",
      "     |  warm_start : bool, default: False\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      Useless for liblinear solver.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
      "     |  \n",
      "     |  n_jobs : int, default: 1\n",
      "     |      Number of CPU cores used when parallelizing over classes if\n",
      "     |      multi_class='ovr'\". This parameter is ignored when the ``solver``is set\n",
      "     |      to 'liblinear' regardless of whether 'multi_class' is specified or\n",
      "     |      not. If given a value of -1, all cores are used.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  coef_ : array, shape (1, n_features) or (n_classes, n_features)\n",
      "     |      Coefficient of the features in the decision function.\n",
      "     |  \n",
      "     |      `coef_` is of shape (1, n_features) when the given problem\n",
      "     |      is binary.\n",
      "     |  \n",
      "     |  intercept_ : array, shape (1,) or (n_classes,)\n",
      "     |      Intercept (a.k.a. bias) added to the decision function.\n",
      "     |  \n",
      "     |      If `fit_intercept` is set to False, the intercept is set to zero.\n",
      "     |      `intercept_` is of shape(1,) when the problem is binary.\n",
      "     |  \n",
      "     |  n_iter_ : array, shape (n_classes,) or (1, )\n",
      "     |      Actual number of iterations for all classes. If binary or multinomial,\n",
      "     |      it returns only 1 element. For liblinear solver, only the maximum\n",
      "     |      number of iteration across all classes is given.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  SGDClassifier : incrementally trained logistic regression (when given\n",
      "     |      the parameter ``loss=\"log\"``).\n",
      "     |  sklearn.svm.LinearSVC : learns SVM models using the same algorithm.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The underlying C implementation uses a random number generator to\n",
      "     |  select features when fitting the model. It is thus not uncommon,\n",
      "     |  to have slightly different results for the same input data. If\n",
      "     |  that happens, try with a smaller tol parameter.\n",
      "     |  \n",
      "     |  Predict output may not match that of standalone liblinear in certain\n",
      "     |  cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
      "     |  in the narrative documentation.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  LIBLINEAR -- A Library for Large Linear Classification\n",
      "     |      http://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
      "     |  \n",
      "     |  SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
      "     |      Minimizing Finite Sums with the Stochastic Average Gradient\n",
      "     |      https://hal.inria.fr/hal-00860051/document\n",
      "     |  \n",
      "     |  SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
      "     |      SAGA: A Fast Incremental Gradient Method With Support\n",
      "     |      for Non-Strongly Convex Composite Objectives\n",
      "     |      https://arxiv.org/abs/1407.0202\n",
      "     |  \n",
      "     |  Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
      "     |      methods for logistic regression and maximum entropy models.\n",
      "     |      Machine Learning 85(1-2):41-75.\n",
      "     |      http://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LogisticRegression\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.linear_model.base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.linear_model.base.SparseCoefMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear', max_iter=100, multi_class='ovr', verbose=0, warm_start=False, n_jobs=1)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training vector, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,)\n",
      "     |          Target vector relative to X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,) optional\n",
      "     |          Array of weights that are assigned to individual samples.\n",
      "     |          If not provided, then each sample is given unit weight.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.17\n",
      "     |             *sample_weight* support to LogisticRegression.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Log of probability estimates.\n",
      "     |      \n",
      "     |      The returned estimates for all classes are ordered by the\n",
      "     |      label of classes.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = [n_samples, n_features]\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : array-like, shape = [n_samples, n_classes]\n",
      "     |          Returns the log-probability of the sample for each class in the\n",
      "     |          model, where classes are ordered as they are in ``self.classes_``.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Probability estimates.\n",
      "     |      \n",
      "     |      The returned estimates for all classes are ordered by the\n",
      "     |      label of classes.\n",
      "     |      \n",
      "     |      For a multi_class problem, if multi_class is set to be \"multinomial\"\n",
      "     |      the softmax function is used to find the predicted probability of\n",
      "     |      each class.\n",
      "     |      Else use a one-vs-rest approach, i.e calculate the probability\n",
      "     |      of each class assuming it to be positive using the logistic function.\n",
      "     |      and normalize these values across all the classes.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = [n_samples, n_features]\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : array-like, shape = [n_samples, n_classes]\n",
      "     |          Returns the probability of the sample for each class in the model,\n",
      "     |          where classes are ordered as they are in ``self.classes_``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is the signed distance of that\n",
      "     |      sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      "     |          Confidence scores per (sample, class) combination. In the binary\n",
      "     |          case, confidence score for self.classes_[1] where >0 means this\n",
      "     |          class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = [n_samples]\n",
      "     |          Predicted class label per sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator\n",
      "    \n",
      "    class LogisticRegressionCV(LogisticRegression, sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin)\n",
      "     |  Logistic Regression CV (aka logit, MaxEnt) classifier.\n",
      "     |  \n",
      "     |  This class implements logistic regression using liblinear, newton-cg, sag\n",
      "     |  of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2\n",
      "     |  regularization with primal formulation. The liblinear solver supports both\n",
      "     |  L1 and L2 regularization, with a dual formulation only for the L2 penalty.\n",
      "     |  \n",
      "     |  For the grid of Cs values (that are set by default to be ten values in\n",
      "     |  a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is\n",
      "     |  selected by the cross-validator StratifiedKFold, but it can be changed\n",
      "     |  using the cv parameter. In the case of newton-cg and lbfgs solvers,\n",
      "     |  we warm start along the path i.e guess the initial coefficients of the\n",
      "     |  present fit to be the coefficients got after convergence in the previous\n",
      "     |  fit, so it is supposed to be faster for high-dimensional dense data.\n",
      "     |  \n",
      "     |  For a multiclass problem, the hyperparameters for each class are computed\n",
      "     |  using the best scores got by doing a one-vs-rest in parallel across all\n",
      "     |  folds and classes. Hence this is not the true multinomial loss.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <logistic_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  Cs : list of floats | int\n",
      "     |      Each of the values in Cs describes the inverse of regularization\n",
      "     |      strength. If Cs is as an int, then a grid of Cs values are chosen\n",
      "     |      in a logarithmic scale between 1e-4 and 1e4.\n",
      "     |      Like in support vector machines, smaller values specify stronger\n",
      "     |      regularization.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default: True\n",
      "     |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      "     |      added to the decision function.\n",
      "     |  \n",
      "     |  cv : integer or cross-validation generator\n",
      "     |      The default cross-validation generator used is Stratified K-Folds.\n",
      "     |      If an integer is provided, then it is the number of folds used.\n",
      "     |      See the module :mod:`sklearn.model_selection` module for the\n",
      "     |      list of possible cross-validation objects.\n",
      "     |  \n",
      "     |  dual : bool\n",
      "     |      Dual or primal formulation. Dual formulation is only implemented for\n",
      "     |      l2 penalty with liblinear solver. Prefer dual=False when\n",
      "     |      n_samples > n_features.\n",
      "     |  \n",
      "     |  penalty : str, 'l1' or 'l2'\n",
      "     |      Used to specify the norm used in the penalization. The 'newton-cg',\n",
      "     |      'sag' and 'lbfgs' solvers support only l2 penalties.\n",
      "     |  \n",
      "     |  scoring : string, callable, or None\n",
      "     |      A string (see model evaluation documentation) or\n",
      "     |      a scorer callable object / function with signature\n",
      "     |      ``scorer(estimator, X, y)``. For a list of scoring functions\n",
      "     |      that can be used, look at :mod:`sklearn.metrics`. The\n",
      "     |      default scoring option used is 'accuracy'.\n",
      "     |  \n",
      "     |  solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},\n",
      "     |      default: 'lbfgs'\n",
      "     |      Algorithm to use in the optimization problem.\n",
      "     |  \n",
      "     |      - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n",
      "     |          'saga' are faster for large ones.\n",
      "     |      - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
      "     |          handle multinomial loss; 'liblinear' is limited to one-versus-rest\n",
      "     |          schemes.\n",
      "     |      - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas\n",
      "     |          'liblinear' and 'saga' handle L1 penalty.\n",
      "     |      - 'liblinear' might be slower in LogisticRegressionCV because it does\n",
      "     |          not handle warm-starting.\n",
      "     |  \n",
      "     |      Note that 'sag' and 'saga' fast convergence is only guaranteed on\n",
      "     |      features with approximately the same scale. You can preprocess the data\n",
      "     |      with a scaler from sklearn.preprocessing.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         Stochastic Average Gradient descent solver.\n",
      "     |      .. versionadded:: 0.19\n",
      "     |         SAGA solver.\n",
      "     |  \n",
      "     |  tol : float, optional\n",
      "     |      Tolerance for stopping criteria.\n",
      "     |  \n",
      "     |  max_iter : int, optional\n",
      "     |      Maximum number of iterations of the optimization algorithm.\n",
      "     |  \n",
      "     |  class_weight : dict or 'balanced', optional\n",
      "     |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |      If not given, all classes are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "     |  \n",
      "     |      Note that these weights will be multiplied with sample_weight (passed\n",
      "     |      through the fit method) if sample_weight is specified.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         class_weight == 'balanced'\n",
      "     |  \n",
      "     |  n_jobs : int, optional\n",
      "     |      Number of CPU cores used during the cross-validation loop. If given\n",
      "     |      a value of -1, all cores are used.\n",
      "     |  \n",
      "     |  verbose : int\n",
      "     |      For the 'liblinear', 'sag' and 'lbfgs' solvers set verbose to any\n",
      "     |      positive number for verbosity.\n",
      "     |  \n",
      "     |  refit : bool\n",
      "     |      If set to True, the scores are averaged across all folds, and the\n",
      "     |      coefs and the C that corresponds to the best score is taken, and a\n",
      "     |      final refit is done using these parameters.\n",
      "     |      Otherwise the coefs, intercepts and C that correspond to the\n",
      "     |      best scores across folds are averaged.\n",
      "     |  \n",
      "     |  intercept_scaling : float, default 1.\n",
      "     |      Useful only when the solver 'liblinear' is used\n",
      "     |      and self.fit_intercept is set to True. In this case, x becomes\n",
      "     |      [x, self.intercept_scaling],\n",
      "     |      i.e. a \"synthetic\" feature with constant value equal to\n",
      "     |      intercept_scaling is appended to the instance vector.\n",
      "     |      The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
      "     |  \n",
      "     |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      "     |      as all other features.\n",
      "     |      To lessen the effect of regularization on synthetic feature weight\n",
      "     |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
      "     |  \n",
      "     |  multi_class : str, {'ovr', 'multinomial'}\n",
      "     |      Multiclass option can be either 'ovr' or 'multinomial'. If the option\n",
      "     |      chosen is 'ovr', then a binary problem is fit for each label. Else\n",
      "     |      the loss minimised is the multinomial loss fit across\n",
      "     |      the entire probability distribution. Works only for the 'newton-cg',\n",
      "     |      'sag', 'saga' and 'lbfgs' solver.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |         Stochastic Average Gradient descent solver for 'multinomial' case.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional, default None\n",
      "     |      If int, random_state is the seed used by the random number generator;\n",
      "     |      If RandomState instance, random_state is the random number generator;\n",
      "     |      If None, the random number generator is the RandomState instance used\n",
      "     |      by `np.random`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape (1, n_features) or (n_classes, n_features)\n",
      "     |      Coefficient of the features in the decision function.\n",
      "     |  \n",
      "     |      `coef_` is of shape (1, n_features) when the given problem\n",
      "     |      is binary.\n",
      "     |  \n",
      "     |  intercept_ : array, shape (1,) or (n_classes,)\n",
      "     |      Intercept (a.k.a. bias) added to the decision function.\n",
      "     |  \n",
      "     |      If `fit_intercept` is set to False, the intercept is set to zero.\n",
      "     |      `intercept_` is of shape(1,) when the problem is binary.\n",
      "     |  \n",
      "     |  Cs_ : array\n",
      "     |      Array of C i.e. inverse of regularization parameter values used\n",
      "     |      for cross-validation.\n",
      "     |  \n",
      "     |  coefs_paths_ : array, shape ``(n_folds, len(Cs_), n_features)`` or                    ``(n_folds, len(Cs_), n_features + 1)``\n",
      "     |      dict with classes as the keys, and the path of coefficients obtained\n",
      "     |      during cross-validating across each fold and then across each Cs\n",
      "     |      after doing an OvR for the corresponding class as values.\n",
      "     |      If the 'multi_class' option is set to 'multinomial', then\n",
      "     |      the coefs_paths are the coefficients corresponding to each class.\n",
      "     |      Each dict value has shape ``(n_folds, len(Cs_), n_features)`` or\n",
      "     |      ``(n_folds, len(Cs_), n_features + 1)`` depending on whether the\n",
      "     |      intercept is fit or not.\n",
      "     |  \n",
      "     |  scores_ : dict\n",
      "     |      dict with classes as the keys, and the values as the\n",
      "     |      grid of scores obtained during cross-validating each fold, after doing\n",
      "     |      an OvR for the corresponding class. If the 'multi_class' option\n",
      "     |      given is 'multinomial' then the same scores are repeated across\n",
      "     |      all classes, since this is the multinomial class.\n",
      "     |      Each dict value has shape (n_folds, len(Cs))\n",
      "     |  \n",
      "     |  C_ : array, shape (n_classes,) or (n_classes - 1,)\n",
      "     |      Array of C that maps to the best scores across every class. If refit is\n",
      "     |      set to False, then for each class, the best C is the average of the\n",
      "     |      C's that correspond to the best scores for each fold.\n",
      "     |      `C_` is of shape(n_classes,) when the problem is binary.\n",
      "     |  \n",
      "     |  n_iter_ : array, shape (n_classes, n_folds, n_cs) or (1, n_folds, n_cs)\n",
      "     |      Actual number of iterations for all classes, folds and Cs.\n",
      "     |      In the binary or multinomial cases, the first dimension is equal to 1.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  LogisticRegression\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LogisticRegressionCV\n",
      "     |      LogisticRegression\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.linear_model.base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.linear_model.base.SparseCoefMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, Cs=10, fit_intercept=True, cv=None, dual=False, penalty='l2', scoring=None, solver='lbfgs', tol=0.0001, max_iter=100, class_weight=None, n_jobs=1, verbose=0, refit=True, intercept_scaling=1.0, multi_class='ovr', random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training vector, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,)\n",
      "     |          Target vector relative to X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,) optional\n",
      "     |          Array of weights that are assigned to individual samples.\n",
      "     |          If not provided, then each sample is given unit weight.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LogisticRegression:\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Log of probability estimates.\n",
      "     |      \n",
      "     |      The returned estimates for all classes are ordered by the\n",
      "     |      label of classes.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = [n_samples, n_features]\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : array-like, shape = [n_samples, n_classes]\n",
      "     |          Returns the log-probability of the sample for each class in the\n",
      "     |          model, where classes are ordered as they are in ``self.classes_``.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Probability estimates.\n",
      "     |      \n",
      "     |      The returned estimates for all classes are ordered by the\n",
      "     |      label of classes.\n",
      "     |      \n",
      "     |      For a multi_class problem, if multi_class is set to be \"multinomial\"\n",
      "     |      the softmax function is used to find the predicted probability of\n",
      "     |      each class.\n",
      "     |      Else use a one-vs-rest approach, i.e calculate the probability\n",
      "     |      of each class assuming it to be positive using the logistic function.\n",
      "     |      and normalize these values across all the classes.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = [n_samples, n_features]\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : array-like, shape = [n_samples, n_classes]\n",
      "     |          Returns the probability of the sample for each class in the model,\n",
      "     |          where classes are ordered as they are in ``self.classes_``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is the signed distance of that\n",
      "     |      sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      "     |          Confidence scores per (sample, class) combination. In the binary\n",
      "     |          case, confidence score for self.classes_[1] where >0 means this\n",
      "     |          class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = [n_samples]\n",
      "     |          Predicted class label per sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator\n",
      "    \n",
      "    class ModifiedHuber(Classification)\n",
      "     |  Modified Huber loss for binary classification with y in {-1, 1}\n",
      "     |  \n",
      "     |  This is equivalent to quadratically smoothed SVM with gamma = 2.\n",
      "     |  \n",
      "     |  See T. Zhang 'Solving Large Scale Linear Prediction Problems Using\n",
      "     |  Stochastic Gradient Descent', ICML'04.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ModifiedHuber\n",
      "     |      Classification\n",
      "     |      LossFunction\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      helper for pickle\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __pyx_vtable__ = <capsule object NULL>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Classification:\n",
      "     |  \n",
      "     |  __setstate__ = __setstate_cython__(...)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunction:\n",
      "     |  \n",
      "     |  dloss(...)\n",
      "     |      Evaluate the derivative of the loss function with respect to\n",
      "     |      the prediction `p`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : double\n",
      "     |          The prediction, p = w^T x\n",
      "     |      y : double\n",
      "     |          The true value (aka target)\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      double\n",
      "     |          The derivative of the loss function with regards to `p`.\n",
      "    \n",
      "    class MultiTaskElasticNet(Lasso)\n",
      "     |  Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer\n",
      "     |  \n",
      "     |  The optimization objective for MultiTaskElasticNet is::\n",
      "     |  \n",
      "     |      (1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n",
      "     |      + alpha * l1_ratio * ||W||_21\n",
      "     |      + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |  \n",
      "     |  Where::\n",
      "     |  \n",
      "     |      ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |  \n",
      "     |  i.e. the sum of norm of each row.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <multi_task_elastic_net>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, optional\n",
      "     |      Constant that multiplies the L1/L2 term. Defaults to 1.0\n",
      "     |  \n",
      "     |  l1_ratio : float\n",
      "     |      The ElasticNet mixing parameter, with 0 < l1_ratio <= 1.\n",
      "     |      For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it\n",
      "     |      is an L2 penalty.\n",
      "     |      For ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2.\n",
      "     |  \n",
      "     |  fit_intercept : boolean\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  max_iter : int, optional\n",
      "     |      The maximum number of iterations\n",
      "     |  \n",
      "     |  tol : float, optional\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  warm_start : bool, optional\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional, default None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update.  If int, random_state is the seed used by the random\n",
      "     |      number generator; If RandomState instance, random_state is the random\n",
      "     |      number generator; If None, the random number generator is the\n",
      "     |      RandomState instance used by `np.random`. Used when ``selection`` ==\n",
      "     |      'random'.\n",
      "     |  \n",
      "     |  selection : str, default 'cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  intercept_ : array, shape (n_tasks,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  coef_ : array, shape (n_tasks, n_features)\n",
      "     |      Parameter vector (W in the cost function formula). If a 1D y is         passed in at fit (non multi-task usage), ``coef_`` is then a 1D array.\n",
      "     |      Note that ``coef_`` stores the transpose of ``W``, ``W.T``.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.MultiTaskElasticNet(alpha=0.1)\n",
      "     |  >>> clf.fit([[0,0], [1, 1], [2, 2]], [[0, 0], [1, 1], [2, 2]])\n",
      "     |  ... #doctest: +NORMALIZE_WHITESPACE\n",
      "     |  MultiTaskElasticNet(alpha=0.1, copy_X=True, fit_intercept=True,\n",
      "     |          l1_ratio=0.5, max_iter=1000, normalize=False, random_state=None,\n",
      "     |          selection='cyclic', tol=0.0001, warm_start=False)\n",
      "     |  >>> print(clf.coef_)\n",
      "     |  [[ 0.45663524  0.45612256]\n",
      "     |   [ 0.45663524  0.45612256]]\n",
      "     |  >>> print(clf.intercept_)\n",
      "     |  [ 0.0872422  0.0872422]\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  ElasticNet, MultiTaskLasso\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The algorithm used to fit the model is coordinate descent.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |  should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultiTaskElasticNet\n",
      "     |      Lasso\n",
      "     |      ElasticNet\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, l1_ratio=0.5, fit_intercept=True, normalize=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit MultiTaskElasticNet model with coordinate descent\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      -----------\n",
      "     |      X : ndarray, shape (n_samples, n_features)\n",
      "     |          Data\n",
      "     |      y : ndarray, shape (n_samples, n_tasks)\n",
      "     |          Target. Will be cast to X's dtype if necessary\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      \n",
      "     |      Coordinate descent is an algorithm that considers each column of\n",
      "     |      data at a time hence it will automatically convert the X input\n",
      "     |      as a Fortran-contiguous numpy array if necessary.\n",
      "     |      \n",
      "     |      To avoid memory re-allocation it is advised to allocate the\n",
      "     |      initial data in memory directly using that format.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from Lasso:\n",
      "     |  \n",
      "     |  path = enet_path(X, y, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      "     |      Compute elastic net path with coordinate descent\n",
      "     |      \n",
      "     |      The elastic net optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n",
      "     |          + alpha * l1_ratio * ||W||_21\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like}, shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : ndarray, shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      l1_ratio : float, optional\n",
      "     |          float between 0 and 1 passed to elastic net (scaling between\n",
      "     |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso\n",
      "     |      \n",
      "     |      eps : float\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``\n",
      "     |      \n",
      "     |      n_alphas : int, optional\n",
      "     |          Number of alphas along the regularization path\n",
      "     |      \n",
      "     |      alphas : ndarray, optional\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If None alphas are set automatically\n",
      "     |      \n",
      "     |      precompute : True | False | 'auto' | array-like\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like, optional\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : boolean, optional, default True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : array, shape (n_features, ) | None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or integer\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool\n",
      "     |          whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      check_input : bool, default True\n",
      "     |          Skip input validation checks, including the Gram matrix when provided\n",
      "     |          assuming there are handled by the caller when check_input=False.\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : array, shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : array, shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : array, shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : array-like, shape (n_alphas,)\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |          (Is returned when ``return_n_iter`` is set to True).\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "     |      \n",
      "     |      See also\n",
      "     |      --------\n",
      "     |      MultiTaskElasticNet\n",
      "     |      MultiTaskElasticNetCV\n",
      "     |      ElasticNet\n",
      "     |      ElasticNetCV\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from ElasticNet:\n",
      "     |  \n",
      "     |  sparse_coef_\n",
      "     |      sparse representation of the fitted ``coef_``\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class MultiTaskElasticNetCV(LinearModelCV, sklearn.base.RegressorMixin)\n",
      "     |  Multi-task L1/L2 ElasticNet with built-in cross-validation.\n",
      "     |  \n",
      "     |  The optimization objective for MultiTaskElasticNet is::\n",
      "     |  \n",
      "     |      (1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n",
      "     |      + alpha * l1_ratio * ||W||_21\n",
      "     |      + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |  \n",
      "     |  Where::\n",
      "     |  \n",
      "     |      ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |  \n",
      "     |  i.e. the sum of norm of each row.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <multi_task_elastic_net>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  l1_ratio : float or array of floats\n",
      "     |      The ElasticNet mixing parameter, with 0 < l1_ratio <= 1.\n",
      "     |      For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it\n",
      "     |      is an L2 penalty.\n",
      "     |      For ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2.\n",
      "     |      This parameter can be a list, in which case the different\n",
      "     |      values are tested by cross-validation and the one giving the best\n",
      "     |      prediction score is used. Note that a good choice of list of\n",
      "     |      values for l1_ratio is often to put more values close to 1\n",
      "     |      (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7,\n",
      "     |      .9, .95, .99, 1]``\n",
      "     |  \n",
      "     |  eps : float, optional\n",
      "     |      Length of the path. ``eps=1e-3`` means that\n",
      "     |      ``alpha_min / alpha_max = 1e-3``.\n",
      "     |  \n",
      "     |  n_alphas : int, optional\n",
      "     |      Number of alphas along the regularization path\n",
      "     |  \n",
      "     |  alphas : array-like, optional\n",
      "     |      List of alphas where to compute the models.\n",
      "     |      If not provided, set automatically.\n",
      "     |  \n",
      "     |  fit_intercept : boolean\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  max_iter : int, optional\n",
      "     |      The maximum number of iterations\n",
      "     |  \n",
      "     |  tol : float, optional\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, optional\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 3-fold cross-validation,\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - An object to be used as a cross-validation generator.\n",
      "     |      - An iterable yielding train/test splits.\n",
      "     |  \n",
      "     |      For integer/None inputs, :class:`KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  verbose : bool or integer\n",
      "     |      Amount of verbosity.\n",
      "     |  \n",
      "     |  n_jobs : integer, optional\n",
      "     |      Number of CPUs to use during the cross validation. If ``-1``, use\n",
      "     |      all the CPUs. Note that this is used only if multiple values for\n",
      "     |      l1_ratio are given.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional, default None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update.  If int, random_state is the seed used by the random\n",
      "     |      number generator; If RandomState instance, random_state is the random\n",
      "     |      number generator; If None, the random number generator is the\n",
      "     |      RandomState instance used by `np.random`. Used when ``selection`` ==\n",
      "     |      'random'.\n",
      "     |  \n",
      "     |  selection : str, default 'cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  intercept_ : array, shape (n_tasks,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  coef_ : array, shape (n_tasks, n_features)\n",
      "     |      Parameter vector (W in the cost function formula).\n",
      "     |      Note that ``coef_`` stores the transpose of ``W``, ``W.T``.\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      The amount of penalization chosen by cross validation\n",
      "     |  \n",
      "     |  mse_path_ : array, shape (n_alphas, n_folds) or                 (n_l1_ratio, n_alphas, n_folds)\n",
      "     |      mean square error for the test set on each fold, varying alpha\n",
      "     |  \n",
      "     |  alphas_ : numpy array, shape (n_alphas,) or (n_l1_ratio, n_alphas)\n",
      "     |      The grid of alphas used for fitting, for each l1_ratio\n",
      "     |  \n",
      "     |  l1_ratio_ : float\n",
      "     |      best l1_ratio obtained by cross-validation.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance for the optimal alpha.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.MultiTaskElasticNetCV()\n",
      "     |  >>> clf.fit([[0,0], [1, 1], [2, 2]],\n",
      "     |  ...         [[0, 0], [1, 1], [2, 2]])\n",
      "     |  ... #doctest: +NORMALIZE_WHITESPACE\n",
      "     |  MultiTaskElasticNetCV(alphas=None, copy_X=True, cv=None, eps=0.001,\n",
      "     |         fit_intercept=True, l1_ratio=0.5, max_iter=1000, n_alphas=100,\n",
      "     |         n_jobs=1, normalize=False, random_state=None, selection='cyclic',\n",
      "     |         tol=0.0001, verbose=0)\n",
      "     |  >>> print(clf.coef_)\n",
      "     |  [[ 0.52875032  0.46958558]\n",
      "     |   [ 0.52875032  0.46958558]]\n",
      "     |  >>> print(clf.intercept_)\n",
      "     |  [ 0.00166409  0.00166409]\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  MultiTaskElasticNet\n",
      "     |  ElasticNetCV\n",
      "     |  MultiTaskLassoCV\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The algorithm used to fit the model is coordinate descent.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |  should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultiTaskElasticNetCV\n",
      "     |      LinearModelCV\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize=False, max_iter=1000, tol=0.0001, cv=None, copy_X=True, verbose=0, n_jobs=1, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  path = enet_path(X, y, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      "     |      Compute elastic net path with coordinate descent\n",
      "     |      \n",
      "     |      The elastic net optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n",
      "     |          + alpha * l1_ratio * ||W||_21\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like}, shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : ndarray, shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      l1_ratio : float, optional\n",
      "     |          float between 0 and 1 passed to elastic net (scaling between\n",
      "     |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso\n",
      "     |      \n",
      "     |      eps : float\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``\n",
      "     |      \n",
      "     |      n_alphas : int, optional\n",
      "     |          Number of alphas along the regularization path\n",
      "     |      \n",
      "     |      alphas : ndarray, optional\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If None alphas are set automatically\n",
      "     |      \n",
      "     |      precompute : True | False | 'auto' | array-like\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like, optional\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : boolean, optional, default True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : array, shape (n_features, ) | None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or integer\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool\n",
      "     |          whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      check_input : bool, default True\n",
      "     |          Skip input validation checks, including the Gram matrix when provided\n",
      "     |          assuming there are handled by the caller when check_input=False.\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : array, shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : array, shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : array, shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : array-like, shape (n_alphas,)\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |          (Is returned when ``return_n_iter`` is set to True).\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "     |      \n",
      "     |      See also\n",
      "     |      --------\n",
      "     |      MultiTaskElasticNet\n",
      "     |      MultiTaskElasticNetCV\n",
      "     |      ElasticNet\n",
      "     |      ElasticNetCV\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LinearModelCV:\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit linear model with coordinate descent\n",
      "     |      \n",
      "     |      Fit is on grid of alphas and best alpha estimated by cross-validation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like}, shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data\n",
      "     |          to avoid unnecessary memory duplication. If y is mono-output,\n",
      "     |          X can be sparse.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class MultiTaskLasso(MultiTaskElasticNet)\n",
      "     |  Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer\n",
      "     |  \n",
      "     |  The optimization objective for Lasso is::\n",
      "     |  \n",
      "     |      (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\n",
      "     |  \n",
      "     |  Where::\n",
      "     |  \n",
      "     |      ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |  \n",
      "     |  i.e. the sum of norm of each row.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <multi_task_lasso>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, optional\n",
      "     |      Constant that multiplies the L1/L2 term. Defaults to 1.0\n",
      "     |  \n",
      "     |  fit_intercept : boolean\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  max_iter : int, optional\n",
      "     |      The maximum number of iterations\n",
      "     |  \n",
      "     |  tol : float, optional\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  warm_start : bool, optional\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional, default None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update.  If int, random_state is the seed used by the random\n",
      "     |      number generator; If RandomState instance, random_state is the random\n",
      "     |      number generator; If None, the random number generator is the\n",
      "     |      RandomState instance used by `np.random`. Used when ``selection`` ==\n",
      "     |      'random'.\n",
      "     |  \n",
      "     |  selection : str, default 'cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape (n_tasks, n_features)\n",
      "     |      Parameter vector (W in the cost function formula).\n",
      "     |      Note that ``coef_`` stores the transpose of ``W``, ``W.T``.\n",
      "     |  \n",
      "     |  intercept_ : array, shape (n_tasks,)\n",
      "     |      independent term in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.MultiTaskLasso(alpha=0.1)\n",
      "     |  >>> clf.fit([[0,0], [1, 1], [2, 2]], [[0, 0], [1, 1], [2, 2]])\n",
      "     |  MultiTaskLasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "     |          normalize=False, random_state=None, selection='cyclic', tol=0.0001,\n",
      "     |          warm_start=False)\n",
      "     |  >>> print(clf.coef_)\n",
      "     |  [[ 0.89393398  0.        ]\n",
      "     |   [ 0.89393398  0.        ]]\n",
      "     |  >>> print(clf.intercept_)\n",
      "     |  [ 0.10606602  0.10606602]\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  Lasso, MultiTaskElasticNet\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The algorithm used to fit the model is coordinate descent.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |  should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultiTaskLasso\n",
      "     |      MultiTaskElasticNet\n",
      "     |      Lasso\n",
      "     |      ElasticNet\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from MultiTaskElasticNet:\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit MultiTaskElasticNet model with coordinate descent\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      -----------\n",
      "     |      X : ndarray, shape (n_samples, n_features)\n",
      "     |          Data\n",
      "     |      y : ndarray, shape (n_samples, n_tasks)\n",
      "     |          Target. Will be cast to X's dtype if necessary\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      \n",
      "     |      Coordinate descent is an algorithm that considers each column of\n",
      "     |      data at a time hence it will automatically convert the X input\n",
      "     |      as a Fortran-contiguous numpy array if necessary.\n",
      "     |      \n",
      "     |      To avoid memory re-allocation it is advised to allocate the\n",
      "     |      initial data in memory directly using that format.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from Lasso:\n",
      "     |  \n",
      "     |  path = enet_path(X, y, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      "     |      Compute elastic net path with coordinate descent\n",
      "     |      \n",
      "     |      The elastic net optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n",
      "     |          + alpha * l1_ratio * ||W||_21\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like}, shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : ndarray, shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      l1_ratio : float, optional\n",
      "     |          float between 0 and 1 passed to elastic net (scaling between\n",
      "     |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso\n",
      "     |      \n",
      "     |      eps : float\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``\n",
      "     |      \n",
      "     |      n_alphas : int, optional\n",
      "     |          Number of alphas along the regularization path\n",
      "     |      \n",
      "     |      alphas : ndarray, optional\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If None alphas are set automatically\n",
      "     |      \n",
      "     |      precompute : True | False | 'auto' | array-like\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like, optional\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : boolean, optional, default True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : array, shape (n_features, ) | None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or integer\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool\n",
      "     |          whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      check_input : bool, default True\n",
      "     |          Skip input validation checks, including the Gram matrix when provided\n",
      "     |          assuming there are handled by the caller when check_input=False.\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : array, shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : array, shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : array, shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : array-like, shape (n_alphas,)\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |          (Is returned when ``return_n_iter`` is set to True).\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "     |      \n",
      "     |      See also\n",
      "     |      --------\n",
      "     |      MultiTaskElasticNet\n",
      "     |      MultiTaskElasticNetCV\n",
      "     |      ElasticNet\n",
      "     |      ElasticNetCV\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from ElasticNet:\n",
      "     |  \n",
      "     |  sparse_coef_\n",
      "     |      sparse representation of the fitted ``coef_``\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class MultiTaskLassoCV(LinearModelCV, sklearn.base.RegressorMixin)\n",
      "     |  Multi-task L1/L2 Lasso with built-in cross-validation.\n",
      "     |  \n",
      "     |  The optimization objective for MultiTaskLasso is::\n",
      "     |  \n",
      "     |      (1 / (2 * n_samples)) * ||Y - XW||^Fro_2 + alpha * ||W||_21\n",
      "     |  \n",
      "     |  Where::\n",
      "     |  \n",
      "     |      ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |  \n",
      "     |  i.e. the sum of norm of each row.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <multi_task_lasso>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  eps : float, optional\n",
      "     |      Length of the path. ``eps=1e-3`` means that\n",
      "     |      ``alpha_min / alpha_max = 1e-3``.\n",
      "     |  \n",
      "     |  n_alphas : int, optional\n",
      "     |      Number of alphas along the regularization path\n",
      "     |  \n",
      "     |  alphas : array-like, optional\n",
      "     |      List of alphas where to compute the models.\n",
      "     |      If not provided, set automatically.\n",
      "     |  \n",
      "     |  fit_intercept : boolean\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  max_iter : int, optional\n",
      "     |      The maximum number of iterations.\n",
      "     |  \n",
      "     |  tol : float, optional\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, optional\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 3-fold cross-validation,\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - An object to be used as a cross-validation generator.\n",
      "     |      - An iterable yielding train/test splits.\n",
      "     |  \n",
      "     |      For integer/None inputs, :class:`KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |  verbose : bool or integer\n",
      "     |      Amount of verbosity.\n",
      "     |  \n",
      "     |  n_jobs : integer, optional\n",
      "     |      Number of CPUs to use during the cross validation. If ``-1``, use\n",
      "     |      all the CPUs. Note that this is used only if multiple values for\n",
      "     |      l1_ratio are given.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional, default None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update.  If int, random_state is the seed used by the random\n",
      "     |      number generator; If RandomState instance, random_state is the random\n",
      "     |      number generator; If None, the random number generator is the\n",
      "     |      RandomState instance used by `np.random`. Used when ``selection`` ==\n",
      "     |      'random'\n",
      "     |  \n",
      "     |  selection : str, default 'cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  intercept_ : array, shape (n_tasks,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  coef_ : array, shape (n_tasks, n_features)\n",
      "     |      Parameter vector (W in the cost function formula).\n",
      "     |      Note that ``coef_`` stores the transpose of ``W``, ``W.T``.\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      The amount of penalization chosen by cross validation\n",
      "     |  \n",
      "     |  mse_path_ : array, shape (n_alphas, n_folds)\n",
      "     |      mean square error for the test set on each fold, varying alpha\n",
      "     |  \n",
      "     |  alphas_ : numpy array, shape (n_alphas,)\n",
      "     |      The grid of alphas used for fitting.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance for the optimal alpha.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  MultiTaskElasticNet\n",
      "     |  ElasticNetCV\n",
      "     |  MultiTaskElasticNetCV\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The algorithm used to fit the model is coordinate descent.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |  should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultiTaskLassoCV\n",
      "     |      LinearModelCV\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize=False, max_iter=1000, tol=0.0001, copy_X=True, cv=None, verbose=False, n_jobs=1, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  path = lasso_path(X, y, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, **params)\n",
      "     |      Compute Lasso path with coordinate descent\n",
      "     |      \n",
      "     |      The Lasso optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <lasso>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : ndarray, shape (n_samples,), or (n_samples, n_outputs)\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      eps : float, optional\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``\n",
      "     |      \n",
      "     |      n_alphas : int, optional\n",
      "     |          Number of alphas along the regularization path\n",
      "     |      \n",
      "     |      alphas : ndarray, optional\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If ``None`` alphas are set automatically\n",
      "     |      \n",
      "     |      precompute : True | False | 'auto' | array-like\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like, optional\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : boolean, optional, default True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : array, shape (n_features, ) | None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or integer\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool\n",
      "     |          whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : array, shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : array, shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : array, shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : array-like, shape (n_alphas,)\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "     |      \n",
      "     |      To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |      should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |      \n",
      "     |      Note that in certain cases, the Lars solver may be significantly\n",
      "     |      faster to implement this functionality. In particular, linear\n",
      "     |      interpolation can be used to retrieve model coefficients between the\n",
      "     |      values output by lars_path\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      ---------\n",
      "     |      \n",
      "     |      Comparing lasso_path and lars_path with interpolation:\n",
      "     |      \n",
      "     |      >>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n",
      "     |      >>> y = np.array([1, 2, 3.1])\n",
      "     |      >>> # Use lasso_path to compute a coefficient path\n",
      "     |      >>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])\n",
      "     |      >>> print(coef_path)\n",
      "     |      [[ 0.          0.          0.46874778]\n",
      "     |       [ 0.2159048   0.4425765   0.23689075]]\n",
      "     |      \n",
      "     |      >>> # Now use lars_path and 1D linear interpolation to compute the\n",
      "     |      >>> # same path\n",
      "     |      >>> from sklearn.linear_model import lars_path\n",
      "     |      >>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')\n",
      "     |      >>> from scipy import interpolate\n",
      "     |      >>> coef_path_continuous = interpolate.interp1d(alphas[::-1],\n",
      "     |      ...                                             coef_path_lars[:, ::-1])\n",
      "     |      >>> print(coef_path_continuous([5., 1., .5]))\n",
      "     |      [[ 0.          0.          0.46915237]\n",
      "     |       [ 0.2159048   0.4425765   0.23668876]]\n",
      "     |      \n",
      "     |      \n",
      "     |      See also\n",
      "     |      --------\n",
      "     |      lars_path\n",
      "     |      Lasso\n",
      "     |      LassoLars\n",
      "     |      LassoCV\n",
      "     |      LassoLarsCV\n",
      "     |      sklearn.decomposition.sparse_encode\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LinearModelCV:\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit linear model with coordinate descent\n",
      "     |      \n",
      "     |      Fit is on grid of alphas and best alpha estimated by cross-validation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like}, shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data\n",
      "     |          to avoid unnecessary memory duplication. If y is mono-output,\n",
      "     |          X can be sparse.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class OrthogonalMatchingPursuit(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "     |  Orthogonal Matching Pursuit model (OMP)\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <omp>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_nonzero_coefs : int, optional\n",
      "     |      Desired number of non-zero entries in the solution. If None (by\n",
      "     |      default) this value is set to 10% of n_features.\n",
      "     |  \n",
      "     |  tol : float, optional\n",
      "     |      Maximum norm of the residual. If not None, overrides n_nonzero_coefs.\n",
      "     |  \n",
      "     |  fit_intercept : boolean, optional\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default True\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  precompute : {True, False, 'auto'}, default 'auto'\n",
      "     |      Whether to use a precomputed Gram and Xy matrix to speed up\n",
      "     |      calculations. Improves performance when `n_targets` or `n_samples` is\n",
      "     |      very large. Note that if you already have such matrices, you can pass\n",
      "     |      them directly to the fit method.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape (n_features,) or (n_targets, n_features)\n",
      "     |      parameter vector (w in the formula)\n",
      "     |  \n",
      "     |  intercept_ : float or array, shape (n_targets,)\n",
      "     |      independent term in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : int or array-like\n",
      "     |      Number of active features across every target.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang,\n",
      "     |  Matching pursuits with time-frequency dictionaries, IEEE Transactions on\n",
      "     |  Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\n",
      "     |  (http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf)\n",
      "     |  \n",
      "     |  This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\n",
      "     |  M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\n",
      "     |  Matching Pursuit Technical Report - CS Technion, April 2008.\n",
      "     |  http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  orthogonal_mp\n",
      "     |  orthogonal_mp_gram\n",
      "     |  lars_path\n",
      "     |  Lars\n",
      "     |  LassoLars\n",
      "     |  decomposition.sparse_encode\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OrthogonalMatchingPursuit\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_nonzero_coefs=None, tol=None, fit_intercept=True, normalize=True, precompute='auto')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values. Will be cast to X's dtype if necessary\n",
      "     |      \n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class OrthogonalMatchingPursuitCV(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "     |  Cross-validated Orthogonal Matching Pursuit model (OMP)\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <omp>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  copy : bool, optional\n",
      "     |      Whether the design matrix X must be copied by the algorithm. A false\n",
      "     |      value is only helpful if X is already Fortran-ordered, otherwise a\n",
      "     |      copy is made anyway.\n",
      "     |  \n",
      "     |  fit_intercept : boolean, optional\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default True\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  max_iter : integer, optional\n",
      "     |      Maximum numbers of iterations to perform, therefore maximum features\n",
      "     |      to include. 10% of ``n_features`` but at least 5 if available.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, optional\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 3-fold cross-validation,\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - An object to be used as a cross-validation generator.\n",
      "     |      - An iterable yielding train/test splits.\n",
      "     |  \n",
      "     |      For integer/None inputs, :class:`KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |  n_jobs : integer, optional\n",
      "     |      Number of CPUs to use during the cross validation. If ``-1``, use\n",
      "     |      all the CPUs\n",
      "     |  \n",
      "     |  verbose : boolean or integer, optional\n",
      "     |      Sets the verbosity amount\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  intercept_ : float or array, shape (n_targets,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  coef_ : array, shape (n_features,) or (n_targets, n_features)\n",
      "     |      Parameter vector (w in the problem formulation).\n",
      "     |  \n",
      "     |  n_nonzero_coefs_ : int\n",
      "     |      Estimated number of non-zero coefficients giving the best mean squared\n",
      "     |      error over the cross-validation folds.\n",
      "     |  \n",
      "     |  n_iter_ : int or array-like\n",
      "     |      Number of active features across every target for the model refit with\n",
      "     |      the best hyperparameters got by cross-validating across all folds.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  orthogonal_mp\n",
      "     |  orthogonal_mp_gram\n",
      "     |  lars_path\n",
      "     |  Lars\n",
      "     |  LassoLars\n",
      "     |  OrthogonalMatchingPursuit\n",
      "     |  LarsCV\n",
      "     |  LassoLarsCV\n",
      "     |  decomposition.sparse_encode\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OrthogonalMatchingPursuitCV\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, copy=True, fit_intercept=True, normalize=True, max_iter=None, cv=None, n_jobs=1, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape [n_samples, n_features]\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like, shape [n_samples]\n",
      "     |          Target values. Will be cast to X's dtype if necessary\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class PassiveAggressiveClassifier(sklearn.linear_model.stochastic_gradient.BaseSGDClassifier)\n",
      "     |  Passive Aggressive Classifier\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <passive_aggressive>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  C : float\n",
      "     |      Maximum step size (regularization). Defaults to 1.0.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=False\n",
      "     |      Whether the intercept should be estimated or not. If False, the\n",
      "     |      data is assumed to be already centered.\n",
      "     |  \n",
      "     |  max_iter : int, optional\n",
      "     |      The maximum number of passes over the training data (aka epochs).\n",
      "     |      It only impacts the behavior in the ``fit`` method, and not the\n",
      "     |      `partial_fit`.\n",
      "     |      Defaults to 5. Defaults to 1000 from 0.21, or if tol is not None.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  tol : float or None, optional\n",
      "     |      The stopping criterion. If it is not None, the iterations will stop\n",
      "     |      when (loss > previous_loss - tol). Defaults to None.\n",
      "     |      Defaults to 1e-3 from 0.21.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  shuffle : bool, default=True\n",
      "     |      Whether or not the training data should be shuffled after each epoch.\n",
      "     |  \n",
      "     |  verbose : integer, optional\n",
      "     |      The verbosity level\n",
      "     |  \n",
      "     |  loss : string, optional\n",
      "     |      The loss function to be used:\n",
      "     |      hinge: equivalent to PA-I in the reference paper.\n",
      "     |      squared_hinge: equivalent to PA-II in the reference paper.\n",
      "     |  \n",
      "     |  n_jobs : integer, optional\n",
      "     |      The number of CPUs to use to do the OVA (One Versus All, for\n",
      "     |      multi-class problems) computation. -1 means 'all CPUs'. Defaults\n",
      "     |      to 1.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional, default=None\n",
      "     |      The seed of the pseudo random number generator to use when shuffling\n",
      "     |      the data.  If int, random_state is the seed used by the random number\n",
      "     |      generator; If RandomState instance, random_state is the random number\n",
      "     |      generator; If None, the random number generator is the RandomState\n",
      "     |      instance used by `np.random`.\n",
      "     |  \n",
      "     |  warm_start : bool, optional\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |  \n",
      "     |  class_weight : dict, {class_label: weight} or \"balanced\" or None, optional\n",
      "     |      Preset for the class_weight fit parameter.\n",
      "     |  \n",
      "     |      Weights associated with classes. If not given, all classes\n",
      "     |      are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         parameter *class_weight* to automatically weight samples.\n",
      "     |  \n",
      "     |  average : bool or int, optional\n",
      "     |      When set to True, computes the averaged SGD weights and stores the\n",
      "     |      result in the ``coef_`` attribute. If set to an int greater than 1,\n",
      "     |      averaging will begin once the total number of samples seen reaches\n",
      "     |      average. So average=10 will begin averaging after seeing 10 samples.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |         parameter *average* to use weights averaging in SGD\n",
      "     |  \n",
      "     |  n_iter : int, optional\n",
      "     |      The number of passes over the training data (aka epochs).\n",
      "     |      Defaults to None. Deprecated, will be removed in 0.21.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.19\n",
      "     |          Deprecated\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]\n",
      "     |      Weights assigned to the features.\n",
      "     |  \n",
      "     |  intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      The actual number of iterations to reach the stopping criterion.\n",
      "     |      For multiclass fits, it is the maximum over every binary fit.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import PassiveAggressiveClassifier\n",
      "     |  >>> from sklearn.datasets import make_classification\n",
      "     |  >>>\n",
      "     |  >>> X, y = make_classification(n_features=4, random_state=0)\n",
      "     |  >>> clf = PassiveAggressiveClassifier(random_state=0)\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  PassiveAggressiveClassifier(C=1.0, average=False, class_weight=None,\n",
      "     |                fit_intercept=True, loss='hinge', max_iter=None, n_iter=None,\n",
      "     |                n_jobs=1, random_state=0, shuffle=True, tol=None, verbose=0,\n",
      "     |                warm_start=False)\n",
      "     |  >>> print(clf.coef_)\n",
      "     |  [[ 0.49324685  1.0552176   1.49519589  1.33798314]]\n",
      "     |  >>> print(clf.intercept_)\n",
      "     |  [ 2.18438388]\n",
      "     |  >>> print(clf.predict([[0, 0, 0, 0]]))\n",
      "     |  [1]\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  \n",
      "     |  SGDClassifier\n",
      "     |  Perceptron\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  Online Passive-Aggressive Algorithms\n",
      "     |  <http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf>\n",
      "     |  K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PassiveAggressiveClassifier\n",
      "     |      sklearn.linear_model.stochastic_gradient.BaseSGDClassifier\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.linear_model.stochastic_gradient.BaseSGD\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.linear_model.base.SparseCoefMixin\n",
      "     |      sklearn.linear_model.base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, C=1.0, fit_intercept=True, max_iter=None, tol=None, shuffle=True, verbose=0, loss='hinge', n_jobs=1, random_state=None, warm_start=False, class_weight=None, average=False, n_iter=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, coef_init=None, intercept_init=None)\n",
      "     |      Fit linear model with Passive Aggressive algorithm.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Training data\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      coef_init : array, shape = [n_classes,n_features]\n",
      "     |          The initial coefficients to warm-start the optimization.\n",
      "     |      \n",
      "     |      intercept_init : array, shape = [n_classes]\n",
      "     |          The initial intercept to warm-start the optimization.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y, classes=None)\n",
      "     |      Fit linear model with Passive Aggressive algorithm.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Subset of the training data\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Subset of the target values\n",
      "     |      \n",
      "     |      classes : array, shape = [n_classes]\n",
      "     |          Classes across all calls to partial_fit.\n",
      "     |          Can be obtained by via `np.unique(y_all)`, where y_all is the\n",
      "     |          target vector of the entire dataset.\n",
      "     |          This argument is required for the first call to partial_fit\n",
      "     |          and can be omitted in the subsequent calls.\n",
      "     |          Note that y doesn't need to contain all labels in `classes`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.linear_model.stochastic_gradient.BaseSGDClassifier:\n",
      "     |  \n",
      "     |  loss_function\n",
      "     |      DEPRECATED: Attribute loss_function was deprecated in version 0.19 and will be removed in 0.21. Use ``loss_function_`` instead\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from sklearn.linear_model.stochastic_gradient.BaseSGDClassifier:\n",
      "     |  \n",
      "     |  loss_functions = {'epsilon_insensitive': (<class 'sklearn.linear_model...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.stochastic_gradient.BaseSGD:\n",
      "     |  \n",
      "     |  set_params(self, *args, **kwargs)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is the signed distance of that\n",
      "     |      sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      "     |          Confidence scores per (sample, class) combination. In the binary\n",
      "     |          case, confidence score for self.classes_[1] where >0 means this\n",
      "     |          class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = [n_samples]\n",
      "     |          Predicted class label per sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "    \n",
      "    class PassiveAggressiveRegressor(sklearn.linear_model.stochastic_gradient.BaseSGDRegressor)\n",
      "     |  Passive Aggressive Regressor\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <passive_aggressive>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  C : float\n",
      "     |      Maximum step size (regularization). Defaults to 1.0.\n",
      "     |  \n",
      "     |  fit_intercept : bool\n",
      "     |      Whether the intercept should be estimated or not. If False, the\n",
      "     |      data is assumed to be already centered. Defaults to True.\n",
      "     |  \n",
      "     |  max_iter : int, optional\n",
      "     |      The maximum number of passes over the training data (aka epochs).\n",
      "     |      It only impacts the behavior in the ``fit`` method, and not the\n",
      "     |      `partial_fit`.\n",
      "     |      Defaults to 5. Defaults to 1000 from 0.21, or if tol is not None.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  tol : float or None, optional\n",
      "     |      The stopping criterion. If it is not None, the iterations will stop\n",
      "     |      when (loss > previous_loss - tol). Defaults to None.\n",
      "     |      Defaults to 1e-3 from 0.21.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  shuffle : bool, default=True\n",
      "     |      Whether or not the training data should be shuffled after each epoch.\n",
      "     |  \n",
      "     |  verbose : integer, optional\n",
      "     |      The verbosity level\n",
      "     |  \n",
      "     |  loss : string, optional\n",
      "     |      The loss function to be used:\n",
      "     |      epsilon_insensitive: equivalent to PA-I in the reference paper.\n",
      "     |      squared_epsilon_insensitive: equivalent to PA-II in the reference\n",
      "     |      paper.\n",
      "     |  \n",
      "     |  epsilon : float\n",
      "     |      If the difference between the current prediction and the correct label\n",
      "     |      is below this threshold, the model is not updated.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional, default=None\n",
      "     |      The seed of the pseudo random number generator to use when shuffling\n",
      "     |      the data.  If int, random_state is the seed used by the random number\n",
      "     |      generator; If RandomState instance, random_state is the random number\n",
      "     |      generator; If None, the random number generator is the RandomState\n",
      "     |      instance used by `np.random`.\n",
      "     |  \n",
      "     |  warm_start : bool, optional\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |  \n",
      "     |  average : bool or int, optional\n",
      "     |      When set to True, computes the averaged SGD weights and stores the\n",
      "     |      result in the ``coef_`` attribute. If set to an int greater than 1,\n",
      "     |      averaging will begin once the total number of samples seen reaches\n",
      "     |      average. So average=10 will begin averaging after seeing 10 samples.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |         parameter *average* to use weights averaging in SGD\n",
      "     |  \n",
      "     |  n_iter : int, optional\n",
      "     |      The number of passes over the training data (aka epochs).\n",
      "     |      Defaults to None. Deprecated, will be removed in 0.21.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.19\n",
      "     |          Deprecated\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]\n",
      "     |      Weights assigned to the features.\n",
      "     |  \n",
      "     |  intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      The actual number of iterations to reach the stopping criterion.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import PassiveAggressiveRegressor\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>>\n",
      "     |  >>> X, y = make_regression(n_features=4, random_state=0)\n",
      "     |  >>> regr = PassiveAggressiveRegressor(random_state=0)\n",
      "     |  >>> regr.fit(X, y)\n",
      "     |  PassiveAggressiveRegressor(C=1.0, average=False, epsilon=0.1,\n",
      "     |                fit_intercept=True, loss='epsilon_insensitive',\n",
      "     |                max_iter=None, n_iter=None, random_state=0, shuffle=True,\n",
      "     |                tol=None, verbose=0, warm_start=False)\n",
      "     |  >>> print(regr.coef_)\n",
      "     |  [ 20.48736655  34.18818427  67.59122734  87.94731329]\n",
      "     |  >>> print(regr.intercept_)\n",
      "     |  [-0.02306214]\n",
      "     |  >>> print(regr.predict([[0, 0, 0, 0]]))\n",
      "     |  [-0.02306214]\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  \n",
      "     |  SGDRegressor\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  Online Passive-Aggressive Algorithms\n",
      "     |  <http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf>\n",
      "     |  K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PassiveAggressiveRegressor\n",
      "     |      sklearn.linear_model.stochastic_gradient.BaseSGDRegressor\n",
      "     |      sklearn.linear_model.stochastic_gradient.BaseSGD\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.linear_model.base.SparseCoefMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, C=1.0, fit_intercept=True, max_iter=None, tol=None, shuffle=True, verbose=0, loss='epsilon_insensitive', epsilon=0.1, random_state=None, warm_start=False, average=False, n_iter=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, coef_init=None, intercept_init=None)\n",
      "     |      Fit linear model with Passive Aggressive algorithm.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Training data\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      coef_init : array, shape = [n_features]\n",
      "     |          The initial coefficients to warm-start the optimization.\n",
      "     |      \n",
      "     |      intercept_init : array, shape = [1]\n",
      "     |          The initial intercept to warm-start the optimization.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y)\n",
      "     |      Fit linear model with Passive Aggressive algorithm.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Subset of training data\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Subset of target values\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.stochastic_gradient.BaseSGDRegressor:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      array, shape (n_samples,)\n",
      "     |         Predicted target values per element in X.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from sklearn.linear_model.stochastic_gradient.BaseSGDRegressor:\n",
      "     |  \n",
      "     |  loss_functions = {'epsilon_insensitive': (<class 'sklearn.linear_model...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.stochastic_gradient.BaseSGD:\n",
      "     |  \n",
      "     |  set_params(self, *args, **kwargs)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class Perceptron(sklearn.linear_model.stochastic_gradient.BaseSGDClassifier)\n",
      "     |  Perceptron\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <perceptron>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  penalty : None, 'l2' or 'l1' or 'elasticnet'\n",
      "     |      The penalty (aka regularization term) to be used. Defaults to None.\n",
      "     |  \n",
      "     |  alpha : float\n",
      "     |      Constant that multiplies the regularization term if regularization is\n",
      "     |      used. Defaults to 0.0001\n",
      "     |  \n",
      "     |  fit_intercept : bool\n",
      "     |      Whether the intercept should be estimated or not. If False, the\n",
      "     |      data is assumed to be already centered. Defaults to True.\n",
      "     |  \n",
      "     |  max_iter : int, optional\n",
      "     |      The maximum number of passes over the training data (aka epochs).\n",
      "     |      It only impacts the behavior in the ``fit`` method, and not the\n",
      "     |      `partial_fit`.\n",
      "     |      Defaults to 5. Defaults to 1000 from 0.21, or if tol is not None.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  tol : float or None, optional\n",
      "     |      The stopping criterion. If it is not None, the iterations will stop\n",
      "     |      when (loss > previous_loss - tol). Defaults to None.\n",
      "     |      Defaults to 1e-3 from 0.21.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  shuffle : bool, optional, default True\n",
      "     |      Whether or not the training data should be shuffled after each epoch.\n",
      "     |  \n",
      "     |  verbose : integer, optional\n",
      "     |      The verbosity level\n",
      "     |  \n",
      "     |  eta0 : double\n",
      "     |      Constant by which the updates are multiplied. Defaults to 1.\n",
      "     |  \n",
      "     |  n_jobs : integer, optional\n",
      "     |      The number of CPUs to use to do the OVA (One Versus All, for\n",
      "     |      multi-class problems) computation. -1 means 'all CPUs'. Defaults\n",
      "     |      to 1.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional, default None\n",
      "     |      The seed of the pseudo random number generator to use when shuffling\n",
      "     |      the data.  If int, random_state is the seed used by the random number\n",
      "     |      generator; If RandomState instance, random_state is the random number\n",
      "     |      generator; If None, the random number generator is the RandomState\n",
      "     |      instance used by `np.random`.\n",
      "     |  \n",
      "     |  class_weight : dict, {class_label: weight} or \"balanced\" or None, optional\n",
      "     |      Preset for the class_weight fit parameter.\n",
      "     |  \n",
      "     |      Weights associated with classes. If not given, all classes\n",
      "     |      are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      "     |  \n",
      "     |  warm_start : bool, optional\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |  \n",
      "     |  n_iter : int, optional\n",
      "     |      The number of passes over the training data (aka epochs).\n",
      "     |      Defaults to None. Deprecated, will be removed in 0.21.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.19\n",
      "     |          Deprecated\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]\n",
      "     |      Weights assigned to the features.\n",
      "     |  \n",
      "     |  intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      The actual number of iterations to reach the stopping criterion.\n",
      "     |      For multiclass fits, it is the maximum over every binary fit.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  \n",
      "     |  `Perceptron` and `SGDClassifier` share the same underlying implementation.\n",
      "     |  In fact, `Perceptron()` is equivalent to `SGDClassifier(loss=\"perceptron\",\n",
      "     |  eta0=1, learning_rate=\"constant\", penalty=None)`.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  \n",
      "     |  SGDClassifier\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  https://en.wikipedia.org/wiki/Perceptron and references therein.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Perceptron\n",
      "     |      sklearn.linear_model.stochastic_gradient.BaseSGDClassifier\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.linear_model.stochastic_gradient.BaseSGD\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.linear_model.base.SparseCoefMixin\n",
      "     |      sklearn.linear_model.base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, penalty=None, alpha=0.0001, fit_intercept=True, max_iter=None, tol=None, shuffle=True, verbose=0, eta0=1.0, n_jobs=1, random_state=0, class_weight=None, warm_start=False, n_iter=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.stochastic_gradient.BaseSGDClassifier:\n",
      "     |  \n",
      "     |  fit(self, X, y, coef_init=None, intercept_init=None, sample_weight=None)\n",
      "     |      Fit linear model with Stochastic Gradient Descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training data\n",
      "     |      \n",
      "     |      y : numpy array, shape (n_samples,)\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      coef_init : array, shape (n_classes, n_features)\n",
      "     |          The initial coefficients to warm-start the optimization.\n",
      "     |      \n",
      "     |      intercept_init : array, shape (n_classes,)\n",
      "     |          The initial intercept to warm-start the optimization.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), optional\n",
      "     |          Weights applied to individual samples.\n",
      "     |          If not provided, uniform weights are assumed. These weights will\n",
      "     |          be multiplied with class_weight (passed through the\n",
      "     |          constructor) if class_weight is specified\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      "     |      Fit linear model with Stochastic Gradient Descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Subset of the training data\n",
      "     |      \n",
      "     |      y : numpy array, shape (n_samples,)\n",
      "     |          Subset of the target values\n",
      "     |      \n",
      "     |      classes : array, shape (n_classes,)\n",
      "     |          Classes across all calls to partial_fit.\n",
      "     |          Can be obtained by via `np.unique(y_all)`, where y_all is the\n",
      "     |          target vector of the entire dataset.\n",
      "     |          This argument is required for the first call to partial_fit\n",
      "     |          and can be omitted in the subsequent calls.\n",
      "     |          Note that y doesn't need to contain all labels in `classes`.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), optional\n",
      "     |          Weights applied to individual samples.\n",
      "     |          If not provided, uniform weights are assumed.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.linear_model.stochastic_gradient.BaseSGDClassifier:\n",
      "     |  \n",
      "     |  loss_function\n",
      "     |      DEPRECATED: Attribute loss_function was deprecated in version 0.19 and will be removed in 0.21. Use ``loss_function_`` instead\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from sklearn.linear_model.stochastic_gradient.BaseSGDClassifier:\n",
      "     |  \n",
      "     |  loss_functions = {'epsilon_insensitive': (<class 'sklearn.linear_model...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.stochastic_gradient.BaseSGD:\n",
      "     |  \n",
      "     |  set_params(self, *args, **kwargs)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is the signed distance of that\n",
      "     |      sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      "     |          Confidence scores per (sample, class) combination. In the binary\n",
      "     |          case, confidence score for self.classes_[1] where >0 means this\n",
      "     |          class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = [n_samples]\n",
      "     |          Predicted class label per sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "    \n",
      "    class RANSACRegressor(sklearn.base.BaseEstimator, sklearn.base.MetaEstimatorMixin, sklearn.base.RegressorMixin)\n",
      "     |  RANSAC (RANdom SAmple Consensus) algorithm.\n",
      "     |  \n",
      "     |  RANSAC is an iterative algorithm for the robust estimation of parameters\n",
      "     |  from a subset of inliers from the complete data set. More information can\n",
      "     |  be found in the general documentation of linear models.\n",
      "     |  \n",
      "     |  A detailed description of the algorithm can be found in the documentation\n",
      "     |  of the ``linear_model`` sub-package.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <ransac_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  base_estimator : object, optional\n",
      "     |      Base estimator object which implements the following methods:\n",
      "     |  \n",
      "     |       * `fit(X, y)`: Fit model to given training data and target values.\n",
      "     |       * `score(X, y)`: Returns the mean accuracy on the given test data,\n",
      "     |         which is used for the stop criterion defined by `stop_score`.\n",
      "     |         Additionally, the score is used to decide which of two equally\n",
      "     |         large consensus sets is chosen as the better one.\n",
      "     |  \n",
      "     |      If `base_estimator` is None, then\n",
      "     |      ``base_estimator=sklearn.linear_model.LinearRegression()`` is used for\n",
      "     |      target values of dtype float.\n",
      "     |  \n",
      "     |      Note that the current implementation only supports regression\n",
      "     |      estimators.\n",
      "     |  \n",
      "     |  min_samples : int (>= 1) or float ([0, 1]), optional\n",
      "     |      Minimum number of samples chosen randomly from original data. Treated\n",
      "     |      as an absolute number of samples for `min_samples >= 1`, treated as a\n",
      "     |      relative number `ceil(min_samples * X.shape[0]`) for\n",
      "     |      `min_samples < 1`. This is typically chosen as the minimal number of\n",
      "     |      samples necessary to estimate the given `base_estimator`. By default a\n",
      "     |      ``sklearn.linear_model.LinearRegression()`` estimator is assumed and\n",
      "     |      `min_samples` is chosen as ``X.shape[1] + 1``.\n",
      "     |  \n",
      "     |  residual_threshold : float, optional\n",
      "     |      Maximum residual for a data sample to be classified as an inlier.\n",
      "     |      By default the threshold is chosen as the MAD (median absolute\n",
      "     |      deviation) of the target values `y`.\n",
      "     |  \n",
      "     |  is_data_valid : callable, optional\n",
      "     |      This function is called with the randomly selected data before the\n",
      "     |      model is fitted to it: `is_data_valid(X, y)`. If its return value is\n",
      "     |      False the current randomly chosen sub-sample is skipped.\n",
      "     |  \n",
      "     |  is_model_valid : callable, optional\n",
      "     |      This function is called with the estimated model and the randomly\n",
      "     |      selected data: `is_model_valid(model, X, y)`. If its return value is\n",
      "     |      False the current randomly chosen sub-sample is skipped.\n",
      "     |      Rejecting samples with this function is computationally costlier than\n",
      "     |      with `is_data_valid`. `is_model_valid` should therefore only be used if\n",
      "     |      the estimated model is needed for making the rejection decision.\n",
      "     |  \n",
      "     |  max_trials : int, optional\n",
      "     |      Maximum number of iterations for random sample selection.\n",
      "     |  \n",
      "     |  max_skips : int, optional\n",
      "     |      Maximum number of iterations that can be skipped due to finding zero\n",
      "     |      inliers or invalid data defined by ``is_data_valid`` or invalid models\n",
      "     |      defined by ``is_model_valid``.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  stop_n_inliers : int, optional\n",
      "     |      Stop iteration if at least this number of inliers are found.\n",
      "     |  \n",
      "     |  stop_score : float, optional\n",
      "     |      Stop iteration if score is greater equal than this threshold.\n",
      "     |  \n",
      "     |  stop_probability : float in range [0, 1], optional\n",
      "     |      RANSAC iteration stops if at least one outlier-free set of the training\n",
      "     |      data is sampled in RANSAC. This requires to generate at least N\n",
      "     |      samples (iterations)::\n",
      "     |  \n",
      "     |          N >= log(1 - probability) / log(1 - e**m)\n",
      "     |  \n",
      "     |      where the probability (confidence) is typically set to high value such\n",
      "     |      as 0.99 (the default) and e is the current fraction of inliers w.r.t.\n",
      "     |      the total number of samples.\n",
      "     |  \n",
      "     |  residual_metric : callable, optional\n",
      "     |      Metric to reduce the dimensionality of the residuals to 1 for\n",
      "     |      multi-dimensional target values ``y.shape[1] > 1``. By default the sum\n",
      "     |      of absolute differences is used::\n",
      "     |  \n",
      "     |          lambda dy: np.sum(np.abs(dy), axis=1)\n",
      "     |  \n",
      "     |      .. deprecated:: 0.18\n",
      "     |         ``residual_metric`` is deprecated from 0.18 and will be removed in\n",
      "     |         0.20. Use ``loss`` instead.\n",
      "     |  \n",
      "     |  loss : string, callable, optional, default \"absolute_loss\"\n",
      "     |      String inputs, \"absolute_loss\" and \"squared_loss\" are supported which\n",
      "     |      find the absolute loss and squared loss per sample\n",
      "     |      respectively.\n",
      "     |  \n",
      "     |      If ``loss`` is a callable, then it should be a function that takes\n",
      "     |      two arrays as inputs, the true and predicted value and returns a 1-D\n",
      "     |      array with the i-th value of the array corresponding to the loss\n",
      "     |      on ``X[i]``.\n",
      "     |  \n",
      "     |      If the loss on a sample is greater than the ``residual_threshold``,\n",
      "     |      then this sample is classified as an outlier.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional, default None\n",
      "     |      The generator used to initialize the centers.  If int, random_state is\n",
      "     |      the seed used by the random number generator; If RandomState instance,\n",
      "     |      random_state is the random number generator; If None, the random number\n",
      "     |      generator is the RandomState instance used by `np.random`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimator_ : object\n",
      "     |      Best fitted model (copy of the `base_estimator` object).\n",
      "     |  \n",
      "     |  n_trials_ : int\n",
      "     |      Number of random selection trials until one of the stop criteria is\n",
      "     |      met. It is always ``<= max_trials``.\n",
      "     |  \n",
      "     |  inlier_mask_ : bool array of shape [n_samples]\n",
      "     |      Boolean mask of inliers classified as ``True``.\n",
      "     |  \n",
      "     |  n_skips_no_inliers_ : int\n",
      "     |      Number of iterations skipped due to finding zero inliers.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  n_skips_invalid_data_ : int\n",
      "     |      Number of iterations skipped due to invalid data defined by\n",
      "     |      ``is_data_valid``.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  n_skips_invalid_model_ : int\n",
      "     |      Number of iterations skipped due to an invalid model defined by\n",
      "     |      ``is_model_valid``.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] https://en.wikipedia.org/wiki/RANSAC\n",
      "     |  .. [2] http://www.cs.columbia.edu/~belhumeur/courses/compPhoto/ransac.pdf\n",
      "     |  .. [3] http://www.bmva.org/bmvc/2009/Papers/Paper355/Paper355.pdf\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RANSACRegressor\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, base_estimator=None, min_samples=None, residual_threshold=None, is_data_valid=None, is_model_valid=None, max_trials=100, max_skips=inf, stop_n_inliers=inf, stop_score=inf, stop_probability=0.99, residual_metric=None, loss='absolute_loss', random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit estimator using RANSAC algorithm.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape [n_samples, n_features]\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples] or [n_samples, n_targets]\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples]\n",
      "     |          Individual weights for each sample\n",
      "     |          raises error if sample_weight is passed and base_estimator\n",
      "     |          fit method does not support it.\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      ValueError\n",
      "     |          If no valid consensus set could be found. This occurs if\n",
      "     |          `is_data_valid` and `is_model_valid` return False for all\n",
      "     |          `max_trials` randomly chosen sub-samples.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the estimated model.\n",
      "     |      \n",
      "     |      This is a wrapper for `estimator_.predict(X)`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : numpy array of shape [n_samples, n_features]\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array, shape = [n_samples] or [n_samples, n_targets]\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  score(self, X, y)\n",
      "     |      Returns the score of the prediction.\n",
      "     |      \n",
      "     |      This is a wrapper for `estimator_.score(X, y)`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : numpy array or sparse matrix of shape [n_samples, n_features]\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array, shape = [n_samples] or [n_samples, n_targets]\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      z : float\n",
      "     |          Score of the prediction.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class RandomizedLasso(BaseRandomizedLinearModel)\n",
      "     |  Randomized Lasso.\n",
      "     |  \n",
      "     |  Randomized Lasso works by subsampling the training data and\n",
      "     |  computing a Lasso estimate where the penalty of a random subset of\n",
      "     |  coefficients has been scaled. By performing this double\n",
      "     |  randomization several times, the method assigns high scores to\n",
      "     |  features that are repeatedly selected across randomizations. This\n",
      "     |  is known as stability selection. In short, features selected more\n",
      "     |  often are considered good features.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, 'aic', or 'bic', optional\n",
      "     |      The regularization parameter alpha parameter in the Lasso.\n",
      "     |      Warning: this is not the alpha parameter in the stability selection\n",
      "     |      article which is scaling.\n",
      "     |  \n",
      "     |  scaling : float, optional\n",
      "     |      The s parameter used to randomly scale the penalty of different\n",
      "     |      features.\n",
      "     |      Should be between 0 and 1.\n",
      "     |  \n",
      "     |  sample_fraction : float, optional\n",
      "     |      The fraction of samples to be used in each randomized design.\n",
      "     |      Should be between 0 and 1. If 1, all samples are used.\n",
      "     |  \n",
      "     |  n_resampling : int, optional\n",
      "     |      Number of randomized models.\n",
      "     |  \n",
      "     |  selection_threshold : float, optional\n",
      "     |      The score above which features should be selected.\n",
      "     |  \n",
      "     |  fit_intercept : boolean, optional\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  verbose : boolean or integer, optional\n",
      "     |      Sets the verbosity amount\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default True\n",
      "     |      If True, the regressors X will be normalized before regression.\n",
      "     |      This parameter is ignored when `fit_intercept` is set to False.\n",
      "     |      When the regressors are normalized, note that this makes the\n",
      "     |      hyperparameters learned more robust and almost independent of\n",
      "     |      the number of samples. The same property is not valid for\n",
      "     |      standardized data. However, if you wish to standardize, please\n",
      "     |      use `preprocessing.StandardScaler` before calling `fit` on an\n",
      "     |      estimator with `normalize=False`.\n",
      "     |  \n",
      "     |  precompute : True | False | 'auto' | array-like\n",
      "     |      Whether to use a precomputed Gram matrix to speed up calculations.\n",
      "     |      If set to 'auto' let us decide.\n",
      "     |      The Gram matrix can also be passed as argument, but it will be used\n",
      "     |      only for the selection of parameter alpha, if alpha is 'aic' or 'bic'.\n",
      "     |  \n",
      "     |  max_iter : integer, optional\n",
      "     |      Maximum number of iterations to perform in the Lars algorithm.\n",
      "     |  \n",
      "     |  eps : float, optional\n",
      "     |      The machine-precision regularization in the computation of the\n",
      "     |      Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "     |      systems. Unlike the 'tol' parameter in some iterative\n",
      "     |      optimization-based algorithms, this parameter does not control\n",
      "     |      the tolerance of the optimization.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
      "     |      If int, random_state is the seed used by the random number generator;\n",
      "     |      If RandomState instance, random_state is the random number generator;\n",
      "     |      If None, the random number generator is the RandomState instance used\n",
      "     |      by `np.random`.\n",
      "     |  \n",
      "     |  n_jobs : integer, optional\n",
      "     |      Number of CPUs to use during the resampling. If '-1', use\n",
      "     |      all the CPUs\n",
      "     |  \n",
      "     |  pre_dispatch : int, or string, optional\n",
      "     |      Controls the number of jobs that get dispatched during parallel\n",
      "     |      execution. Reducing this number can be useful to avoid an\n",
      "     |      explosion of memory consumption when more jobs get dispatched\n",
      "     |      than CPUs can process. This parameter can be:\n",
      "     |  \n",
      "     |          - None, in which case all the jobs are immediately\n",
      "     |            created and spawned. Use this for lightweight and\n",
      "     |            fast-running jobs, to avoid delays due to on-demand\n",
      "     |            spawning of the jobs\n",
      "     |  \n",
      "     |          - An int, giving the exact number of total jobs that are\n",
      "     |            spawned\n",
      "     |  \n",
      "     |          - A string, giving an expression as a function of n_jobs,\n",
      "     |            as in '2*n_jobs'\n",
      "     |  \n",
      "     |  memory : None, str or object with the joblib.Memory interface, optional             (default=None)\n",
      "     |      Used for internal caching. By default, no caching is done.\n",
      "     |      If a string is given, it is the path to the caching directory.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  scores_ : array, shape = [n_features]\n",
      "     |      Feature scores between 0 and 1.\n",
      "     |  \n",
      "     |  all_scores_ : array, shape = [n_features, n_reg_parameter]\n",
      "     |      Feature scores between 0 and 1 for all values of the regularization         parameter. The reference article suggests ``scores_`` is the max of         ``all_scores_``.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import RandomizedLasso\n",
      "     |  >>> randomized_lasso = RandomizedLasso()\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  Stability selection\n",
      "     |  Nicolai Meinshausen, Peter Buhlmann\n",
      "     |  Journal of the Royal Statistical Society: Series B\n",
      "     |  Volume 72, Issue 4, pages 417-473, September 2010\n",
      "     |  DOI: 10.1111/j.1467-9868.2010.00740.x\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  RandomizedLogisticRegression, Lasso, ElasticNet\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RandomizedLasso\n",
      "     |      BaseRandomizedLinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.feature_selection.base.SelectorMixin\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(*args, **kwargs)\n",
      "     |      DEPRECATED: The class RandomizedLasso is deprecated in 0.19 and will be removed in 0.21.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseRandomizedLinearModel:\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = [n_samples, n_features]\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples]\n",
      "     |          Target values. Will be cast to X's dtype if necessary\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |             Returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.feature_selection.base.SelectorMixin:\n",
      "     |  \n",
      "     |  get_support(self, indices=False)\n",
      "     |      Get a mask, or integer index, of the features selected\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      indices : boolean (default False)\n",
      "     |          If True, the return value will be an array of integers, rather\n",
      "     |          than a boolean mask.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      support : array\n",
      "     |          An index that selects the retained features from a feature vector.\n",
      "     |          If `indices` is False, this is a boolean array of shape\n",
      "     |          [# input features], in which an element is True iff its\n",
      "     |          corresponding feature is selected for retention. If `indices` is\n",
      "     |          True, this is an integer array of shape [# output features] whose\n",
      "     |          values are indices into the input feature vector.\n",
      "     |  \n",
      "     |  inverse_transform(self, X)\n",
      "     |      Reverse the transformation operation\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array of shape [n_samples, n_selected_features]\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_r : array of shape [n_samples, n_original_features]\n",
      "     |          `X` with columns of zeros inserted where features would have\n",
      "     |          been removed by `transform`.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Reduce X to the selected features.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array of shape [n_samples, n_features]\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_r : array of shape [n_samples, n_selected_features]\n",
      "     |          The input samples with only the selected features.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to X and y with optional parameters fit_params\n",
      "     |      and returns a transformed version of X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : numpy array of shape [n_samples, n_features]\n",
      "     |          Training set.\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : numpy array of shape [n_samples, n_features_new]\n",
      "     |          Transformed array.\n",
      "    \n",
      "    class RandomizedLogisticRegression(BaseRandomizedLinearModel)\n",
      "     |  Randomized Logistic Regression\n",
      "     |  \n",
      "     |  Randomized Logistic Regression works by subsampling the training\n",
      "     |  data and fitting a L1-penalized LogisticRegression model where the\n",
      "     |  penalty of a random subset of coefficients has been scaled. By\n",
      "     |  performing this double randomization several times, the method\n",
      "     |  assigns high scores to features that are repeatedly selected across\n",
      "     |  randomizations. This is known as stability selection. In short,\n",
      "     |  features selected more often are considered good features.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  C : float or array-like of shape [n_reg_parameter], optional, default=1\n",
      "     |      The regularization parameter C in the LogisticRegression.\n",
      "     |      When C is an array, fit will take each regularization parameter in C\n",
      "     |      one by one for LogisticRegression and store results for each one\n",
      "     |      in ``all_scores_``, where columns and rows represent corresponding\n",
      "     |      reg_parameters and features.\n",
      "     |  \n",
      "     |  scaling : float, optional, default=0.5\n",
      "     |      The s parameter used to randomly scale the penalty of different\n",
      "     |      features.\n",
      "     |      Should be between 0 and 1.\n",
      "     |  \n",
      "     |  sample_fraction : float, optional, default=0.75\n",
      "     |      The fraction of samples to be used in each randomized design.\n",
      "     |      Should be between 0 and 1. If 1, all samples are used.\n",
      "     |  \n",
      "     |  n_resampling : int, optional, default=200\n",
      "     |      Number of randomized models.\n",
      "     |  \n",
      "     |  selection_threshold : float, optional, default=0.25\n",
      "     |      The score above which features should be selected.\n",
      "     |  \n",
      "     |  tol : float, optional, default=1e-3\n",
      "     |       tolerance for stopping criteria of LogisticRegression\n",
      "     |  \n",
      "     |  fit_intercept : boolean, optional, default=True\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  verbose : boolean or integer, optional\n",
      "     |      Sets the verbosity amount\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default True\n",
      "     |      If True, the regressors X will be normalized before regression.\n",
      "     |      This parameter is ignored when `fit_intercept` is set to False.\n",
      "     |      When the regressors are normalized, note that this makes the\n",
      "     |      hyperparameters learnt more robust and almost independent of the number\n",
      "     |      of samples. The same property is not valid for standardized data.\n",
      "     |      However, if you wish to standardize, please use\n",
      "     |      `preprocessing.StandardScaler` before calling `fit` on an estimator\n",
      "     |      with `normalize=False`.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
      "     |      If int, random_state is the seed used by the random number generator;\n",
      "     |      If RandomState instance, random_state is the random number generator;\n",
      "     |      If None, the random number generator is the RandomState instance used\n",
      "     |      by `np.random`.\n",
      "     |  \n",
      "     |  n_jobs : integer, optional\n",
      "     |      Number of CPUs to use during the resampling. If '-1', use\n",
      "     |      all the CPUs\n",
      "     |  \n",
      "     |  pre_dispatch : int, or string, optional\n",
      "     |      Controls the number of jobs that get dispatched during parallel\n",
      "     |      execution. Reducing this number can be useful to avoid an\n",
      "     |      explosion of memory consumption when more jobs get dispatched\n",
      "     |      than CPUs can process. This parameter can be:\n",
      "     |  \n",
      "     |          - None, in which case all the jobs are immediately\n",
      "     |            created and spawned. Use this for lightweight and\n",
      "     |            fast-running jobs, to avoid delays due to on-demand\n",
      "     |            spawning of the jobs\n",
      "     |  \n",
      "     |          - An int, giving the exact number of total jobs that are\n",
      "     |            spawned\n",
      "     |  \n",
      "     |          - A string, giving an expression as a function of n_jobs,\n",
      "     |            as in '2*n_jobs'\n",
      "     |  \n",
      "     |  memory : None, str or object with the joblib.Memory interface, optional             (default=None)\n",
      "     |      Used for internal caching. By default, no caching is done.\n",
      "     |      If a string is given, it is the path to the caching directory.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  scores_ : array, shape = [n_features]\n",
      "     |      Feature scores between 0 and 1.\n",
      "     |  \n",
      "     |  all_scores_ : array, shape = [n_features, n_reg_parameter]\n",
      "     |      Feature scores between 0 and 1 for all values of the regularization         parameter. The reference article suggests ``scores_`` is the max         of ``all_scores_``.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import RandomizedLogisticRegression\n",
      "     |  >>> randomized_logistic = RandomizedLogisticRegression()\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  Stability selection\n",
      "     |  Nicolai Meinshausen, Peter Buhlmann\n",
      "     |  Journal of the Royal Statistical Society: Series B\n",
      "     |  Volume 72, Issue 4, pages 417-473, September 2010\n",
      "     |  DOI: 10.1111/j.1467-9868.2010.00740.x\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  RandomizedLasso, LogisticRegression\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RandomizedLogisticRegression\n",
      "     |      BaseRandomizedLinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.feature_selection.base.SelectorMixin\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(*args, **kwargs)\n",
      "     |      DEPRECATED: The class RandomizedLogisticRegression is deprecated in 0.19 and will be removed in 0.21.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseRandomizedLinearModel:\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = [n_samples, n_features]\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples]\n",
      "     |          Target values. Will be cast to X's dtype if necessary\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |             Returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.feature_selection.base.SelectorMixin:\n",
      "     |  \n",
      "     |  get_support(self, indices=False)\n",
      "     |      Get a mask, or integer index, of the features selected\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      indices : boolean (default False)\n",
      "     |          If True, the return value will be an array of integers, rather\n",
      "     |          than a boolean mask.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      support : array\n",
      "     |          An index that selects the retained features from a feature vector.\n",
      "     |          If `indices` is False, this is a boolean array of shape\n",
      "     |          [# input features], in which an element is True iff its\n",
      "     |          corresponding feature is selected for retention. If `indices` is\n",
      "     |          True, this is an integer array of shape [# output features] whose\n",
      "     |          values are indices into the input feature vector.\n",
      "     |  \n",
      "     |  inverse_transform(self, X)\n",
      "     |      Reverse the transformation operation\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array of shape [n_samples, n_selected_features]\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_r : array of shape [n_samples, n_original_features]\n",
      "     |          `X` with columns of zeros inserted where features would have\n",
      "     |          been removed by `transform`.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Reduce X to the selected features.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array of shape [n_samples, n_features]\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_r : array of shape [n_samples, n_selected_features]\n",
      "     |          The input samples with only the selected features.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to X and y with optional parameters fit_params\n",
      "     |      and returns a transformed version of X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : numpy array of shape [n_samples, n_features]\n",
      "     |          Training set.\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : numpy array of shape [n_samples, n_features_new]\n",
      "     |          Transformed array.\n",
      "    \n",
      "    class Ridge(_BaseRidge, sklearn.base.RegressorMixin)\n",
      "     |  Linear least squares with l2 regularization.\n",
      "     |  \n",
      "     |  This model solves a regression model where the loss function is\n",
      "     |  the linear least squares function and regularization is given by\n",
      "     |  the l2-norm. Also known as Ridge Regression or Tikhonov regularization.\n",
      "     |  This estimator has built-in support for multi-variate regression\n",
      "     |  (i.e., when y is a 2d-array of shape [n_samples, n_targets]).\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <ridge_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : {float, array-like}, shape (n_targets)\n",
      "     |      Regularization strength; must be a positive float. Regularization\n",
      "     |      improves the conditioning of the problem and reduces the variance of\n",
      "     |      the estimates. Larger values specify stronger regularization.\n",
      "     |      Alpha corresponds to ``C^-1`` in other linear models such as\n",
      "     |      LogisticRegression or LinearSVC. If an array is passed, penalties are\n",
      "     |      assumed to be specific to the targets. Hence they must correspond in\n",
      "     |      number.\n",
      "     |  \n",
      "     |  fit_intercept : boolean\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  max_iter : int, optional\n",
      "     |      Maximum number of iterations for conjugate gradient solver.\n",
      "     |      For 'sparse_cg' and 'lsqr' solvers, the default value is determined\n",
      "     |      by scipy.sparse.linalg. For 'sag' solver, the default value is 1000.\n",
      "     |  \n",
      "     |  tol : float\n",
      "     |      Precision of the solution.\n",
      "     |  \n",
      "     |  solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'}\n",
      "     |      Solver to use in the computational routines:\n",
      "     |  \n",
      "     |      - 'auto' chooses the solver automatically based on the type of data.\n",
      "     |  \n",
      "     |      - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n",
      "     |        coefficients. More stable for singular matrices than\n",
      "     |        'cholesky'.\n",
      "     |  \n",
      "     |      - 'cholesky' uses the standard scipy.linalg.solve function to\n",
      "     |        obtain a closed-form solution.\n",
      "     |  \n",
      "     |      - 'sparse_cg' uses the conjugate gradient solver as found in\n",
      "     |        scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n",
      "     |        more appropriate than 'cholesky' for large-scale data\n",
      "     |        (possibility to set `tol` and `max_iter`).\n",
      "     |  \n",
      "     |      - 'lsqr' uses the dedicated regularized least-squares routine\n",
      "     |        scipy.sparse.linalg.lsqr. It is the fastest but may not be available\n",
      "     |        in old scipy versions. It also uses an iterative procedure.\n",
      "     |  \n",
      "     |      - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n",
      "     |        its improved, unbiased version named SAGA. Both methods also use an\n",
      "     |        iterative procedure, and are often faster than other solvers when\n",
      "     |        both n_samples and n_features are large. Note that 'sag' and\n",
      "     |        'saga' fast convergence is only guaranteed on features with\n",
      "     |        approximately the same scale. You can preprocess the data with a\n",
      "     |        scaler from sklearn.preprocessing.\n",
      "     |  \n",
      "     |      All last five solvers support both dense and sparse data. However,\n",
      "     |      only 'sag' and 'saga' supports sparse input when `fit_intercept` is\n",
      "     |      True.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         Stochastic Average Gradient descent solver.\n",
      "     |      .. versionadded:: 0.19\n",
      "     |         SAGA solver.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional, default None\n",
      "     |      The seed of the pseudo random number generator to use when shuffling\n",
      "     |      the data.  If int, random_state is the seed used by the random number\n",
      "     |      generator; If RandomState instance, random_state is the random number\n",
      "     |      generator; If None, the random number generator is the RandomState\n",
      "     |      instance used by `np.random`. Used when ``solver`` == 'sag'.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         *random_state* to support Stochastic Average Gradient.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape (n_features,) or (n_targets, n_features)\n",
      "     |      Weight vector(s).\n",
      "     |  \n",
      "     |  intercept_ : float | array, shape = (n_targets,)\n",
      "     |      Independent term in decision function. Set to 0.0 if\n",
      "     |      ``fit_intercept = False``.\n",
      "     |  \n",
      "     |  n_iter_ : array or None, shape (n_targets,)\n",
      "     |      Actual number of iterations for each target. Available only for\n",
      "     |      sag and lsqr solvers. Other solvers will return None.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  RidgeClassifier, RidgeCV, :class:`sklearn.kernel_ridge.KernelRidge`\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import Ridge\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> n_samples, n_features = 10, 5\n",
      "     |  >>> np.random.seed(0)\n",
      "     |  >>> y = np.random.randn(n_samples)\n",
      "     |  >>> X = np.random.randn(n_samples, n_features)\n",
      "     |  >>> clf = Ridge(alpha=1.0)\n",
      "     |  >>> clf.fit(X, y) # doctest: +NORMALIZE_WHITESPACE\n",
      "     |  Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "     |        normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Ridge\n",
      "     |      _BaseRidge\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver='auto', random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit Ridge regression model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Training data\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples] or [n_samples, n_targets]\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      sample_weight : float or numpy array of shape [n_samples]\n",
      "     |          Individual weights for each sample\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class RidgeCV(_BaseRidgeCV, sklearn.base.RegressorMixin)\n",
      "     |  Ridge regression with built-in cross-validation.\n",
      "     |  \n",
      "     |  By default, it performs Generalized Cross-Validation, which is a form of\n",
      "     |  efficient Leave-One-Out cross-validation.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <ridge_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alphas : numpy array of shape [n_alphas]\n",
      "     |      Array of alpha values to try.\n",
      "     |      Regularization strength; must be a positive float. Regularization\n",
      "     |      improves the conditioning of the problem and reduces the variance of\n",
      "     |      the estimates. Larger values specify stronger regularization.\n",
      "     |      Alpha corresponds to ``C^-1`` in other linear models such as\n",
      "     |      LogisticRegression or LinearSVC.\n",
      "     |  \n",
      "     |  fit_intercept : boolean\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  scoring : string, callable or None, optional, default: None\n",
      "     |      A string (see model evaluation documentation) or\n",
      "     |      a scorer callable object / function with signature\n",
      "     |      ``scorer(estimator, X, y)``.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, optional\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the efficient Leave-One-Out cross-validation\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - An object to be used as a cross-validation generator.\n",
      "     |      - An iterable yielding train/test splits.\n",
      "     |  \n",
      "     |      For integer/None inputs, if ``y`` is binary or multiclass,\n",
      "     |      :class:`sklearn.model_selection.StratifiedKFold` is used, else,\n",
      "     |      :class:`sklearn.model_selection.KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |  gcv_mode : {None, 'auto', 'svd', eigen'}, optional\n",
      "     |      Flag indicating which strategy to use when performing\n",
      "     |      Generalized Cross-Validation. Options are::\n",
      "     |  \n",
      "     |          'auto' : use svd if n_samples > n_features or when X is a sparse\n",
      "     |                   matrix, otherwise use eigen\n",
      "     |          'svd' : force computation via singular value decomposition of X\n",
      "     |                  (does not work for sparse matrices)\n",
      "     |          'eigen' : force computation via eigendecomposition of X^T X\n",
      "     |  \n",
      "     |      The 'auto' mode is the default and is intended to pick the cheaper\n",
      "     |      option of the two depending upon the shape and format of the training\n",
      "     |      data.\n",
      "     |  \n",
      "     |  store_cv_values : boolean, default=False\n",
      "     |      Flag indicating if the cross-validation values corresponding to\n",
      "     |      each alpha should be stored in the `cv_values_` attribute (see\n",
      "     |      below). This flag is only compatible with `cv=None` (i.e. using\n",
      "     |      Generalized Cross-Validation).\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  cv_values_ : array, shape = [n_samples, n_alphas] or         shape = [n_samples, n_targets, n_alphas], optional\n",
      "     |      Cross-validation values for each alpha (if `store_cv_values=True` and         `cv=None`). After `fit()` has been called, this attribute will         contain the mean squared errors (by default) or the values of the         `{loss,score}_func` function (if provided in the constructor).\n",
      "     |  \n",
      "     |  coef_ : array, shape = [n_features] or [n_targets, n_features]\n",
      "     |      Weight vector(s).\n",
      "     |  \n",
      "     |  intercept_ : float | array, shape = (n_targets,)\n",
      "     |      Independent term in decision function. Set to 0.0 if\n",
      "     |      ``fit_intercept = False``.\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      Estimated regularization parameter.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  Ridge: Ridge regression\n",
      "     |  RidgeClassifier: Ridge classifier\n",
      "     |  RidgeClassifierCV: Ridge classifier with built-in cross validation\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RidgeCV\n",
      "     |      _BaseRidgeCV\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseRidgeCV:\n",
      "     |  \n",
      "     |  __init__(self, alphas=(0.1, 1.0, 10.0), fit_intercept=True, normalize=False, scoring=None, cv=None, gcv_mode=None, store_cv_values=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit Ridge regression model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = [n_samples, n_features]\n",
      "     |          Training data\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples] or [n_samples, n_targets]\n",
      "     |          Target values. Will be cast to X's dtype if necessary\n",
      "     |      \n",
      "     |      sample_weight : float or array-like of shape [n_samples]\n",
      "     |          Sample weight\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : Returns self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class RidgeClassifier(sklearn.linear_model.base.LinearClassifierMixin, _BaseRidge)\n",
      "     |  Classifier using Ridge regression.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <ridge_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float\n",
      "     |      Regularization strength; must be a positive float. Regularization\n",
      "     |      improves the conditioning of the problem and reduces the variance of\n",
      "     |      the estimates. Larger values specify stronger regularization.\n",
      "     |      Alpha corresponds to ``C^-1`` in other linear models such as\n",
      "     |      LogisticRegression or LinearSVC.\n",
      "     |  \n",
      "     |  fit_intercept : boolean\n",
      "     |      Whether to calculate the intercept for this model. If set to false, no\n",
      "     |      intercept will be used in calculations (e.g. data is expected to be\n",
      "     |      already centered).\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  max_iter : int, optional\n",
      "     |      Maximum number of iterations for conjugate gradient solver.\n",
      "     |      The default value is determined by scipy.sparse.linalg.\n",
      "     |  \n",
      "     |  tol : float\n",
      "     |      Precision of the solution.\n",
      "     |  \n",
      "     |  class_weight : dict or 'balanced', optional\n",
      "     |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |      If not given, all classes are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      "     |  \n",
      "     |  solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'}\n",
      "     |      Solver to use in the computational routines:\n",
      "     |  \n",
      "     |      - 'auto' chooses the solver automatically based on the type of data.\n",
      "     |  \n",
      "     |      - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n",
      "     |        coefficients. More stable for singular matrices than\n",
      "     |        'cholesky'.\n",
      "     |  \n",
      "     |      - 'cholesky' uses the standard scipy.linalg.solve function to\n",
      "     |        obtain a closed-form solution.\n",
      "     |  \n",
      "     |      - 'sparse_cg' uses the conjugate gradient solver as found in\n",
      "     |        scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n",
      "     |        more appropriate than 'cholesky' for large-scale data\n",
      "     |        (possibility to set `tol` and `max_iter`).\n",
      "     |  \n",
      "     |      - 'lsqr' uses the dedicated regularized least-squares routine\n",
      "     |        scipy.sparse.linalg.lsqr. It is the fastest but may not be available\n",
      "     |        in old scipy versions. It also uses an iterative procedure.\n",
      "     |  \n",
      "     |      - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n",
      "     |        its unbiased and more flexible version named SAGA. Both methods\n",
      "     |        use an iterative procedure, and are often faster than other solvers\n",
      "     |        when both n_samples and n_features are large. Note that 'sag' and\n",
      "     |        'saga' fast convergence is only guaranteed on features with\n",
      "     |        approximately the same scale. You can preprocess the data with a\n",
      "     |        scaler from sklearn.preprocessing.\n",
      "     |  \n",
      "     |        .. versionadded:: 0.17\n",
      "     |           Stochastic Average Gradient descent solver.\n",
      "     |        .. versionadded:: 0.19\n",
      "     |         SAGA solver.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional, default None\n",
      "     |      The seed of the pseudo random number generator to use when shuffling\n",
      "     |      the data.  If int, random_state is the seed used by the random number\n",
      "     |      generator; If RandomState instance, random_state is the random number\n",
      "     |      generator; If None, the random number generator is the RandomState\n",
      "     |      instance used by `np.random`. Used when ``solver`` == 'sag'.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape (n_features,) or (n_classes, n_features)\n",
      "     |      Weight vector(s).\n",
      "     |  \n",
      "     |  intercept_ : float | array, shape = (n_targets,)\n",
      "     |      Independent term in decision function. Set to 0.0 if\n",
      "     |      ``fit_intercept = False``.\n",
      "     |  \n",
      "     |  n_iter_ : array or None, shape (n_targets,)\n",
      "     |      Actual number of iterations for each target. Available only for\n",
      "     |      sag and lsqr solvers. Other solvers will return None.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  Ridge, RidgeClassifierCV\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  For multi-class classification, n_class classifiers are trained in\n",
      "     |  a one-versus-all approach. Concretely, this is implemented by taking\n",
      "     |  advantage of the multi-variate response support in Ridge.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RidgeClassifier\n",
      "     |      sklearn.linear_model.base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      _BaseRidge\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, class_weight=None, solver='auto', random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit Ridge regression model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples,n_features]\n",
      "     |          Training data\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples]\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      sample_weight : float or numpy array of shape (n_samples,)\n",
      "     |          Sample weight.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.17\n",
      "     |             *sample_weight* support to Classifier.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  classes_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is the signed distance of that\n",
      "     |      sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      "     |          Confidence scores per (sample, class) combination. In the binary\n",
      "     |          case, confidence score for self.classes_[1] where >0 means this\n",
      "     |          class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = [n_samples]\n",
      "     |          Predicted class label per sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "    \n",
      "    class RidgeClassifierCV(sklearn.linear_model.base.LinearClassifierMixin, _BaseRidgeCV)\n",
      "     |  Ridge classifier with built-in cross-validation.\n",
      "     |  \n",
      "     |  By default, it performs Generalized Cross-Validation, which is a form of\n",
      "     |  efficient Leave-One-Out cross-validation. Currently, only the n_features >\n",
      "     |  n_samples case is handled efficiently.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <ridge_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alphas : numpy array of shape [n_alphas]\n",
      "     |      Array of alpha values to try.\n",
      "     |      Regularization strength; must be a positive float. Regularization\n",
      "     |      improves the conditioning of the problem and reduces the variance of\n",
      "     |      the estimates. Larger values specify stronger regularization.\n",
      "     |      Alpha corresponds to ``C^-1`` in other linear models such as\n",
      "     |      LogisticRegression or LinearSVC.\n",
      "     |  \n",
      "     |  fit_intercept : boolean\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      "     |      If True, the regressors X will be normalized before regression by\n",
      "     |      subtracting the mean and dividing by the l2-norm.\n",
      "     |      If you wish to standardize, please use\n",
      "     |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      "     |      on an estimator with ``normalize=False``.\n",
      "     |  \n",
      "     |  scoring : string, callable or None, optional, default: None\n",
      "     |      A string (see model evaluation documentation) or\n",
      "     |      a scorer callable object / function with signature\n",
      "     |      ``scorer(estimator, X, y)``.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, optional\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the efficient Leave-One-Out cross-validation\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - An object to be used as a cross-validation generator.\n",
      "     |      - An iterable yielding train/test splits.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |  class_weight : dict or 'balanced', optional\n",
      "     |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |      If not given, all classes are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  cv_values_ : array, shape = [n_samples, n_alphas] or     shape = [n_samples, n_responses, n_alphas], optional\n",
      "     |      Cross-validation values for each alpha (if `store_cv_values=True` and\n",
      "     |  `cv=None`). After `fit()` has been called, this attribute will contain     the mean squared errors (by default) or the values of the     `{loss,score}_func` function (if provided in the constructor).\n",
      "     |  \n",
      "     |  coef_ : array, shape = [n_features] or [n_targets, n_features]\n",
      "     |      Weight vector(s).\n",
      "     |  \n",
      "     |  intercept_ : float | array, shape = (n_targets,)\n",
      "     |      Independent term in decision function. Set to 0.0 if\n",
      "     |      ``fit_intercept = False``.\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      Estimated regularization parameter\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  Ridge: Ridge regression\n",
      "     |  RidgeClassifier: Ridge classifier\n",
      "     |  RidgeCV: Ridge regression with built-in cross validation\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  For multi-class classification, n_class classifiers are trained in\n",
      "     |  a one-versus-all approach. Concretely, this is implemented by taking\n",
      "     |  advantage of the multi-variate response support in Ridge.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RidgeClassifierCV\n",
      "     |      sklearn.linear_model.base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      _BaseRidgeCV\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alphas=(0.1, 1.0, 10.0), fit_intercept=True, normalize=False, scoring=None, cv=None, class_weight=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the ridge classifier.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Training vectors, where n_samples is the number of samples\n",
      "     |          and n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,)\n",
      "     |          Target values. Will be cast to X's dtype if necessary\n",
      "     |      \n",
      "     |      sample_weight : float or numpy array of shape (n_samples,)\n",
      "     |          Sample weight.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  classes_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is the signed distance of that\n",
      "     |      sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      "     |          Confidence scores per (sample, class) combination. In the binary\n",
      "     |          case, confidence score for self.classes_[1] where >0 means this\n",
      "     |          class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = [n_samples]\n",
      "     |          Predicted class label per sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "    \n",
      "    class SGDClassifier(BaseSGDClassifier)\n",
      "     |  Linear classifiers (SVM, logistic regression, a.o.) with SGD training.\n",
      "     |  \n",
      "     |  This estimator implements regularized linear models with stochastic\n",
      "     |  gradient descent (SGD) learning: the gradient of the loss is estimated\n",
      "     |  each sample at a time and the model is updated along the way with a\n",
      "     |  decreasing strength schedule (aka learning rate). SGD allows minibatch\n",
      "     |  (online/out-of-core) learning, see the partial_fit method.\n",
      "     |  For best results using the default learning rate schedule, the data should\n",
      "     |  have zero mean and unit variance.\n",
      "     |  \n",
      "     |  This implementation works with data represented as dense or sparse arrays\n",
      "     |  of floating point values for the features. The model it fits can be\n",
      "     |  controlled with the loss parameter; by default, it fits a linear support\n",
      "     |  vector machine (SVM).\n",
      "     |  \n",
      "     |  The regularizer is a penalty added to the loss function that shrinks model\n",
      "     |  parameters towards the zero vector using either the squared euclidean norm\n",
      "     |  L2 or the absolute norm L1 or a combination of both (Elastic Net). If the\n",
      "     |  parameter update crosses the 0.0 value because of the regularizer, the\n",
      "     |  update is truncated to 0.0 to allow for learning sparse models and achieve\n",
      "     |  online feature selection.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <sgd>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  loss : str, default: 'hinge'\n",
      "     |      The loss function to be used. Defaults to 'hinge', which gives a\n",
      "     |      linear SVM.\n",
      "     |  \n",
      "     |      The possible options are 'hinge', 'log', 'modified_huber',\n",
      "     |      'squared_hinge', 'perceptron', or a regression loss: 'squared_loss',\n",
      "     |      'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n",
      "     |  \n",
      "     |      The 'log' loss gives logistic regression, a probabilistic classifier.\n",
      "     |      'modified_huber' is another smooth loss that brings tolerance to\n",
      "     |      outliers as well as probability estimates.\n",
      "     |      'squared_hinge' is like hinge but is quadratically penalized.\n",
      "     |      'perceptron' is the linear loss used by the perceptron algorithm.\n",
      "     |      The other losses are designed for regression but can be useful in\n",
      "     |      classification as well; see SGDRegressor for a description.\n",
      "     |  \n",
      "     |  penalty : str, 'none', 'l2', 'l1', or 'elasticnet'\n",
      "     |      The penalty (aka regularization term) to be used. Defaults to 'l2'\n",
      "     |      which is the standard regularizer for linear SVM models. 'l1' and\n",
      "     |      'elasticnet' might bring sparsity to the model (feature selection)\n",
      "     |      not achievable with 'l2'.\n",
      "     |  \n",
      "     |  alpha : float\n",
      "     |      Constant that multiplies the regularization term. Defaults to 0.0001\n",
      "     |      Also used to compute learning_rate when set to 'optimal'.\n",
      "     |  \n",
      "     |  l1_ratio : float\n",
      "     |      The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\n",
      "     |      l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\n",
      "     |      Defaults to 0.15.\n",
      "     |  \n",
      "     |  fit_intercept : bool\n",
      "     |      Whether the intercept should be estimated or not. If False, the\n",
      "     |      data is assumed to be already centered. Defaults to True.\n",
      "     |  \n",
      "     |  max_iter : int, optional\n",
      "     |      The maximum number of passes over the training data (aka epochs).\n",
      "     |      It only impacts the behavior in the ``fit`` method, and not the\n",
      "     |      `partial_fit`.\n",
      "     |      Defaults to 5. Defaults to 1000 from 0.21, or if tol is not None.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  tol : float or None, optional\n",
      "     |      The stopping criterion. If it is not None, the iterations will stop\n",
      "     |      when (loss > previous_loss - tol). Defaults to None.\n",
      "     |      Defaults to 1e-3 from 0.21.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  shuffle : bool, optional\n",
      "     |      Whether or not the training data should be shuffled after each epoch.\n",
      "     |      Defaults to True.\n",
      "     |  \n",
      "     |  verbose : integer, optional\n",
      "     |      The verbosity level\n",
      "     |  \n",
      "     |  epsilon : float\n",
      "     |      Epsilon in the epsilon-insensitive loss functions; only if `loss` is\n",
      "     |      'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n",
      "     |      For 'huber', determines the threshold at which it becomes less\n",
      "     |      important to get the prediction exactly right.\n",
      "     |      For epsilon-insensitive, any differences between the current prediction\n",
      "     |      and the correct label are ignored if they are less than this threshold.\n",
      "     |  \n",
      "     |  n_jobs : integer, optional\n",
      "     |      The number of CPUs to use to do the OVA (One Versus All, for\n",
      "     |      multi-class problems) computation. -1 means 'all CPUs'. Defaults\n",
      "     |      to 1.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
      "     |      The seed of the pseudo random number generator to use when shuffling\n",
      "     |      the data.  If int, random_state is the seed used by the random number\n",
      "     |      generator; If RandomState instance, random_state is the random number\n",
      "     |      generator; If None, the random number generator is the RandomState\n",
      "     |      instance used by `np.random`.\n",
      "     |  \n",
      "     |  learning_rate : string, optional\n",
      "     |      The learning rate schedule:\n",
      "     |  \n",
      "     |      - 'constant': eta = eta0\n",
      "     |      - 'optimal': eta = 1.0 / (alpha * (t + t0)) [default]\n",
      "     |      - 'invscaling': eta = eta0 / pow(t, power_t)\n",
      "     |  \n",
      "     |      where t0 is chosen by a heuristic proposed by Leon Bottou.\n",
      "     |  \n",
      "     |  eta0 : double\n",
      "     |      The initial learning rate for the 'constant' or 'invscaling'\n",
      "     |      schedules. The default value is 0.0 as eta0 is not used by the\n",
      "     |      default schedule 'optimal'.\n",
      "     |  \n",
      "     |  power_t : double\n",
      "     |      The exponent for inverse scaling learning rate [default 0.5].\n",
      "     |  \n",
      "     |  class_weight : dict, {class_label: weight} or \"balanced\" or None, optional\n",
      "     |      Preset for the class_weight fit parameter.\n",
      "     |  \n",
      "     |      Weights associated with classes. If not given, all classes\n",
      "     |      are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      "     |  \n",
      "     |  warm_start : bool, optional\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |  \n",
      "     |  average : bool or int, optional\n",
      "     |      When set to True, computes the averaged SGD weights and stores the\n",
      "     |      result in the ``coef_`` attribute. If set to an int greater than 1,\n",
      "     |      averaging will begin once the total number of samples seen reaches\n",
      "     |      average. So ``average=10`` will begin averaging after seeing 10\n",
      "     |      samples.\n",
      "     |  \n",
      "     |  n_iter : int, optional\n",
      "     |      The number of passes over the training data (aka epochs).\n",
      "     |      Defaults to None. Deprecated, will be removed in 0.21.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.19\n",
      "     |          Deprecated\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape (1, n_features) if n_classes == 2 else (n_classes,            n_features)\n",
      "     |      Weights assigned to the features.\n",
      "     |  \n",
      "     |  intercept_ : array, shape (1,) if n_classes == 2 else (n_classes,)\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      The actual number of iterations to reach the stopping criterion.\n",
      "     |      For multiclass fits, it is the maximum over every binary fit.\n",
      "     |  \n",
      "     |  loss_function_ : concrete ``LossFunction``\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
      "     |  >>> Y = np.array([1, 1, 2, 2])\n",
      "     |  >>> clf = linear_model.SGDClassifier()\n",
      "     |  >>> clf.fit(X, Y)\n",
      "     |  ... #doctest: +NORMALIZE_WHITESPACE\n",
      "     |  SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "     |         eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "     |         learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
      "     |         n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
      "     |         shuffle=True, tol=None, verbose=0, warm_start=False)\n",
      "     |  \n",
      "     |  >>> print(clf.predict([[-0.8, -1]]))\n",
      "     |  [1]\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  LinearSVC, LogisticRegression, Perceptron\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SGDClassifier\n",
      "     |      BaseSGDClassifier\n",
      "     |      abc.NewBase\n",
      "     |      BaseSGD\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.linear_model.base.SparseCoefMixin\n",
      "     |      sklearn.linear_model.base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, loss='hinge', penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, class_weight=None, warm_start=False, average=False, n_iter=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  predict_log_proba\n",
      "     |      Log of probability estimates.\n",
      "     |      \n",
      "     |      This method is only available for log loss and modified Huber loss.\n",
      "     |      \n",
      "     |      When loss=\"modified_huber\", probability estimates may be hard zeros\n",
      "     |      and ones, so taking the logarithm is not possible.\n",
      "     |      \n",
      "     |      See ``predict_proba`` for details.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : array-like, shape (n_samples, n_classes)\n",
      "     |          Returns the log-probability of the sample for each class in the\n",
      "     |          model, where classes are ordered as they are in\n",
      "     |          `self.classes_`.\n",
      "     |  \n",
      "     |  predict_proba\n",
      "     |      Probability estimates.\n",
      "     |      \n",
      "     |      This method is only available for log loss and modified Huber loss.\n",
      "     |      \n",
      "     |      Multiclass probability estimates are derived from binary (one-vs.-rest)\n",
      "     |      estimates by simple normalization, as recommended by Zadrozny and\n",
      "     |      Elkan.\n",
      "     |      \n",
      "     |      Binary probability estimates for loss=\"modified_huber\" are given by\n",
      "     |      (clip(decision_function(X), -1, 1) + 1) / 2. For other loss functions\n",
      "     |      it is necessary to perform proper probability calibration by wrapping\n",
      "     |      the classifier with\n",
      "     |      :class:`sklearn.calibration.CalibratedClassifierCV` instead.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      array, shape (n_samples, n_classes)\n",
      "     |          Returns the probability of the sample for each class in the model,\n",
      "     |          where classes are ordered as they are in `self.classes_`.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      Zadrozny and Elkan, \"Transforming classifier scores into multiclass\n",
      "     |      probability estimates\", SIGKDD'02,\n",
      "     |      http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf\n",
      "     |      \n",
      "     |      The justification for the formula in the loss=\"modified_huber\"\n",
      "     |      case is in the appendix B in:\n",
      "     |      http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseSGDClassifier:\n",
      "     |  \n",
      "     |  fit(self, X, y, coef_init=None, intercept_init=None, sample_weight=None)\n",
      "     |      Fit linear model with Stochastic Gradient Descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training data\n",
      "     |      \n",
      "     |      y : numpy array, shape (n_samples,)\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      coef_init : array, shape (n_classes, n_features)\n",
      "     |          The initial coefficients to warm-start the optimization.\n",
      "     |      \n",
      "     |      intercept_init : array, shape (n_classes,)\n",
      "     |          The initial intercept to warm-start the optimization.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), optional\n",
      "     |          Weights applied to individual samples.\n",
      "     |          If not provided, uniform weights are assumed. These weights will\n",
      "     |          be multiplied with class_weight (passed through the\n",
      "     |          constructor) if class_weight is specified\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      "     |      Fit linear model with Stochastic Gradient Descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Subset of the training data\n",
      "     |      \n",
      "     |      y : numpy array, shape (n_samples,)\n",
      "     |          Subset of the target values\n",
      "     |      \n",
      "     |      classes : array, shape (n_classes,)\n",
      "     |          Classes across all calls to partial_fit.\n",
      "     |          Can be obtained by via `np.unique(y_all)`, where y_all is the\n",
      "     |          target vector of the entire dataset.\n",
      "     |          This argument is required for the first call to partial_fit\n",
      "     |          and can be omitted in the subsequent calls.\n",
      "     |          Note that y doesn't need to contain all labels in `classes`.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), optional\n",
      "     |          Weights applied to individual samples.\n",
      "     |          If not provided, uniform weights are assumed.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseSGDClassifier:\n",
      "     |  \n",
      "     |  loss_function\n",
      "     |      DEPRECATED: Attribute loss_function was deprecated in version 0.19 and will be removed in 0.21. Use ``loss_function_`` instead\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from BaseSGDClassifier:\n",
      "     |  \n",
      "     |  loss_functions = {'epsilon_insensitive': (<class 'sklearn.linear_model...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseSGD:\n",
      "     |  \n",
      "     |  set_params(self, *args, **kwargs)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is the signed distance of that\n",
      "     |      sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      "     |          Confidence scores per (sample, class) combination. In the binary\n",
      "     |          case, confidence score for self.classes_[1] where >0 means this\n",
      "     |          class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = [n_samples]\n",
      "     |          Predicted class label per sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "    \n",
      "    class SGDRegressor(BaseSGDRegressor)\n",
      "     |  Linear model fitted by minimizing a regularized empirical loss with SGD\n",
      "     |  \n",
      "     |  SGD stands for Stochastic Gradient Descent: the gradient of the loss is\n",
      "     |  estimated each sample at a time and the model is updated along the way with\n",
      "     |  a decreasing strength schedule (aka learning rate).\n",
      "     |  \n",
      "     |  The regularizer is a penalty added to the loss function that shrinks model\n",
      "     |  parameters towards the zero vector using either the squared euclidean norm\n",
      "     |  L2 or the absolute norm L1 or a combination of both (Elastic Net). If the\n",
      "     |  parameter update crosses the 0.0 value because of the regularizer, the\n",
      "     |  update is truncated to 0.0 to allow for learning sparse models and achieve\n",
      "     |  online feature selection.\n",
      "     |  \n",
      "     |  This implementation works with data represented as dense numpy arrays of\n",
      "     |  floating point values for the features.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <sgd>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  loss : str, default: 'squared_loss'\n",
      "     |      The loss function to be used. The possible values are 'squared_loss',\n",
      "     |      'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'\n",
      "     |  \n",
      "     |      The 'squared_loss' refers to the ordinary least squares fit.\n",
      "     |      'huber' modifies 'squared_loss' to focus less on getting outliers\n",
      "     |      correct by switching from squared to linear loss past a distance of\n",
      "     |      epsilon. 'epsilon_insensitive' ignores errors less than epsilon and is\n",
      "     |      linear past that; this is the loss function used in SVR.\n",
      "     |      'squared_epsilon_insensitive' is the same but becomes squared loss past\n",
      "     |      a tolerance of epsilon.\n",
      "     |  \n",
      "     |  penalty : str, 'none', 'l2', 'l1', or 'elasticnet'\n",
      "     |      The penalty (aka regularization term) to be used. Defaults to 'l2'\n",
      "     |      which is the standard regularizer for linear SVM models. 'l1' and\n",
      "     |      'elasticnet' might bring sparsity to the model (feature selection)\n",
      "     |      not achievable with 'l2'.\n",
      "     |  \n",
      "     |  alpha : float\n",
      "     |      Constant that multiplies the regularization term. Defaults to 0.0001\n",
      "     |      Also used to compute learning_rate when set to 'optimal'.\n",
      "     |  \n",
      "     |  l1_ratio : float\n",
      "     |      The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\n",
      "     |      l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\n",
      "     |      Defaults to 0.15.\n",
      "     |  \n",
      "     |  fit_intercept : bool\n",
      "     |      Whether the intercept should be estimated or not. If False, the\n",
      "     |      data is assumed to be already centered. Defaults to True.\n",
      "     |  \n",
      "     |  max_iter : int, optional\n",
      "     |      The maximum number of passes over the training data (aka epochs).\n",
      "     |      It only impacts the behavior in the ``fit`` method, and not the\n",
      "     |      `partial_fit`.\n",
      "     |      Defaults to 5. Defaults to 1000 from 0.21, or if tol is not None.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  tol : float or None, optional\n",
      "     |      The stopping criterion. If it is not None, the iterations will stop\n",
      "     |      when (loss > previous_loss - tol). Defaults to None.\n",
      "     |      Defaults to 1e-3 from 0.21.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  shuffle : bool, optional\n",
      "     |      Whether or not the training data should be shuffled after each epoch.\n",
      "     |      Defaults to True.\n",
      "     |  \n",
      "     |  verbose : integer, optional\n",
      "     |      The verbosity level.\n",
      "     |  \n",
      "     |  epsilon : float\n",
      "     |      Epsilon in the epsilon-insensitive loss functions; only if `loss` is\n",
      "     |      'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n",
      "     |      For 'huber', determines the threshold at which it becomes less\n",
      "     |      important to get the prediction exactly right.\n",
      "     |      For epsilon-insensitive, any differences between the current prediction\n",
      "     |      and the correct label are ignored if they are less than this threshold.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
      "     |      The seed of the pseudo random number generator to use when shuffling\n",
      "     |      the data.  If int, random_state is the seed used by the random number\n",
      "     |      generator; If RandomState instance, random_state is the random number\n",
      "     |      generator; If None, the random number generator is the RandomState\n",
      "     |      instance used by `np.random`.\n",
      "     |  \n",
      "     |  learning_rate : string, optional\n",
      "     |      The learning rate schedule:\n",
      "     |  \n",
      "     |      - 'constant': eta = eta0\n",
      "     |      - 'optimal': eta = 1.0 / (alpha * (t + t0)) [default]\n",
      "     |      - 'invscaling': eta = eta0 / pow(t, power_t)\n",
      "     |  \n",
      "     |      where t0 is chosen by a heuristic proposed by Leon Bottou.\n",
      "     |  \n",
      "     |  eta0 : double, optional\n",
      "     |      The initial learning rate [default 0.01].\n",
      "     |  \n",
      "     |  power_t : double, optional\n",
      "     |      The exponent for inverse scaling learning rate [default 0.25].\n",
      "     |  \n",
      "     |  warm_start : bool, optional\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |  \n",
      "     |  average : bool or int, optional\n",
      "     |      When set to True, computes the averaged SGD weights and stores the\n",
      "     |      result in the ``coef_`` attribute. If set to an int greater than 1,\n",
      "     |      averaging will begin once the total number of samples seen reaches\n",
      "     |      average. So ``average=10`` will begin averaging after seeing 10\n",
      "     |      samples.\n",
      "     |  \n",
      "     |  n_iter : int, optional\n",
      "     |      The number of passes over the training data (aka epochs).\n",
      "     |      Defaults to None. Deprecated, will be removed in 0.21.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.19\n",
      "     |          Deprecated\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape (n_features,)\n",
      "     |      Weights assigned to the features.\n",
      "     |  \n",
      "     |  intercept_ : array, shape (1,)\n",
      "     |      The intercept term.\n",
      "     |  \n",
      "     |  average_coef_ : array, shape (n_features,)\n",
      "     |      Averaged weights assigned to the features.\n",
      "     |  \n",
      "     |  average_intercept_ : array, shape (1,)\n",
      "     |      The averaged intercept term.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      The actual number of iterations to reach the stopping criterion.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> n_samples, n_features = 10, 5\n",
      "     |  >>> np.random.seed(0)\n",
      "     |  >>> y = np.random.randn(n_samples)\n",
      "     |  >>> X = np.random.randn(n_samples, n_features)\n",
      "     |  >>> clf = linear_model.SGDRegressor()\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  ... #doctest: +NORMALIZE_WHITESPACE\n",
      "     |  SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,\n",
      "     |         fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',\n",
      "     |         loss='squared_loss', max_iter=None, n_iter=None, penalty='l2',\n",
      "     |         power_t=0.25, random_state=None, shuffle=True, tol=None,\n",
      "     |         verbose=0, warm_start=False)\n",
      "     |  \n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  Ridge, ElasticNet, Lasso, SVR\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SGDRegressor\n",
      "     |      BaseSGDRegressor\n",
      "     |      BaseSGD\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.linear_model.base.SparseCoefMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, loss='squared_loss', penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, shuffle=True, verbose=0, epsilon=0.1, random_state=None, learning_rate='invscaling', eta0=0.01, power_t=0.25, warm_start=False, average=False, n_iter=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseSGDRegressor:\n",
      "     |  \n",
      "     |  fit(self, X, y, coef_init=None, intercept_init=None, sample_weight=None)\n",
      "     |      Fit linear model with Stochastic Gradient Descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training data\n",
      "     |      \n",
      "     |      y : numpy array, shape (n_samples,)\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      coef_init : array, shape (n_features,)\n",
      "     |          The initial coefficients to warm-start the optimization.\n",
      "     |      \n",
      "     |      intercept_init : array, shape (1,)\n",
      "     |          The initial intercept to warm-start the optimization.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), optional\n",
      "     |          Weights applied to individual samples (1. for unweighted).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y, sample_weight=None)\n",
      "     |      Fit linear model with Stochastic Gradient Descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Subset of training data\n",
      "     |      \n",
      "     |      y : numpy array of shape (n_samples,)\n",
      "     |          Subset of target values\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), optional\n",
      "     |          Weights applied to individual samples.\n",
      "     |          If not provided, uniform weights are assumed.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      array, shape (n_samples,)\n",
      "     |         Predicted target values per element in X.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from BaseSGDRegressor:\n",
      "     |  \n",
      "     |  loss_functions = {'epsilon_insensitive': (<class 'sklearn.linear_model...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseSGD:\n",
      "     |  \n",
      "     |  set_params(self, *args, **kwargs)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class SquaredLoss(Regression)\n",
      "     |  Squared loss traditional used in linear regression.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SquaredLoss\n",
      "     |      Regression\n",
      "     |      LossFunction\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      helper for pickle\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __pyx_vtable__ = <capsule object NULL>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Regression:\n",
      "     |  \n",
      "     |  __setstate__ = __setstate_cython__(...)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunction:\n",
      "     |  \n",
      "     |  dloss(...)\n",
      "     |      Evaluate the derivative of the loss function with respect to\n",
      "     |      the prediction `p`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : double\n",
      "     |          The prediction, p = w^T x\n",
      "     |      y : double\n",
      "     |          The true value (aka target)\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      double\n",
      "     |          The derivative of the loss function with regards to `p`.\n",
      "    \n",
      "    class TheilSenRegressor(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "     |  Theil-Sen Estimator: robust multivariate regression model.\n",
      "     |  \n",
      "     |  The algorithm calculates least square solutions on subsets with size\n",
      "     |  n_subsamples of the samples in X. Any value of n_subsamples between the\n",
      "     |  number of features and samples leads to an estimator with a compromise\n",
      "     |  between robustness and efficiency. Since the number of least square\n",
      "     |  solutions is \"n_samples choose n_subsamples\", it can be extremely large\n",
      "     |  and can therefore be limited with max_subpopulation. If this limit is\n",
      "     |  reached, the subsets are chosen randomly. In a final step, the spatial\n",
      "     |  median (or L1 median) is calculated of all least square solutions.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <theil_sen_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fit_intercept : boolean, optional, default True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations.\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  max_subpopulation : int, optional, default 1e4\n",
      "     |      Instead of computing with a set of cardinality 'n choose k', where n is\n",
      "     |      the number of samples and k is the number of subsamples (at least\n",
      "     |      number of features), consider only a stochastic subpopulation of a\n",
      "     |      given maximal size if 'n choose k' is larger than max_subpopulation.\n",
      "     |      For other than small problem sizes this parameter will determine\n",
      "     |      memory usage and runtime if n_subsamples is not changed.\n",
      "     |  \n",
      "     |  n_subsamples : int, optional, default None\n",
      "     |      Number of samples to calculate the parameters. This is at least the\n",
      "     |      number of features (plus 1 if fit_intercept=True) and the number of\n",
      "     |      samples as a maximum. A lower number leads to a higher breakdown\n",
      "     |      point and a low efficiency while a high number leads to a low\n",
      "     |      breakdown point and a high efficiency. If None, take the\n",
      "     |      minimum number of subsamples leading to maximal robustness.\n",
      "     |      If n_subsamples is set to n_samples, Theil-Sen is identical to least\n",
      "     |      squares.\n",
      "     |  \n",
      "     |  max_iter : int, optional, default 300\n",
      "     |      Maximum number of iterations for the calculation of spatial median.\n",
      "     |  \n",
      "     |  tol : float, optional, default 1.e-3\n",
      "     |      Tolerance when calculating spatial median.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional, default None\n",
      "     |      A random number generator instance to define the state of the random\n",
      "     |      permutations generator.  If int, random_state is the seed used by the\n",
      "     |      random number generator; If RandomState instance, random_state is the\n",
      "     |      random number generator; If None, the random number generator is the\n",
      "     |      RandomState instance used by `np.random`.\n",
      "     |  \n",
      "     |  n_jobs : integer, optional, default 1\n",
      "     |      Number of CPUs to use during the cross validation. If ``-1``, use\n",
      "     |      all the CPUs.\n",
      "     |  \n",
      "     |  verbose : boolean, optional, default False\n",
      "     |      Verbose mode when fitting the model.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape = (n_features)\n",
      "     |      Coefficients of the regression model (median of distribution).\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      Estimated intercept of regression model.\n",
      "     |  \n",
      "     |  breakdown_ : float\n",
      "     |      Approximated breakdown point.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Number of iterations needed for the spatial median.\n",
      "     |  \n",
      "     |  n_subpopulation_ : int\n",
      "     |      Number of combinations taken into account from 'n choose k', where n is\n",
      "     |      the number of samples and k is the number of subsamples.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  - Theil-Sen Estimators in a Multiple Linear Regression Model, 2009\n",
      "     |    Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang\n",
      "     |    http://home.olemiss.edu/~xdang/papers/MTSE.pdf\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TheilSenRegressor\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, fit_intercept=True, copy_X=True, max_subpopulation=10000.0, n_subsamples=None, max_iter=300, tol=0.001, random_state=None, n_jobs=1, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : numpy array of shape [n_samples, n_features]\n",
      "     |          Training data\n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "\n",
      "FUNCTIONS\n",
      "    enet_path(X, y, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      "        Compute elastic net path with coordinate descent\n",
      "        \n",
      "        The elastic net optimization function varies for mono and multi-outputs.\n",
      "        \n",
      "        For mono-output tasks it is::\n",
      "        \n",
      "            1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "            + alpha * l1_ratio * ||w||_1\n",
      "            + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "        \n",
      "        For multi-output tasks it is::\n",
      "        \n",
      "            (1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n",
      "            + alpha * l1_ratio * ||W||_21\n",
      "            + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "        \n",
      "        Where::\n",
      "        \n",
      "            ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "        \n",
      "        i.e. the sum of norm of each row.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <elastic_net>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like}, shape (n_samples, n_features)\n",
      "            Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "            unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "            can be sparse.\n",
      "        \n",
      "        y : ndarray, shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Target values\n",
      "        \n",
      "        l1_ratio : float, optional\n",
      "            float between 0 and 1 passed to elastic net (scaling between\n",
      "            l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso\n",
      "        \n",
      "        eps : float\n",
      "            Length of the path. ``eps=1e-3`` means that\n",
      "            ``alpha_min / alpha_max = 1e-3``\n",
      "        \n",
      "        n_alphas : int, optional\n",
      "            Number of alphas along the regularization path\n",
      "        \n",
      "        alphas : ndarray, optional\n",
      "            List of alphas where to compute the models.\n",
      "            If None alphas are set automatically\n",
      "        \n",
      "        precompute : True | False | 'auto' | array-like\n",
      "            Whether to use a precomputed Gram matrix to speed up\n",
      "            calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "            matrix can also be passed as argument.\n",
      "        \n",
      "        Xy : array-like, optional\n",
      "            Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "            only when the Gram matrix is precomputed.\n",
      "        \n",
      "        copy_X : boolean, optional, default True\n",
      "            If ``True``, X will be copied; else, it may be overwritten.\n",
      "        \n",
      "        coef_init : array, shape (n_features, ) | None\n",
      "            The initial values of the coefficients.\n",
      "        \n",
      "        verbose : bool or integer\n",
      "            Amount of verbosity.\n",
      "        \n",
      "        return_n_iter : bool\n",
      "            whether to return the number of iterations or not.\n",
      "        \n",
      "        positive : bool, default False\n",
      "            If set to True, forces coefficients to be positive.\n",
      "            (Only allowed when ``y.ndim == 1``).\n",
      "        \n",
      "        check_input : bool, default True\n",
      "            Skip input validation checks, including the Gram matrix when provided\n",
      "            assuming there are handled by the caller when check_input=False.\n",
      "        \n",
      "        **params : kwargs\n",
      "            keyword arguments passed to the coordinate descent solver.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        alphas : array, shape (n_alphas,)\n",
      "            The alphas along the path where models are computed.\n",
      "        \n",
      "        coefs : array, shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "            Coefficients along the path.\n",
      "        \n",
      "        dual_gaps : array, shape (n_alphas,)\n",
      "            The dual gaps at the end of the optimization for each alpha.\n",
      "        \n",
      "        n_iters : array-like, shape (n_alphas,)\n",
      "            The number of iterations taken by the coordinate descent optimizer to\n",
      "            reach the specified tolerance for each alpha.\n",
      "            (Is returned when ``return_n_iter`` is set to True).\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For an example, see\n",
      "        :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "        <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        MultiTaskElasticNet\n",
      "        MultiTaskElasticNetCV\n",
      "        ElasticNet\n",
      "        ElasticNetCV\n",
      "    \n",
      "    lars_path(X, y, Xy=None, Gram=None, max_iter=500, alpha_min=0, method='lar', copy_X=True, eps=2.2204460492503131e-16, copy_Gram=True, verbose=0, return_path=True, return_n_iter=False, positive=False)\n",
      "        Compute Least Angle Regression or Lasso path using LARS algorithm [1]\n",
      "        \n",
      "        The optimization objective for the case method='lasso' is::\n",
      "        \n",
      "        (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "        \n",
      "        in the case of method='lars', the objective function is only known in\n",
      "        the form of an implicit equation (see discussion in [1])\n",
      "        \n",
      "        Read more in the :ref:`User Guide <least_angle_regression>`.\n",
      "        \n",
      "        Parameters\n",
      "        -----------\n",
      "        X : array, shape: (n_samples, n_features)\n",
      "            Input data.\n",
      "        \n",
      "        y : array, shape: (n_samples)\n",
      "            Input targets.\n",
      "        \n",
      "        Xy : array-like, shape (n_samples,) or (n_samples, n_targets),             optional\n",
      "            Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "            only when the Gram matrix is precomputed.\n",
      "        \n",
      "        Gram : None, 'auto', array, shape: (n_features, n_features), optional\n",
      "            Precomputed Gram matrix (X' * X), if ``'auto'``, the Gram\n",
      "            matrix is precomputed from the given X, if there are more samples\n",
      "            than features.\n",
      "        \n",
      "        max_iter : integer, optional (default=500)\n",
      "            Maximum number of iterations to perform, set to infinity for no limit.\n",
      "        \n",
      "        alpha_min : float, optional (default=0)\n",
      "            Minimum correlation along the path. It corresponds to the\n",
      "            regularization parameter alpha parameter in the Lasso.\n",
      "        \n",
      "        method : {'lar', 'lasso'}, optional (default='lar')\n",
      "            Specifies the returned model. Select ``'lar'`` for Least Angle\n",
      "            Regression, ``'lasso'`` for the Lasso.\n",
      "        \n",
      "        copy_X : bool, optional (default=True)\n",
      "            If ``False``, ``X`` is overwritten.\n",
      "        \n",
      "        eps : float, optional (default=``np.finfo(np.float).eps``)\n",
      "            The machine-precision regularization in the computation of the\n",
      "            Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "            systems.\n",
      "        \n",
      "        copy_Gram : bool, optional (default=True)\n",
      "            If ``False``, ``Gram`` is overwritten.\n",
      "        \n",
      "        verbose : int (default=0)\n",
      "            Controls output verbosity.\n",
      "        \n",
      "        return_path : bool, optional (default=True)\n",
      "            If ``return_path==True`` returns the entire path, else returns only the\n",
      "            last point of the path.\n",
      "        \n",
      "        return_n_iter : bool, optional (default=False)\n",
      "            Whether to return the number of iterations.\n",
      "        \n",
      "        positive : boolean (default=False)\n",
      "            Restrict coefficients to be >= 0.\n",
      "            When using this option together with method 'lasso' the model\n",
      "            coefficients will not converge to the ordinary-least-squares solution\n",
      "            for small values of alpha (neither will they when using method 'lar'\n",
      "            ..). Only coefficients up to the smallest alpha value\n",
      "            (``alphas_[alphas_ > 0.].min()`` when fit_path=True) reached by the\n",
      "            stepwise Lars-Lasso algorithm are typically in congruence with the\n",
      "            solution of the coordinate descent lasso_path function.\n",
      "        \n",
      "        Returns\n",
      "        --------\n",
      "        alphas : array, shape: [n_alphas + 1]\n",
      "            Maximum of covariances (in absolute value) at each iteration.\n",
      "            ``n_alphas`` is either ``max_iter``, ``n_features`` or the\n",
      "            number of nodes in the path with ``alpha >= alpha_min``, whichever\n",
      "            is smaller.\n",
      "        \n",
      "        active : array, shape [n_alphas]\n",
      "            Indices of active variables at the end of the path.\n",
      "        \n",
      "        coefs : array, shape (n_features, n_alphas + 1)\n",
      "            Coefficients along the path\n",
      "        \n",
      "        n_iter : int\n",
      "            Number of iterations run. Returned only if return_n_iter is set\n",
      "            to True.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        lasso_path\n",
      "        LassoLars\n",
      "        Lars\n",
      "        LassoLarsCV\n",
      "        LarsCV\n",
      "        sklearn.decomposition.sparse_encode\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] \"Least Angle Regression\", Effron et al.\n",
      "               http://statweb.stanford.edu/~tibs/ftp/lars.pdf\n",
      "        \n",
      "        .. [2] `Wikipedia entry on the Least-angle regression\n",
      "               <https://en.wikipedia.org/wiki/Least-angle_regression>`_\n",
      "        \n",
      "        .. [3] `Wikipedia entry on the Lasso\n",
      "               <https://en.wikipedia.org/wiki/Lasso_(statistics)>`_\n",
      "    \n",
      "    lasso_path(X, y, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, **params)\n",
      "        Compute Lasso path with coordinate descent\n",
      "        \n",
      "        The Lasso optimization function varies for mono and multi-outputs.\n",
      "        \n",
      "        For mono-output tasks it is::\n",
      "        \n",
      "            (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "        \n",
      "        For multi-output tasks it is::\n",
      "        \n",
      "            (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\n",
      "        \n",
      "        Where::\n",
      "        \n",
      "            ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "        \n",
      "        i.e. the sum of norm of each row.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <lasso>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "            Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "            unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "            can be sparse.\n",
      "        \n",
      "        y : ndarray, shape (n_samples,), or (n_samples, n_outputs)\n",
      "            Target values\n",
      "        \n",
      "        eps : float, optional\n",
      "            Length of the path. ``eps=1e-3`` means that\n",
      "            ``alpha_min / alpha_max = 1e-3``\n",
      "        \n",
      "        n_alphas : int, optional\n",
      "            Number of alphas along the regularization path\n",
      "        \n",
      "        alphas : ndarray, optional\n",
      "            List of alphas where to compute the models.\n",
      "            If ``None`` alphas are set automatically\n",
      "        \n",
      "        precompute : True | False | 'auto' | array-like\n",
      "            Whether to use a precomputed Gram matrix to speed up\n",
      "            calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "            matrix can also be passed as argument.\n",
      "        \n",
      "        Xy : array-like, optional\n",
      "            Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "            only when the Gram matrix is precomputed.\n",
      "        \n",
      "        copy_X : boolean, optional, default True\n",
      "            If ``True``, X will be copied; else, it may be overwritten.\n",
      "        \n",
      "        coef_init : array, shape (n_features, ) | None\n",
      "            The initial values of the coefficients.\n",
      "        \n",
      "        verbose : bool or integer\n",
      "            Amount of verbosity.\n",
      "        \n",
      "        return_n_iter : bool\n",
      "            whether to return the number of iterations or not.\n",
      "        \n",
      "        positive : bool, default False\n",
      "            If set to True, forces coefficients to be positive.\n",
      "            (Only allowed when ``y.ndim == 1``).\n",
      "        \n",
      "        **params : kwargs\n",
      "            keyword arguments passed to the coordinate descent solver.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        alphas : array, shape (n_alphas,)\n",
      "            The alphas along the path where models are computed.\n",
      "        \n",
      "        coefs : array, shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "            Coefficients along the path.\n",
      "        \n",
      "        dual_gaps : array, shape (n_alphas,)\n",
      "            The dual gaps at the end of the optimization for each alpha.\n",
      "        \n",
      "        n_iters : array-like, shape (n_alphas,)\n",
      "            The number of iterations taken by the coordinate descent optimizer to\n",
      "            reach the specified tolerance for each alpha.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For an example, see\n",
      "        :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "        <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      "        \n",
      "        To avoid unnecessary memory duplication the X argument of the fit method\n",
      "        should be directly passed as a Fortran-contiguous numpy array.\n",
      "        \n",
      "        Note that in certain cases, the Lars solver may be significantly\n",
      "        faster to implement this functionality. In particular, linear\n",
      "        interpolation can be used to retrieve model coefficients between the\n",
      "        values output by lars_path\n",
      "        \n",
      "        Examples\n",
      "        ---------\n",
      "        \n",
      "        Comparing lasso_path and lars_path with interpolation:\n",
      "        \n",
      "        >>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n",
      "        >>> y = np.array([1, 2, 3.1])\n",
      "        >>> # Use lasso_path to compute a coefficient path\n",
      "        >>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])\n",
      "        >>> print(coef_path)\n",
      "        [[ 0.          0.          0.46874778]\n",
      "         [ 0.2159048   0.4425765   0.23689075]]\n",
      "        \n",
      "        >>> # Now use lars_path and 1D linear interpolation to compute the\n",
      "        >>> # same path\n",
      "        >>> from sklearn.linear_model import lars_path\n",
      "        >>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')\n",
      "        >>> from scipy import interpolate\n",
      "        >>> coef_path_continuous = interpolate.interp1d(alphas[::-1],\n",
      "        ...                                             coef_path_lars[:, ::-1])\n",
      "        >>> print(coef_path_continuous([5., 1., .5]))\n",
      "        [[ 0.          0.          0.46915237]\n",
      "         [ 0.2159048   0.4425765   0.23668876]]\n",
      "        \n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        lars_path\n",
      "        Lasso\n",
      "        LassoLars\n",
      "        LassoCV\n",
      "        LassoLarsCV\n",
      "        sklearn.decomposition.sparse_encode\n",
      "    \n",
      "    lasso_stability_path(X, y, scaling=0.5, random_state=None, n_resampling=200, n_grid=100, sample_fraction=0.75, eps=8.8817841970012523e-16, n_jobs=1, verbose=False)\n",
      "        DEPRECATED: The function lasso_stability_path is deprecated in 0.19 and will be removed in 0.21.\n",
      "        \n",
      "        Stability path based on randomized Lasso estimates\n",
      "        \n",
      "            Parameters\n",
      "            ----------\n",
      "            X : array-like, shape = [n_samples, n_features]\n",
      "                training data.\n",
      "        \n",
      "            y : array-like, shape = [n_samples]\n",
      "                target values.\n",
      "        \n",
      "            scaling : float, optional, default=0.5\n",
      "                The alpha parameter in the stability selection article used to\n",
      "                randomly scale the features. Should be between 0 and 1.\n",
      "        \n",
      "            random_state : int, RandomState instance or None, optional, default=None\n",
      "                The generator used to randomize the design.  If int, random_state is\n",
      "                the seed used by the random number generator; If RandomState instance,\n",
      "                random_state is the random number generator; If None, the random number\n",
      "                generator is the RandomState instance used by `np.random`.\n",
      "        \n",
      "            n_resampling : int, optional, default=200\n",
      "                Number of randomized models.\n",
      "        \n",
      "            n_grid : int, optional, default=100\n",
      "                Number of grid points. The path is linearly reinterpolated\n",
      "                on a grid between 0 and 1 before computing the scores.\n",
      "        \n",
      "            sample_fraction : float, optional, default=0.75\n",
      "                The fraction of samples to be used in each randomized design.\n",
      "                Should be between 0 and 1. If 1, all samples are used.\n",
      "        \n",
      "            eps : float, optional\n",
      "                Smallest value of alpha / alpha_max considered\n",
      "        \n",
      "            n_jobs : integer, optional\n",
      "                Number of CPUs to use during the resampling. If '-1', use\n",
      "                all the CPUs\n",
      "        \n",
      "            verbose : boolean or integer, optional\n",
      "                Sets the verbosity amount\n",
      "        \n",
      "            Returns\n",
      "            -------\n",
      "            alphas_grid : array, shape ~ [n_grid]\n",
      "                The grid points between 0 and 1: alpha/alpha_max\n",
      "        \n",
      "            scores_path : array, shape = [n_features, n_grid]\n",
      "                The scores for each feature along the path.\n",
      "    \n",
      "    logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True, max_iter=100, tol=0.0001, verbose=0, solver='lbfgs', coef=None, class_weight=None, dual=False, penalty='l2', intercept_scaling=1.0, multi_class='ovr', random_state=None, check_input=True, max_squared_sum=None, sample_weight=None)\n",
      "        Compute a Logistic Regression model for a list of regularization\n",
      "        parameters.\n",
      "        \n",
      "        This is an implementation that uses the result of the previous model\n",
      "        to speed up computations along the set of solutions, making it faster\n",
      "        than sequentially calling LogisticRegression for the different parameters.\n",
      "        Note that there will be no speedup with liblinear solver, since it does\n",
      "        not handle warm-starting.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <logistic_regression>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "            Input data.\n",
      "        \n",
      "        y : array-like, shape (n_samples,)\n",
      "            Input data, target values.\n",
      "        \n",
      "        pos_class : int, None\n",
      "            The class with respect to which we perform a one-vs-all fit.\n",
      "            If None, then it is assumed that the given problem is binary.\n",
      "        \n",
      "        Cs : int | array-like, shape (n_cs,)\n",
      "            List of values for the regularization parameter or integer specifying\n",
      "            the number of regularization parameters that should be used. In this\n",
      "            case, the parameters will be chosen in a logarithmic scale between\n",
      "            1e-4 and 1e4.\n",
      "        \n",
      "        fit_intercept : bool\n",
      "            Whether to fit an intercept for the model. In this case the shape of\n",
      "            the returned array is (n_cs, n_features + 1).\n",
      "        \n",
      "        max_iter : int\n",
      "            Maximum number of iterations for the solver.\n",
      "        \n",
      "        tol : float\n",
      "            Stopping criterion. For the newton-cg and lbfgs solvers, the iteration\n",
      "            will stop when ``max{|g_i | i = 1, ..., n} <= tol``\n",
      "            where ``g_i`` is the i-th component of the gradient.\n",
      "        \n",
      "        verbose : int\n",
      "            For the liblinear and lbfgs solvers set verbose to any positive\n",
      "            number for verbosity.\n",
      "        \n",
      "        solver : {'lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga'}\n",
      "            Numerical solver to use.\n",
      "        \n",
      "        coef : array-like, shape (n_features,), default None\n",
      "            Initialization value for coefficients of logistic regression.\n",
      "            Useless for liblinear solver.\n",
      "        \n",
      "        class_weight : dict or 'balanced', optional\n",
      "            Weights associated with classes in the form ``{class_label: weight}``.\n",
      "            If not given, all classes are supposed to have weight one.\n",
      "        \n",
      "            The \"balanced\" mode uses the values of y to automatically adjust\n",
      "            weights inversely proportional to class frequencies in the input data\n",
      "            as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "        \n",
      "            Note that these weights will be multiplied with sample_weight (passed\n",
      "            through the fit method) if sample_weight is specified.\n",
      "        \n",
      "        dual : bool\n",
      "            Dual or primal formulation. Dual formulation is only implemented for\n",
      "            l2 penalty with liblinear solver. Prefer dual=False when\n",
      "            n_samples > n_features.\n",
      "        \n",
      "        penalty : str, 'l1' or 'l2'\n",
      "            Used to specify the norm used in the penalization. The 'newton-cg',\n",
      "            'sag' and 'lbfgs' solvers support only l2 penalties.\n",
      "        \n",
      "        intercept_scaling : float, default 1.\n",
      "            Useful only when the solver 'liblinear' is used\n",
      "            and self.fit_intercept is set to True. In this case, x becomes\n",
      "            [x, self.intercept_scaling],\n",
      "            i.e. a \"synthetic\" feature with constant value equal to\n",
      "            intercept_scaling is appended to the instance vector.\n",
      "            The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
      "        \n",
      "            Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      "            as all other features.\n",
      "            To lessen the effect of regularization on synthetic feature weight\n",
      "            (and therefore on the intercept) intercept_scaling has to be increased.\n",
      "        \n",
      "        multi_class : str, {'ovr', 'multinomial'}\n",
      "            Multiclass option can be either 'ovr' or 'multinomial'. If the option\n",
      "            chosen is 'ovr', then a binary problem is fit for each label. Else\n",
      "            the loss minimised is the multinomial loss fit across\n",
      "            the entire probability distribution. Works only for the 'lbfgs' and\n",
      "            'newton-cg' solvers.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, optional, default None\n",
      "            The seed of the pseudo random number generator to use when shuffling\n",
      "            the data.  If int, random_state is the seed used by the random number\n",
      "            generator; If RandomState instance, random_state is the random number\n",
      "            generator; If None, the random number generator is the RandomState\n",
      "            instance used by `np.random`. Used when ``solver`` == 'sag' or\n",
      "            'liblinear'.\n",
      "        \n",
      "        check_input : bool, default True\n",
      "            If False, the input arrays X and y will not be checked.\n",
      "        \n",
      "        max_squared_sum : float, default None\n",
      "            Maximum squared sum of X over samples. Used only in SAG solver.\n",
      "            If None, it will be computed, going through all the samples.\n",
      "            The value should be precomputed to speed up cross validation.\n",
      "        \n",
      "        sample_weight : array-like, shape(n_samples,) optional\n",
      "            Array of weights that are assigned to individual samples.\n",
      "            If not provided, then each sample is given unit weight.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        coefs : ndarray, shape (n_cs, n_features) or (n_cs, n_features + 1)\n",
      "            List of coefficients for the Logistic Regression model. If\n",
      "            fit_intercept is set to True then the second dimension will be\n",
      "            n_features + 1, where the last item represents the intercept.\n",
      "        \n",
      "        Cs : ndarray\n",
      "            Grid of Cs used for cross-validation.\n",
      "        \n",
      "        n_iter : array, shape (n_cs,)\n",
      "            Actual number of iteration for each Cs.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        You might get slightly different results with the solver liblinear than\n",
      "        with the others since this uses LIBLINEAR which penalizes the intercept.\n",
      "        \n",
      "        .. versionchanged:: 0.19\n",
      "            The \"copy\" parameter was removed.\n",
      "    \n",
      "    orthogonal_mp(X, y, n_nonzero_coefs=None, tol=None, precompute=False, copy_X=True, return_path=False, return_n_iter=False)\n",
      "        Orthogonal Matching Pursuit (OMP)\n",
      "        \n",
      "        Solves n_targets Orthogonal Matching Pursuit problems.\n",
      "        An instance of the problem has the form:\n",
      "        \n",
      "        When parametrized by the number of non-zero coefficients using\n",
      "        `n_nonzero_coefs`:\n",
      "        argmin ||y - X\\gamma||^2 subject to ||\\gamma||_0 <= n_{nonzero coefs}\n",
      "        \n",
      "        When parametrized by error using the parameter `tol`:\n",
      "        argmin ||\\gamma||_0 subject to ||y - X\\gamma||^2 <= tol\n",
      "        \n",
      "        Read more in the :ref:`User Guide <omp>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array, shape (n_samples, n_features)\n",
      "            Input data. Columns are assumed to have unit norm.\n",
      "        \n",
      "        y : array, shape (n_samples,) or (n_samples, n_targets)\n",
      "            Input targets\n",
      "        \n",
      "        n_nonzero_coefs : int\n",
      "            Desired number of non-zero entries in the solution. If None (by\n",
      "            default) this value is set to 10% of n_features.\n",
      "        \n",
      "        tol : float\n",
      "            Maximum norm of the residual. If not None, overrides n_nonzero_coefs.\n",
      "        \n",
      "        precompute : {True, False, 'auto'},\n",
      "            Whether to perform precomputations. Improves performance when n_targets\n",
      "            or n_samples is very large.\n",
      "        \n",
      "        copy_X : bool, optional\n",
      "            Whether the design matrix X must be copied by the algorithm. A false\n",
      "            value is only helpful if X is already Fortran-ordered, otherwise a\n",
      "            copy is made anyway.\n",
      "        \n",
      "        return_path : bool, optional. Default: False\n",
      "            Whether to return every value of the nonzero coefficients along the\n",
      "            forward path. Useful for cross-validation.\n",
      "        \n",
      "        return_n_iter : bool, optional default False\n",
      "            Whether or not to return the number of iterations.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        coef : array, shape (n_features,) or (n_features, n_targets)\n",
      "            Coefficients of the OMP solution. If `return_path=True`, this contains\n",
      "            the whole coefficient path. In this case its shape is\n",
      "            (n_features, n_features) or (n_features, n_targets, n_features) and\n",
      "            iterating over the last axis yields coefficients in increasing order\n",
      "            of active features.\n",
      "        \n",
      "        n_iters : array-like or int\n",
      "            Number of active features across every target. Returned only if\n",
      "            `return_n_iter` is set to True.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        OrthogonalMatchingPursuit\n",
      "        orthogonal_mp_gram\n",
      "        lars_path\n",
      "        decomposition.sparse_encode\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Orthogonal matching pursuit was introduced in S. Mallat, Z. Zhang,\n",
      "        Matching pursuits with time-frequency dictionaries, IEEE Transactions on\n",
      "        Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\n",
      "        (http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf)\n",
      "        \n",
      "        This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\n",
      "        M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\n",
      "        Matching Pursuit Technical Report - CS Technion, April 2008.\n",
      "        http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf\n",
      "    \n",
      "    orthogonal_mp_gram(Gram, Xy, n_nonzero_coefs=None, tol=None, norms_squared=None, copy_Gram=True, copy_Xy=True, return_path=False, return_n_iter=False)\n",
      "        Gram Orthogonal Matching Pursuit (OMP)\n",
      "        \n",
      "        Solves n_targets Orthogonal Matching Pursuit problems using only\n",
      "        the Gram matrix X.T * X and the product X.T * y.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <omp>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        Gram : array, shape (n_features, n_features)\n",
      "            Gram matrix of the input data: X.T * X\n",
      "        \n",
      "        Xy : array, shape (n_features,) or (n_features, n_targets)\n",
      "            Input targets multiplied by X: X.T * y\n",
      "        \n",
      "        n_nonzero_coefs : int\n",
      "            Desired number of non-zero entries in the solution. If None (by\n",
      "            default) this value is set to 10% of n_features.\n",
      "        \n",
      "        tol : float\n",
      "            Maximum norm of the residual. If not None, overrides n_nonzero_coefs.\n",
      "        \n",
      "        norms_squared : array-like, shape (n_targets,)\n",
      "            Squared L2 norms of the lines of y. Required if tol is not None.\n",
      "        \n",
      "        copy_Gram : bool, optional\n",
      "            Whether the gram matrix must be copied by the algorithm. A false\n",
      "            value is only helpful if it is already Fortran-ordered, otherwise a\n",
      "            copy is made anyway.\n",
      "        \n",
      "        copy_Xy : bool, optional\n",
      "            Whether the covariance vector Xy must be copied by the algorithm.\n",
      "            If False, it may be overwritten.\n",
      "        \n",
      "        return_path : bool, optional. Default: False\n",
      "            Whether to return every value of the nonzero coefficients along the\n",
      "            forward path. Useful for cross-validation.\n",
      "        \n",
      "        return_n_iter : bool, optional default False\n",
      "            Whether or not to return the number of iterations.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        coef : array, shape (n_features,) or (n_features, n_targets)\n",
      "            Coefficients of the OMP solution. If `return_path=True`, this contains\n",
      "            the whole coefficient path. In this case its shape is\n",
      "            (n_features, n_features) or (n_features, n_targets, n_features) and\n",
      "            iterating over the last axis yields coefficients in increasing order\n",
      "            of active features.\n",
      "        \n",
      "        n_iters : array-like or int\n",
      "            Number of active features across every target. Returned only if\n",
      "            `return_n_iter` is set to True.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        OrthogonalMatchingPursuit\n",
      "        orthogonal_mp\n",
      "        lars_path\n",
      "        decomposition.sparse_encode\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang,\n",
      "        Matching pursuits with time-frequency dictionaries, IEEE Transactions on\n",
      "        Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\n",
      "        (http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf)\n",
      "        \n",
      "        This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\n",
      "        M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\n",
      "        Matching Pursuit Technical Report - CS Technion, April 2008.\n",
      "        http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf\n",
      "    \n",
      "    ridge_regression(X, y, alpha, sample_weight=None, solver='auto', max_iter=None, tol=0.001, verbose=0, random_state=None, return_n_iter=False, return_intercept=False)\n",
      "        Solve the ridge equation by the method of normal equations.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <ridge_regression>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix, LinearOperator},\n",
      "            shape = [n_samples, n_features]\n",
      "            Training data\n",
      "        \n",
      "        y : array-like, shape = [n_samples] or [n_samples, n_targets]\n",
      "            Target values\n",
      "        \n",
      "        alpha : {float, array-like},\n",
      "            shape = [n_targets] if array-like\n",
      "            Regularization strength; must be a positive float. Regularization\n",
      "            improves the conditioning of the problem and reduces the variance of\n",
      "            the estimates. Larger values specify stronger regularization.\n",
      "            Alpha corresponds to ``C^-1`` in other linear models such as\n",
      "            LogisticRegression or LinearSVC. If an array is passed, penalties are\n",
      "            assumed to be specific to the targets. Hence they must correspond in\n",
      "            number.\n",
      "        \n",
      "        sample_weight : float or numpy array of shape [n_samples]\n",
      "            Individual weights for each sample. If sample_weight is not None and\n",
      "            solver='auto', the solver will be set to 'cholesky'.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "        \n",
      "        solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'}\n",
      "            Solver to use in the computational routines:\n",
      "        \n",
      "            - 'auto' chooses the solver automatically based on the type of data.\n",
      "        \n",
      "            - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n",
      "              coefficients. More stable for singular matrices than\n",
      "              'cholesky'.\n",
      "        \n",
      "            - 'cholesky' uses the standard scipy.linalg.solve function to\n",
      "              obtain a closed-form solution via a Cholesky decomposition of\n",
      "              dot(X.T, X)\n",
      "        \n",
      "            - 'sparse_cg' uses the conjugate gradient solver as found in\n",
      "              scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n",
      "              more appropriate than 'cholesky' for large-scale data\n",
      "              (possibility to set `tol` and `max_iter`).\n",
      "        \n",
      "            - 'lsqr' uses the dedicated regularized least-squares routine\n",
      "              scipy.sparse.linalg.lsqr. It is the fastest but may not be available\n",
      "              in old scipy versions. It also uses an iterative procedure.\n",
      "        \n",
      "            - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n",
      "              its improved, unbiased version named SAGA. Both methods also use an\n",
      "              iterative procedure, and are often faster than other solvers when\n",
      "              both n_samples and n_features are large. Note that 'sag' and\n",
      "              'saga' fast convergence is only guaranteed on features with\n",
      "              approximately the same scale. You can preprocess the data with a\n",
      "              scaler from sklearn.preprocessing.\n",
      "        \n",
      "        \n",
      "            All last five solvers support both dense and sparse data. However, only\n",
      "            'sag' and 'saga' supports sparse input when`fit_intercept` is True.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "               Stochastic Average Gradient descent solver.\n",
      "            .. versionadded:: 0.19\n",
      "               SAGA solver.\n",
      "        \n",
      "        max_iter : int, optional\n",
      "            Maximum number of iterations for conjugate gradient solver.\n",
      "            For the 'sparse_cg' and 'lsqr' solvers, the default value is determined\n",
      "            by scipy.sparse.linalg. For 'sag' and saga solver, the default value is\n",
      "            1000.\n",
      "        \n",
      "        tol : float\n",
      "            Precision of the solution.\n",
      "        \n",
      "        verbose : int\n",
      "            Verbosity level. Setting verbose > 0 will display additional\n",
      "            information depending on the solver used.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, optional, default None\n",
      "            The seed of the pseudo random number generator to use when shuffling\n",
      "            the data.  If int, random_state is the seed used by the random number\n",
      "            generator; If RandomState instance, random_state is the random number\n",
      "            generator; If None, the random number generator is the RandomState\n",
      "            instance used by `np.random`. Used when ``solver`` == 'sag'.\n",
      "        \n",
      "        return_n_iter : boolean, default False\n",
      "            If True, the method also returns `n_iter`, the actual number of\n",
      "            iteration performed by the solver.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "        \n",
      "        return_intercept : boolean, default False\n",
      "            If True and if X is sparse, the method also returns the intercept,\n",
      "            and the solver is automatically changed to 'sag'. This is only a\n",
      "            temporary fix for fitting the intercept with sparse data. For dense\n",
      "            data, use sklearn.linear_model._preprocess_data before your regression.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        coef : array, shape = [n_features] or [n_targets, n_features]\n",
      "            Weight vector(s).\n",
      "        \n",
      "        n_iter : int, optional\n",
      "            The actual number of iteration performed by the solver.\n",
      "            Only returned if `return_n_iter` is True.\n",
      "        \n",
      "        intercept : float or array, shape = [n_targets]\n",
      "            The intercept of the model. Only returned if `return_intercept`\n",
      "            is True and if X is a scipy sparse array.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function won't compute the intercept.\n",
      "\n",
      "DATA\n",
      "    __all__ = ['ARDRegression', 'BayesianRidge', 'ElasticNet', 'ElasticNet...\n",
      "\n",
      "FILE\n",
      "    /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sl.linear_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   2   3   4   5   7   8   9  10  12  13  14  15  17  18  19  20  22\n",
      "  25  26  27  28  29  30  32  33  34  35  36  37  42  43  44  45  46  47\n",
      "  48  49  51  52  53  54  55  56  57  58  59  60  62  63  64  65  67  68\n",
      "  69  70  73  74  75  76  79  80  81  82  83  84  85  86  87  88  89  91\n",
      "  92  93  94  95  96  97  98 100 101 102 103 104 105 106 107 109 110 111\n",
      " 112 113 114 115 117 118 119 120 121 122 123 124 127 128 129 130 131 132\n",
      " 133 134 136 137 138 139 140 141 142 144 147 149]\n",
      "[  1   6  11  16  21  23  24  31  38  39  40  41  50  61  66  71  72  77\n",
      "  78  90  99 108 116 125 126 135 143 145 146 148]\n",
      "\n",
      "\n",
      "[  0   1   2   3   4   5   6  11  13  16  18  20  21  22  23  24  25  27\n",
      "  28  29  30  31  32  34  35  37  38  39  40  41  42  43  44  45  47  49\n",
      "  50  51  53  54  55  56  57  58  59  60  61  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  82  83  84  86  87  88  89  90  93  94\n",
      "  95  96  97  99 100 102 104 105 106 107 108 109 110 112 113 114 115 116\n",
      " 117 118 119 121 122 123 125 126 127 128 129 130 131 133 134 135 136 137\n",
      " 138 139 140 141 142 143 144 145 146 147 148 149]\n",
      "[  7   8   9  10  12  14  15  17  19  26  33  36  46  48  52  62  63  64\n",
      "  80  81  85  91  92  98 101 103 111 120 124 132]\n",
      "\n",
      "\n",
      "[  1   2   3   4   6   7   8   9  10  11  12  13  14  15  16  17  19  21\n",
      "  22  23  24  25  26  27  28  30  31  32  33  35  36  37  38  39  40  41\n",
      "  42  44  45  46  47  48  49  50  52  53  56  59  60  61  62  63  64  65\n",
      "  66  67  68  70  71  72  73  74  75  76  77  78  79  80  81  82  85  88\n",
      "  89  90  91  92  95  96  97  98  99 100 101 102 103 104 107 108 109 110\n",
      " 111 112 116 117 118 119 120 121 123 124 125 126 127 128 129 130 132 133\n",
      " 134 135 136 139 141 142 143 145 146 147 148 149]\n",
      "[  0   5  18  20  29  34  43  51  54  55  57  58  69  83  84  86  87  93\n",
      "  94 105 106 113 114 115 122 131 137 138 140 144]\n",
      "\n",
      "\n",
      "[  0   1   3   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19\n",
      "  20  21  22  23  24  26  27  29  30  31  32  33  34  35  36  38  39  40\n",
      "  41  43  45  46  48  49  50  51  52  54  55  57  58  61  62  63  64  66\n",
      "  69  71  72  73  74  75  76  77  78  80  81  82  83  84  85  86  87  89\n",
      "  90  91  92  93  94  96  98  99 100 101 103 104 105 106 108 109 110 111\n",
      " 112 113 114 115 116 117 118 119 120 122 123 124 125 126 128 130 131 132\n",
      " 133 134 135 137 138 140 141 143 144 145 146 148]\n",
      "[  2   4  25  28  37  42  44  47  53  56  59  60  65  67  68  70  79  88\n",
      "  95  97 102 107 121 127 129 136 139 142 147 149]\n",
      "\n",
      "\n",
      "[  0   1   2   4   5   6   7   8   9  10  11  12  14  15  16  17  18  19\n",
      "  20  21  23  24  25  26  28  29  31  33  34  36  37  38  39  40  41  42\n",
      "  43  44  46  47  48  50  51  52  53  54  55  56  57  58  59  60  61  62\n",
      "  63  64  65  66  67  68  69  70  71  72  77  78  79  80  81  83  84  85\n",
      "  86  87  88  90  91  92  93  94  95  97  98  99 101 102 103 105 106 107\n",
      " 108 111 113 114 115 116 120 121 122 124 125 126 127 129 131 132 135 136\n",
      " 137 138 139 140 142 143 144 145 146 147 148 149]\n",
      "[  3  13  22  27  30  32  35  45  49  73  74  75  76  82  89  96 100 104\n",
      " 109 110 112 117 118 119 123 128 130 133 134 141]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(5,shuffle=True,random_state=12)\n",
    "kf_sets = kf.split(X,)\n",
    "\n",
    "for train, test in kf_sets:\n",
    "    print(train)\n",
    "    print(test)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class KFold in module sklearn.model_selection._split:\n",
      "\n",
      "class KFold(_BaseKFold)\n",
      " |  K-Folds cross-validator\n",
      " |  \n",
      " |  Provides train/test indices to split data in train/test sets. Split\n",
      " |  dataset into k consecutive folds (without shuffling by default).\n",
      " |  \n",
      " |  Each fold is then used once as a validation while the k - 1 remaining\n",
      " |  folds form the training set.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <cross_validation>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_splits : int, default=3\n",
      " |      Number of folds. Must be at least 2.\n",
      " |  \n",
      " |  shuffle : boolean, optional\n",
      " |      Whether to shuffle the data before splitting into batches.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional, default=None\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`. Used when ``shuffle`` == True.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.model_selection import KFold\n",
      " |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
      " |  >>> y = np.array([1, 2, 3, 4])\n",
      " |  >>> kf = KFold(n_splits=2)\n",
      " |  >>> kf.get_n_splits(X)\n",
      " |  2\n",
      " |  >>> print(kf)  # doctest: +NORMALIZE_WHITESPACE\n",
      " |  KFold(n_splits=2, random_state=None, shuffle=False)\n",
      " |  >>> for train_index, test_index in kf.split(X):\n",
      " |  ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      " |  ...    X_train, X_test = X[train_index], X[test_index]\n",
      " |  ...    y_train, y_test = y[train_index], y[test_index]\n",
      " |  TRAIN: [2 3] TEST: [0 1]\n",
      " |  TRAIN: [0 1] TEST: [2 3]\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The first ``n_samples % n_splits`` folds have size\n",
      " |  ``n_samples // n_splits + 1``, other folds have size\n",
      " |  ``n_samples // n_splits``, where ``n_samples`` is the number of samples.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  StratifiedKFold\n",
      " |      Takes group information into account to avoid building folds with\n",
      " |      imbalanced class distributions (for binary or multiclass\n",
      " |      classification tasks).\n",
      " |  \n",
      " |  GroupKFold: K-fold iterator variant with non-overlapping groups.\n",
      " |  \n",
      " |  RepeatedKFold: Repeats K-Fold n times.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      KFold\n",
      " |      _BaseKFold\n",
      " |      abc.NewBase\n",
      " |      BaseCrossValidator\n",
      " |      abc.NewBase\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_splits=3, shuffle=False, random_state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _BaseKFold:\n",
      " |  \n",
      " |  get_n_splits(self, X=None, y=None, groups=None)\n",
      " |      Returns the number of splitting iterations in the cross-validator\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      y : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      groups : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      n_splits : int\n",
      " |          Returns the number of splitting iterations in the cross-validator.\n",
      " |  \n",
      " |  split(self, X, y=None, groups=None)\n",
      " |      Generate indices to split data into training and test set.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          Training data, where n_samples is the number of samples\n",
      " |          and n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,)\n",
      " |          The target variable for supervised learning problems.\n",
      " |      \n",
      " |      groups : array-like, with shape (n_samples,), optional\n",
      " |          Group labels for the samples used while splitting the dataset into\n",
      " |          train/test set.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      train : ndarray\n",
      " |          The training set indices for that split.\n",
      " |      \n",
      " |      test : ndarray\n",
      " |          The testing set indices for that split.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Randomized CV splitters may return different results for each call of\n",
      " |      split. You can make the results identical by setting ``random_state``\n",
      " |      to an integer.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseCrossValidator:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from abc.NewBase:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(KFold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(KFold.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic 回归分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113db9438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr =LogisticRegression(C=1000.0,random_state=0)\n",
    "lr.fit(X_train_std,y_train)\n",
    "plot_decision_regions(X=X_combined_std,\n",
    "                      y=y_combined,\n",
    "                      classifier=lr,\n",
    "                      test_idx=range(105,150))\n",
    "plt.xlabel('petal length(standardized)')\n",
    "plt.ylabel('petal width(standardized)')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn.svm in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.svm - The :mod:`sklearn.svm` module includes Support Vector Machine algorithms.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    base\n",
      "    bounds\n",
      "    classes\n",
      "    liblinear\n",
      "    libsvm\n",
      "    libsvm_sparse\n",
      "    setup\n",
      "    tests (package)\n",
      "\n",
      "CLASSES\n",
      "    sklearn.base.BaseEstimator(builtins.object)\n",
      "        sklearn.svm.classes.LinearSVC(sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin, sklearn.linear_model.base.SparseCoefMixin)\n",
      "    sklearn.base.RegressorMixin(builtins.object)\n",
      "        sklearn.svm.classes.LinearSVR(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "        sklearn.svm.classes.NuSVR(sklearn.svm.base.BaseLibSVM, sklearn.base.RegressorMixin)\n",
      "        sklearn.svm.classes.SVR(sklearn.svm.base.BaseLibSVM, sklearn.base.RegressorMixin)\n",
      "    sklearn.linear_model.base.LinearClassifierMixin(sklearn.base.ClassifierMixin)\n",
      "        sklearn.svm.classes.LinearSVC(sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin, sklearn.linear_model.base.SparseCoefMixin)\n",
      "    sklearn.linear_model.base.LinearModel(abc.NewBase)\n",
      "        sklearn.svm.classes.LinearSVR(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "    sklearn.linear_model.base.SparseCoefMixin(builtins.object)\n",
      "        sklearn.svm.classes.LinearSVC(sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin, sklearn.linear_model.base.SparseCoefMixin)\n",
      "    sklearn.svm.base.BaseLibSVM(abc.NewBase)\n",
      "        sklearn.svm.classes.NuSVR(sklearn.svm.base.BaseLibSVM, sklearn.base.RegressorMixin)\n",
      "        sklearn.svm.classes.OneClassSVM\n",
      "        sklearn.svm.classes.SVR(sklearn.svm.base.BaseLibSVM, sklearn.base.RegressorMixin)\n",
      "    sklearn.svm.base.BaseSVC(abc.NewBase)\n",
      "        sklearn.svm.classes.NuSVC\n",
      "        sklearn.svm.classes.SVC\n",
      "    \n",
      "    class LinearSVC(sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin, sklearn.linear_model.base.SparseCoefMixin)\n",
      "     |  Linear Support Vector Classification.\n",
      "     |  \n",
      "     |  Similar to SVC with parameter kernel='linear', but implemented in terms of\n",
      "     |  liblinear rather than libsvm, so it has more flexibility in the choice of\n",
      "     |  penalties and loss functions and should scale better to large numbers of\n",
      "     |  samples.\n",
      "     |  \n",
      "     |  This class supports both dense and sparse input and the multiclass support\n",
      "     |  is handled according to a one-vs-the-rest scheme.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <svm_classification>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  penalty : string, 'l1' or 'l2' (default='l2')\n",
      "     |      Specifies the norm used in the penalization. The 'l2'\n",
      "     |      penalty is the standard used in SVC. The 'l1' leads to ``coef_``\n",
      "     |      vectors that are sparse.\n",
      "     |  \n",
      "     |  loss : string, 'hinge' or 'squared_hinge' (default='squared_hinge')\n",
      "     |      Specifies the loss function. 'hinge' is the standard SVM loss\n",
      "     |      (used e.g. by the SVC class) while 'squared_hinge' is the\n",
      "     |      square of the hinge loss.\n",
      "     |  \n",
      "     |  dual : bool, (default=True)\n",
      "     |      Select the algorithm to either solve the dual or primal\n",
      "     |      optimization problem. Prefer dual=False when n_samples > n_features.\n",
      "     |  \n",
      "     |  tol : float, optional (default=1e-4)\n",
      "     |      Tolerance for stopping criteria.\n",
      "     |  \n",
      "     |  C : float, optional (default=1.0)\n",
      "     |      Penalty parameter C of the error term.\n",
      "     |  \n",
      "     |  multi_class : string, 'ovr' or 'crammer_singer' (default='ovr')\n",
      "     |      Determines the multi-class strategy if `y` contains more than\n",
      "     |      two classes.\n",
      "     |      ``\"ovr\"`` trains n_classes one-vs-rest classifiers, while\n",
      "     |      ``\"crammer_singer\"`` optimizes a joint objective over all classes.\n",
      "     |      While `crammer_singer` is interesting from a theoretical perspective\n",
      "     |      as it is consistent, it is seldom used in practice as it rarely leads\n",
      "     |      to better accuracy and is more expensive to compute.\n",
      "     |      If ``\"crammer_singer\"`` is chosen, the options loss, penalty and dual\n",
      "     |      will be ignored.\n",
      "     |  \n",
      "     |  fit_intercept : boolean, optional (default=True)\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be already centered).\n",
      "     |  \n",
      "     |  intercept_scaling : float, optional (default=1)\n",
      "     |      When self.fit_intercept is True, instance vector x becomes\n",
      "     |      ``[x, self.intercept_scaling]``,\n",
      "     |      i.e. a \"synthetic\" feature with constant value equals to\n",
      "     |      intercept_scaling is appended to the instance vector.\n",
      "     |      The intercept becomes intercept_scaling * synthetic feature weight\n",
      "     |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      "     |      as all other features.\n",
      "     |      To lessen the effect of regularization on synthetic feature weight\n",
      "     |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
      "     |  \n",
      "     |  class_weight : {dict, 'balanced'}, optional\n",
      "     |      Set the parameter C of class i to ``class_weight[i]*C`` for\n",
      "     |      SVC. If not given, all classes are supposed to have\n",
      "     |      weight one.\n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      "     |  \n",
      "     |  verbose : int, (default=0)\n",
      "     |      Enable verbose output. Note that this setting takes advantage of a\n",
      "     |      per-process runtime setting in liblinear that, if enabled, may not work\n",
      "     |      properly in a multithreaded context.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
      "     |      The seed of the pseudo random number generator to use when shuffling\n",
      "     |      the data.  If int, random_state is the seed used by the random number\n",
      "     |      generator; If RandomState instance, random_state is the random number\n",
      "     |      generator; If None, the random number generator is the RandomState\n",
      "     |      instance used by `np.random`.\n",
      "     |  \n",
      "     |  max_iter : int, (default=1000)\n",
      "     |      The maximum number of iterations to be run.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape = [n_features] if n_classes == 2 else [n_classes, n_features]\n",
      "     |      Weights assigned to the features (coefficients in the primal\n",
      "     |      problem). This is only available in the case of a linear kernel.\n",
      "     |  \n",
      "     |      ``coef_`` is a readonly property derived from ``raw_coef_`` that\n",
      "     |      follows the internal memory layout of liblinear.\n",
      "     |  \n",
      "     |  intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.svm import LinearSVC\n",
      "     |  >>> from sklearn.datasets import make_classification\n",
      "     |  >>> X, y = make_classification(n_features=4, random_state=0)\n",
      "     |  >>> clf = LinearSVC(random_state=0)\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     |       intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     |       multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,\n",
      "     |       verbose=0)\n",
      "     |  >>> print(clf.coef_)\n",
      "     |  [[ 0.08551385  0.39414796  0.49847831  0.37513797]]\n",
      "     |  >>> print(clf.intercept_)\n",
      "     |  [ 0.28418066]\n",
      "     |  >>> print(clf.predict([[0, 0, 0, 0]]))\n",
      "     |  [1]\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The underlying C implementation uses a random number generator to\n",
      "     |  select features when fitting the model. It is thus not uncommon\n",
      "     |  to have slightly different results for the same input data. If\n",
      "     |  that happens, try with a smaller ``tol`` parameter.\n",
      "     |  \n",
      "     |  The underlying implementation, liblinear, uses a sparse internal\n",
      "     |  representation for the data that will incur a memory copy.\n",
      "     |  \n",
      "     |  Predict output may not match that of standalone liblinear in certain\n",
      "     |  cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
      "     |  in the narrative documentation.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  `LIBLINEAR: A Library for Large Linear Classification\n",
      "     |  <http://www.csie.ntu.edu.tw/~cjlin/liblinear/>`__\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  SVC\n",
      "     |      Implementation of Support Vector Machine classifier using libsvm:\n",
      "     |      the kernel can be non-linear but its SMO algorithm does not\n",
      "     |      scale to large number of samples as LinearSVC does.\n",
      "     |  \n",
      "     |      Furthermore SVC multi-class mode is implemented using one\n",
      "     |      vs one scheme while LinearSVC uses one vs the rest. It is\n",
      "     |      possible to implement one vs the rest with SVC by using the\n",
      "     |      :class:`sklearn.multiclass.OneVsRestClassifier` wrapper.\n",
      "     |  \n",
      "     |      Finally SVC can fit dense data without memory copy if the input\n",
      "     |      is C-contiguous. Sparse data will still incur memory copy though.\n",
      "     |  \n",
      "     |  sklearn.linear_model.SGDClassifier\n",
      "     |      SGDClassifier can optimize the same cost function as LinearSVC\n",
      "     |      by adjusting the penalty and loss parameters. In addition it requires\n",
      "     |      less memory, allows incremental (online) learning, and implements\n",
      "     |      various loss functions and regularization regimes.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LinearSVC\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.linear_model.base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.linear_model.base.SparseCoefMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, penalty='l2', loss='squared_hinge', dual=True, tol=0.0001, C=1.0, multi_class='ovr', fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, random_state=None, max_iter=1000)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Training vector, where n_samples in the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples]\n",
      "     |          Target vector relative to X\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Array of weights that are assigned to individual\n",
      "     |          samples. If not provided,\n",
      "     |          then each sample is given unit weight.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is the signed distance of that\n",
      "     |      sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      "     |          Confidence scores per (sample, class) combination. In the binary\n",
      "     |          case, confidence score for self.classes_[1] where >0 means this\n",
      "     |          class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = [n_samples]\n",
      "     |          Predicted class label per sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator\n",
      "    \n",
      "    class LinearSVR(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "     |  Linear Support Vector Regression.\n",
      "     |  \n",
      "     |  Similar to SVR with parameter kernel='linear', but implemented in terms of\n",
      "     |  liblinear rather than libsvm, so it has more flexibility in the choice of\n",
      "     |  penalties and loss functions and should scale better to large numbers of\n",
      "     |  samples.\n",
      "     |  \n",
      "     |  This class supports both dense and sparse input.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <svm_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  C : float, optional (default=1.0)\n",
      "     |      Penalty parameter C of the error term. The penalty is a squared\n",
      "     |      l2 penalty. The bigger this parameter, the less regularization is used.\n",
      "     |  \n",
      "     |  loss : string, 'epsilon_insensitive' or 'squared_epsilon_insensitive' (default='epsilon_insensitive')\n",
      "     |      Specifies the loss function. 'l1' is the epsilon-insensitive loss\n",
      "     |      (standard SVR) while 'l2' is the squared epsilon-insensitive loss.\n",
      "     |  \n",
      "     |  epsilon : float, optional (default=0.1)\n",
      "     |      Epsilon parameter in the epsilon-insensitive loss function. Note\n",
      "     |      that the value of this parameter depends on the scale of the target\n",
      "     |      variable y. If unsure, set ``epsilon=0``.\n",
      "     |  \n",
      "     |  dual : bool, (default=True)\n",
      "     |      Select the algorithm to either solve the dual or primal\n",
      "     |      optimization problem. Prefer dual=False when n_samples > n_features.\n",
      "     |  \n",
      "     |  tol : float, optional (default=1e-4)\n",
      "     |      Tolerance for stopping criteria.\n",
      "     |  \n",
      "     |  fit_intercept : boolean, optional (default=True)\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be already centered).\n",
      "     |  \n",
      "     |  intercept_scaling : float, optional (default=1)\n",
      "     |      When self.fit_intercept is True, instance vector x becomes\n",
      "     |      [x, self.intercept_scaling],\n",
      "     |      i.e. a \"synthetic\" feature with constant value equals to\n",
      "     |      intercept_scaling is appended to the instance vector.\n",
      "     |      The intercept becomes intercept_scaling * synthetic feature weight\n",
      "     |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      "     |      as all other features.\n",
      "     |      To lessen the effect of regularization on synthetic feature weight\n",
      "     |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
      "     |  \n",
      "     |  verbose : int, (default=0)\n",
      "     |      Enable verbose output. Note that this setting takes advantage of a\n",
      "     |      per-process runtime setting in liblinear that, if enabled, may not work\n",
      "     |      properly in a multithreaded context.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
      "     |      The seed of the pseudo random number generator to use when shuffling\n",
      "     |      the data.  If int, random_state is the seed used by the random number\n",
      "     |      generator; If RandomState instance, random_state is the random number\n",
      "     |      generator; If None, the random number generator is the RandomState\n",
      "     |      instance used by `np.random`.\n",
      "     |  \n",
      "     |  max_iter : int, (default=1000)\n",
      "     |      The maximum number of iterations to be run.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape = [n_features] if n_classes == 2 else [n_classes, n_features]\n",
      "     |      Weights assigned to the features (coefficients in the primal\n",
      "     |      problem). This is only available in the case of a linear kernel.\n",
      "     |  \n",
      "     |      `coef_` is a readonly property derived from `raw_coef_` that\n",
      "     |      follows the internal memory layout of liblinear.\n",
      "     |  \n",
      "     |  intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.svm import LinearSVR\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> X, y = make_regression(n_features=4, random_state=0)\n",
      "     |  >>> regr = LinearSVR(random_state=0)\n",
      "     |  >>> regr.fit(X, y)\n",
      "     |  LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
      "     |       intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n",
      "     |       random_state=0, tol=0.0001, verbose=0)\n",
      "     |  >>> print(regr.coef_)\n",
      "     |  [ 16.35750999  26.91499923  42.30652207  60.47843124]\n",
      "     |  >>> print(regr.intercept_)\n",
      "     |  [-4.29756543]\n",
      "     |  >>> print(regr.predict([[0, 0, 0, 0]]))\n",
      "     |  [-4.29756543]\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  LinearSVC\n",
      "     |      Implementation of Support Vector Machine classifier using the\n",
      "     |      same library as this class (liblinear).\n",
      "     |  \n",
      "     |  SVR\n",
      "     |      Implementation of Support Vector Machine regression using libsvm:\n",
      "     |      the kernel can be non-linear but its SMO algorithm does not\n",
      "     |      scale to large number of samples as LinearSVC does.\n",
      "     |  \n",
      "     |  sklearn.linear_model.SGDRegressor\n",
      "     |      SGDRegressor can optimize the same cost function as LinearSVR\n",
      "     |      by adjusting the penalty and loss parameters. In addition it requires\n",
      "     |      less memory, allows incremental (online) learning, and implements\n",
      "     |      various loss functions and regularization regimes.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LinearSVR\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, epsilon=0.0, tol=0.0001, C=1.0, loss='epsilon_insensitive', fit_intercept=True, intercept_scaling=1.0, dual=True, verbose=0, random_state=None, max_iter=1000)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Training vector, where n_samples in the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples]\n",
      "     |          Target vector relative to X\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Array of weights that are assigned to individual\n",
      "     |          samples. If not provided,\n",
      "     |          then each sample is given unit weight.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class NuSVC(sklearn.svm.base.BaseSVC)\n",
      "     |  Nu-Support Vector Classification.\n",
      "     |  \n",
      "     |  Similar to SVC but uses a parameter to control the number of support\n",
      "     |  vectors.\n",
      "     |  \n",
      "     |  The implementation is based on libsvm.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <svm_classification>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  nu : float, optional (default=0.5)\n",
      "     |      An upper bound on the fraction of training errors and a lower\n",
      "     |      bound of the fraction of support vectors. Should be in the\n",
      "     |      interval (0, 1].\n",
      "     |  \n",
      "     |  kernel : string, optional (default='rbf')\n",
      "     |       Specifies the kernel type to be used in the algorithm.\n",
      "     |       It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n",
      "     |       a callable.\n",
      "     |       If none is given, 'rbf' will be used. If a callable is given it is\n",
      "     |       used to precompute the kernel matrix.\n",
      "     |  \n",
      "     |  degree : int, optional (default=3)\n",
      "     |      Degree of the polynomial kernel function ('poly').\n",
      "     |      Ignored by all other kernels.\n",
      "     |  \n",
      "     |  gamma : float, optional (default='auto')\n",
      "     |      Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n",
      "     |      If gamma is 'auto' then 1/n_features will be used instead.\n",
      "     |  \n",
      "     |  coef0 : float, optional (default=0.0)\n",
      "     |      Independent term in kernel function.\n",
      "     |      It is only significant in 'poly' and 'sigmoid'.\n",
      "     |  \n",
      "     |  probability : boolean, optional (default=False)\n",
      "     |      Whether to enable probability estimates. This must be enabled prior\n",
      "     |      to calling `fit`, and will slow down that method.\n",
      "     |  \n",
      "     |  shrinking : boolean, optional (default=True)\n",
      "     |      Whether to use the shrinking heuristic.\n",
      "     |  \n",
      "     |  tol : float, optional (default=1e-3)\n",
      "     |      Tolerance for stopping criterion.\n",
      "     |  \n",
      "     |  cache_size : float, optional\n",
      "     |      Specify the size of the kernel cache (in MB).\n",
      "     |  \n",
      "     |  class_weight : {dict, 'balanced'}, optional\n",
      "     |      Set the parameter C of class i to class_weight[i]*C for\n",
      "     |      SVC. If not given, all classes are supposed to have\n",
      "     |      weight one. The \"balanced\" mode uses the values of y to automatically\n",
      "     |      adjust weights inversely proportional to class frequencies as\n",
      "     |      ``n_samples / (n_classes * np.bincount(y))``\n",
      "     |  \n",
      "     |  verbose : bool, default: False\n",
      "     |      Enable verbose output. Note that this setting takes advantage of a\n",
      "     |      per-process runtime setting in libsvm that, if enabled, may not work\n",
      "     |      properly in a multithreaded context.\n",
      "     |  \n",
      "     |  max_iter : int, optional (default=-1)\n",
      "     |      Hard limit on iterations within solver, or -1 for no limit.\n",
      "     |  \n",
      "     |  decision_function_shape : 'ovo', 'ovr', default='ovr'\n",
      "     |      Whether to return a one-vs-rest ('ovr') decision function of shape\n",
      "     |      (n_samples, n_classes) as all other classifiers, or the original\n",
      "     |      one-vs-one ('ovo') decision function of libsvm which has shape\n",
      "     |      (n_samples, n_classes * (n_classes - 1) / 2).\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.19\n",
      "     |          decision_function_shape is 'ovr' by default.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         *decision_function_shape='ovr'* is recommended.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.17\n",
      "     |         Deprecated *decision_function_shape='ovo' and None*.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
      "     |      The seed of the pseudo random number generator to use when shuffling\n",
      "     |      the data.  If int, random_state is the seed used by the random number\n",
      "     |      generator; If RandomState instance, random_state is the random number\n",
      "     |      generator; If None, the random number generator is the RandomState\n",
      "     |      instance used by `np.random`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  support_ : array-like, shape = [n_SV]\n",
      "     |      Indices of support vectors.\n",
      "     |  \n",
      "     |  support_vectors_ : array-like, shape = [n_SV, n_features]\n",
      "     |      Support vectors.\n",
      "     |  \n",
      "     |  n_support_ : array-like, dtype=int32, shape = [n_class]\n",
      "     |      Number of support vectors for each class.\n",
      "     |  \n",
      "     |  dual_coef_ : array, shape = [n_class-1, n_SV]\n",
      "     |      Coefficients of the support vector in the decision function.\n",
      "     |      For multiclass, coefficient for all 1-vs-1 classifiers.\n",
      "     |      The layout of the coefficients in the multiclass case is somewhat\n",
      "     |      non-trivial. See the section about multi-class classification in\n",
      "     |      the SVM section of the User Guide for details.\n",
      "     |  \n",
      "     |  coef_ : array, shape = [n_class-1, n_features]\n",
      "     |      Weights assigned to the features (coefficients in the primal\n",
      "     |      problem). This is only available in the case of a linear kernel.\n",
      "     |  \n",
      "     |      `coef_` is readonly property derived from `dual_coef_` and\n",
      "     |      `support_vectors_`.\n",
      "     |  \n",
      "     |  intercept_ : array, shape = [n_class * (n_class-1) / 2]\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
      "     |  >>> y = np.array([1, 1, 2, 2])\n",
      "     |  >>> from sklearn.svm import NuSVC\n",
      "     |  >>> clf = NuSVC()\n",
      "     |  >>> clf.fit(X, y) #doctest: +NORMALIZE_WHITESPACE\n",
      "     |  NuSVC(cache_size=200, class_weight=None, coef0=0.0,\n",
      "     |        decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "     |        max_iter=-1, nu=0.5, probability=False, random_state=None,\n",
      "     |        shrinking=True, tol=0.001, verbose=False)\n",
      "     |  >>> print(clf.predict([[-0.8, -1]]))\n",
      "     |  [1]\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  SVC\n",
      "     |      Support Vector Machine for classification using libsvm.\n",
      "     |  \n",
      "     |  LinearSVC\n",
      "     |      Scalable linear Support Vector Machine for classification using\n",
      "     |      liblinear.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      NuSVC\n",
      "     |      sklearn.svm.base.BaseSVC\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.svm.base.BaseLibSVM\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, nu=0.5, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.svm.base.BaseSVC:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Distance of the samples X to the separating hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X : array-like, shape (n_samples, n_classes * (n_classes-1) / 2)\n",
      "     |          Returns the decision function of the sample for each class\n",
      "     |          in the model.\n",
      "     |          If decision_function_shape='ovr', the shape is (n_samples,\n",
      "     |          n_classes)\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Perform classification on samples in X.\n",
      "     |      \n",
      "     |      For an one-class model, +1 or -1 is returned.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          [n_samples_test, n_samples_train]\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : array, shape (n_samples,)\n",
      "     |          Class labels for samples in X.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.svm.base.BaseSVC:\n",
      "     |  \n",
      "     |  predict_log_proba\n",
      "     |      Compute log probabilities of possible outcomes for samples in X.\n",
      "     |      \n",
      "     |      The model need to have probability information computed at training\n",
      "     |      time: fit with attribute `probability` set to True.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          [n_samples_test, n_samples_train]\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : array-like, shape (n_samples, n_classes)\n",
      "     |          Returns the log-probabilities of the sample for each class in\n",
      "     |          the model. The columns correspond to the classes in sorted\n",
      "     |          order, as they appear in the attribute `classes_`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The probability model is created using cross validation, so\n",
      "     |      the results can be slightly different than those obtained by\n",
      "     |      predict. Also, it will produce meaningless results on very small\n",
      "     |      datasets.\n",
      "     |  \n",
      "     |  predict_proba\n",
      "     |      Compute probabilities of possible outcomes for samples in X.\n",
      "     |      \n",
      "     |      The model need to have probability information computed at training\n",
      "     |      time: fit with attribute `probability` set to True.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          [n_samples_test, n_samples_train]\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : array-like, shape (n_samples, n_classes)\n",
      "     |          Returns the probability of the sample for each class in\n",
      "     |          the model. The columns correspond to the classes in sorted\n",
      "     |          order, as they appear in the attribute `classes_`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The probability model is created using cross validation, so\n",
      "     |      the results can be slightly different than those obtained by\n",
      "     |      predict. Also, it will produce meaningless results on very small\n",
      "     |      datasets.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.svm.base.BaseLibSVM:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the SVM model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training vectors, where n_samples is the number of samples\n",
      "     |          and n_features is the number of features.\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          (n_samples, n_samples).\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,)\n",
      "     |          Target values (class labels in classification, real numbers in\n",
      "     |          regression)\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,)\n",
      "     |          Per-sample weights. Rescale C per sample. Higher weights\n",
      "     |          force the classifier to put more emphasis on these points.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      ------\n",
      "     |      If X and y are not C-ordered and contiguous arrays of np.float64 and\n",
      "     |      X is not a scipy.sparse.csr_matrix, X and/or y may be copied.\n",
      "     |      \n",
      "     |      If X is a dense array, then the other methods will not support sparse\n",
      "     |      matrices as input.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.svm.base.BaseLibSVM:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "    \n",
      "    class NuSVR(sklearn.svm.base.BaseLibSVM, sklearn.base.RegressorMixin)\n",
      "     |  Nu Support Vector Regression.\n",
      "     |  \n",
      "     |  Similar to NuSVC, for regression, uses a parameter nu to control\n",
      "     |  the number of support vectors. However, unlike NuSVC, where nu\n",
      "     |  replaces C, here nu replaces the parameter epsilon of epsilon-SVR.\n",
      "     |  \n",
      "     |  The implementation is based on libsvm.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <svm_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  C : float, optional (default=1.0)\n",
      "     |      Penalty parameter C of the error term.\n",
      "     |  \n",
      "     |  nu : float, optional\n",
      "     |      An upper bound on the fraction of training errors and a lower bound of\n",
      "     |      the fraction of support vectors. Should be in the interval (0, 1].  By\n",
      "     |      default 0.5 will be taken.\n",
      "     |  \n",
      "     |  kernel : string, optional (default='rbf')\n",
      "     |       Specifies the kernel type to be used in the algorithm.\n",
      "     |       It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n",
      "     |       a callable.\n",
      "     |       If none is given, 'rbf' will be used. If a callable is given it is\n",
      "     |       used to precompute the kernel matrix.\n",
      "     |  \n",
      "     |  degree : int, optional (default=3)\n",
      "     |      Degree of the polynomial kernel function ('poly').\n",
      "     |      Ignored by all other kernels.\n",
      "     |  \n",
      "     |  gamma : float, optional (default='auto')\n",
      "     |      Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n",
      "     |      If gamma is 'auto' then 1/n_features will be used instead.\n",
      "     |  \n",
      "     |  coef0 : float, optional (default=0.0)\n",
      "     |      Independent term in kernel function.\n",
      "     |      It is only significant in 'poly' and 'sigmoid'.\n",
      "     |  \n",
      "     |  shrinking : boolean, optional (default=True)\n",
      "     |      Whether to use the shrinking heuristic.\n",
      "     |  \n",
      "     |  tol : float, optional (default=1e-3)\n",
      "     |      Tolerance for stopping criterion.\n",
      "     |  \n",
      "     |  cache_size : float, optional\n",
      "     |      Specify the size of the kernel cache (in MB).\n",
      "     |  \n",
      "     |  verbose : bool, default: False\n",
      "     |      Enable verbose output. Note that this setting takes advantage of a\n",
      "     |      per-process runtime setting in libsvm that, if enabled, may not work\n",
      "     |      properly in a multithreaded context.\n",
      "     |  \n",
      "     |  max_iter : int, optional (default=-1)\n",
      "     |      Hard limit on iterations within solver, or -1 for no limit.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  support_ : array-like, shape = [n_SV]\n",
      "     |      Indices of support vectors.\n",
      "     |  \n",
      "     |  support_vectors_ : array-like, shape = [nSV, n_features]\n",
      "     |      Support vectors.\n",
      "     |  \n",
      "     |  dual_coef_ : array, shape = [1, n_SV]\n",
      "     |      Coefficients of the support vector in the decision function.\n",
      "     |  \n",
      "     |  coef_ : array, shape = [1, n_features]\n",
      "     |      Weights assigned to the features (coefficients in the primal\n",
      "     |      problem). This is only available in the case of a linear kernel.\n",
      "     |  \n",
      "     |      `coef_` is readonly property derived from `dual_coef_` and\n",
      "     |      `support_vectors_`.\n",
      "     |  \n",
      "     |  intercept_ : array, shape = [1]\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.svm import NuSVR\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> n_samples, n_features = 10, 5\n",
      "     |  >>> np.random.seed(0)\n",
      "     |  >>> y = np.random.randn(n_samples)\n",
      "     |  >>> X = np.random.randn(n_samples, n_features)\n",
      "     |  >>> clf = NuSVR(C=1.0, nu=0.1)\n",
      "     |  >>> clf.fit(X, y)  #doctest: +NORMALIZE_WHITESPACE\n",
      "     |  NuSVR(C=1.0, cache_size=200, coef0=0.0, degree=3, gamma='auto',\n",
      "     |        kernel='rbf', max_iter=-1, nu=0.1, shrinking=True, tol=0.001,\n",
      "     |        verbose=False)\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  NuSVC\n",
      "     |      Support Vector Machine for classification implemented with libsvm\n",
      "     |      with a parameter to control the number of support vectors.\n",
      "     |  \n",
      "     |  SVR\n",
      "     |      epsilon Support Vector Machine for regression implemented with libsvm.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      NuSVR\n",
      "     |      sklearn.svm.base.BaseLibSVM\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, nu=0.5, C=1.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, tol=0.001, cache_size=200, verbose=False, max_iter=-1)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.svm.base.BaseLibSVM:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the SVM model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training vectors, where n_samples is the number of samples\n",
      "     |          and n_features is the number of features.\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          (n_samples, n_samples).\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,)\n",
      "     |          Target values (class labels in classification, real numbers in\n",
      "     |          regression)\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,)\n",
      "     |          Per-sample weights. Rescale C per sample. Higher weights\n",
      "     |          force the classifier to put more emphasis on these points.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      ------\n",
      "     |      If X and y are not C-ordered and contiguous arrays of np.float64 and\n",
      "     |      X is not a scipy.sparse.csr_matrix, X and/or y may be copied.\n",
      "     |      \n",
      "     |      If X is a dense array, then the other methods will not support sparse\n",
      "     |      matrices as input.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Perform regression on samples in X.\n",
      "     |      \n",
      "     |      For an one-class model, +1 (inlier) or -1 (outlier) is returned.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          (n_samples_test, n_samples_train).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : array, shape (n_samples,)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.svm.base.BaseLibSVM:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class OneClassSVM(sklearn.svm.base.BaseLibSVM)\n",
      "     |  Unsupervised Outlier Detection.\n",
      "     |  \n",
      "     |  Estimate the support of a high-dimensional distribution.\n",
      "     |  \n",
      "     |  The implementation is based on libsvm.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <svm_outlier_detection>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  kernel : string, optional (default='rbf')\n",
      "     |       Specifies the kernel type to be used in the algorithm.\n",
      "     |       It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n",
      "     |       a callable.\n",
      "     |       If none is given, 'rbf' will be used. If a callable is given it is\n",
      "     |       used to precompute the kernel matrix.\n",
      "     |  \n",
      "     |  nu : float, optional\n",
      "     |      An upper bound on the fraction of training\n",
      "     |      errors and a lower bound of the fraction of support\n",
      "     |      vectors. Should be in the interval (0, 1]. By default 0.5\n",
      "     |      will be taken.\n",
      "     |  \n",
      "     |  degree : int, optional (default=3)\n",
      "     |      Degree of the polynomial kernel function ('poly').\n",
      "     |      Ignored by all other kernels.\n",
      "     |  \n",
      "     |  gamma : float, optional (default='auto')\n",
      "     |      Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n",
      "     |      If gamma is 'auto' then 1/n_features will be used instead.\n",
      "     |  \n",
      "     |  coef0 : float, optional (default=0.0)\n",
      "     |      Independent term in kernel function.\n",
      "     |      It is only significant in 'poly' and 'sigmoid'.\n",
      "     |  \n",
      "     |  tol : float, optional\n",
      "     |      Tolerance for stopping criterion.\n",
      "     |  \n",
      "     |  shrinking : boolean, optional\n",
      "     |      Whether to use the shrinking heuristic.\n",
      "     |  \n",
      "     |  cache_size : float, optional\n",
      "     |      Specify the size of the kernel cache (in MB).\n",
      "     |  \n",
      "     |  verbose : bool, default: False\n",
      "     |      Enable verbose output. Note that this setting takes advantage of a\n",
      "     |      per-process runtime setting in libsvm that, if enabled, may not work\n",
      "     |      properly in a multithreaded context.\n",
      "     |  \n",
      "     |  max_iter : int, optional (default=-1)\n",
      "     |      Hard limit on iterations within solver, or -1 for no limit.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
      "     |      The seed of the pseudo random number generator to use when shuffling\n",
      "     |      the data.  If int, random_state is the seed used by the random number\n",
      "     |      generator; If RandomState instance, random_state is the random number\n",
      "     |      generator; If None, the random number generator is the RandomState\n",
      "     |      instance used by `np.random`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  support_ : array-like, shape = [n_SV]\n",
      "     |      Indices of support vectors.\n",
      "     |  \n",
      "     |  support_vectors_ : array-like, shape = [nSV, n_features]\n",
      "     |      Support vectors.\n",
      "     |  \n",
      "     |  dual_coef_ : array, shape = [1, n_SV]\n",
      "     |      Coefficients of the support vectors in the decision function.\n",
      "     |  \n",
      "     |  coef_ : array, shape = [1, n_features]\n",
      "     |      Weights assigned to the features (coefficients in the primal\n",
      "     |      problem). This is only available in the case of a linear kernel.\n",
      "     |  \n",
      "     |      `coef_` is readonly property derived from `dual_coef_` and\n",
      "     |      `support_vectors_`\n",
      "     |  \n",
      "     |  intercept_ : array, shape = [1,]\n",
      "     |      Constant in the decision function.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OneClassSVM\n",
      "     |      sklearn.svm.base.BaseLibSVM\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, kernel='rbf', degree=3, gamma='auto', coef0=0.0, tol=0.001, nu=0.5, shrinking=True, cache_size=200, verbose=False, max_iter=-1, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Signed distance to the separating hyperplane.\n",
      "     |      \n",
      "     |      Signed distance is positive for an inlier and negative for an outlier.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X : array-like, shape (n_samples,)\n",
      "     |          Returns the decision function of the samples.\n",
      "     |  \n",
      "     |  fit(self, X, y=None, sample_weight=None, **params)\n",
      "     |      Detects the soft boundary of the set of samples X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Set of samples, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,)\n",
      "     |          Per-sample weights. Rescale C per sample. Higher weights\n",
      "     |          force the classifier to put more emphasis on these points.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      If X is not a C-ordered contiguous array it is copied.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Perform classification on samples in X.\n",
      "     |      \n",
      "     |      For an one-class model, +1 or -1 is returned.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          [n_samples_test, n_samples_train]\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : array, shape (n_samples,)\n",
      "     |          Class labels for samples in X.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.svm.base.BaseLibSVM:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class SVC(sklearn.svm.base.BaseSVC)\n",
      "     |  C-Support Vector Classification.\n",
      "     |  \n",
      "     |  The implementation is based on libsvm. The fit time complexity\n",
      "     |  is more than quadratic with the number of samples which makes it hard\n",
      "     |  to scale to dataset with more than a couple of 10000 samples.\n",
      "     |  \n",
      "     |  The multiclass support is handled according to a one-vs-one scheme.\n",
      "     |  \n",
      "     |  For details on the precise mathematical formulation of the provided\n",
      "     |  kernel functions and how `gamma`, `coef0` and `degree` affect each\n",
      "     |  other, see the corresponding section in the narrative documentation:\n",
      "     |  :ref:`svm_kernels`.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <svm_classification>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  C : float, optional (default=1.0)\n",
      "     |      Penalty parameter C of the error term.\n",
      "     |  \n",
      "     |  kernel : string, optional (default='rbf')\n",
      "     |       Specifies the kernel type to be used in the algorithm.\n",
      "     |       It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n",
      "     |       a callable.\n",
      "     |       If none is given, 'rbf' will be used. If a callable is given it is\n",
      "     |       used to pre-compute the kernel matrix from data matrices; that matrix\n",
      "     |       should be an array of shape ``(n_samples, n_samples)``.\n",
      "     |  \n",
      "     |  degree : int, optional (default=3)\n",
      "     |      Degree of the polynomial kernel function ('poly').\n",
      "     |      Ignored by all other kernels.\n",
      "     |  \n",
      "     |  gamma : float, optional (default='auto')\n",
      "     |      Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n",
      "     |      If gamma is 'auto' then 1/n_features will be used instead.\n",
      "     |  \n",
      "     |  coef0 : float, optional (default=0.0)\n",
      "     |      Independent term in kernel function.\n",
      "     |      It is only significant in 'poly' and 'sigmoid'.\n",
      "     |  \n",
      "     |  probability : boolean, optional (default=False)\n",
      "     |      Whether to enable probability estimates. This must be enabled prior\n",
      "     |      to calling `fit`, and will slow down that method.\n",
      "     |  \n",
      "     |  shrinking : boolean, optional (default=True)\n",
      "     |      Whether to use the shrinking heuristic.\n",
      "     |  \n",
      "     |  tol : float, optional (default=1e-3)\n",
      "     |      Tolerance for stopping criterion.\n",
      "     |  \n",
      "     |  cache_size : float, optional\n",
      "     |      Specify the size of the kernel cache (in MB).\n",
      "     |  \n",
      "     |  class_weight : {dict, 'balanced'}, optional\n",
      "     |      Set the parameter C of class i to class_weight[i]*C for\n",
      "     |      SVC. If not given, all classes are supposed to have\n",
      "     |      weight one.\n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      "     |  \n",
      "     |  verbose : bool, default: False\n",
      "     |      Enable verbose output. Note that this setting takes advantage of a\n",
      "     |      per-process runtime setting in libsvm that, if enabled, may not work\n",
      "     |      properly in a multithreaded context.\n",
      "     |  \n",
      "     |  max_iter : int, optional (default=-1)\n",
      "     |      Hard limit on iterations within solver, or -1 for no limit.\n",
      "     |  \n",
      "     |  decision_function_shape : 'ovo', 'ovr', default='ovr'\n",
      "     |      Whether to return a one-vs-rest ('ovr') decision function of shape\n",
      "     |      (n_samples, n_classes) as all other classifiers, or the original\n",
      "     |      one-vs-one ('ovo') decision function of libsvm which has shape\n",
      "     |      (n_samples, n_classes * (n_classes - 1) / 2).\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.19\n",
      "     |          decision_function_shape is 'ovr' by default.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         *decision_function_shape='ovr'* is recommended.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.17\n",
      "     |         Deprecated *decision_function_shape='ovo' and None*.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
      "     |      The seed of the pseudo random number generator to use when shuffling\n",
      "     |      the data.  If int, random_state is the seed used by the random number\n",
      "     |      generator; If RandomState instance, random_state is the random number\n",
      "     |      generator; If None, the random number generator is the RandomState\n",
      "     |      instance used by `np.random`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  support_ : array-like, shape = [n_SV]\n",
      "     |      Indices of support vectors.\n",
      "     |  \n",
      "     |  support_vectors_ : array-like, shape = [n_SV, n_features]\n",
      "     |      Support vectors.\n",
      "     |  \n",
      "     |  n_support_ : array-like, dtype=int32, shape = [n_class]\n",
      "     |      Number of support vectors for each class.\n",
      "     |  \n",
      "     |  dual_coef_ : array, shape = [n_class-1, n_SV]\n",
      "     |      Coefficients of the support vector in the decision function.\n",
      "     |      For multiclass, coefficient for all 1-vs-1 classifiers.\n",
      "     |      The layout of the coefficients in the multiclass case is somewhat\n",
      "     |      non-trivial. See the section about multi-class classification in the\n",
      "     |      SVM section of the User Guide for details.\n",
      "     |  \n",
      "     |  coef_ : array, shape = [n_class-1, n_features]\n",
      "     |      Weights assigned to the features (coefficients in the primal\n",
      "     |      problem). This is only available in the case of a linear kernel.\n",
      "     |  \n",
      "     |      `coef_` is a readonly property derived from `dual_coef_` and\n",
      "     |      `support_vectors_`.\n",
      "     |  \n",
      "     |  intercept_ : array, shape = [n_class * (n_class-1) / 2]\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
      "     |  >>> y = np.array([1, 1, 2, 2])\n",
      "     |  >>> from sklearn.svm import SVC\n",
      "     |  >>> clf = SVC()\n",
      "     |  >>> clf.fit(X, y) #doctest: +NORMALIZE_WHITESPACE\n",
      "     |  SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "     |      decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "     |      max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "     |      tol=0.001, verbose=False)\n",
      "     |  >>> print(clf.predict([[-0.8, -1]]))\n",
      "     |  [1]\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  SVR\n",
      "     |      Support Vector Machine for Regression implemented using libsvm.\n",
      "     |  \n",
      "     |  LinearSVC\n",
      "     |      Scalable Linear Support Vector Machine for classification\n",
      "     |      implemented using liblinear. Check the See also section of\n",
      "     |      LinearSVC for more comparison element.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SVC\n",
      "     |      sklearn.svm.base.BaseSVC\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.svm.base.BaseLibSVM\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, C=1.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.svm.base.BaseSVC:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Distance of the samples X to the separating hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X : array-like, shape (n_samples, n_classes * (n_classes-1) / 2)\n",
      "     |          Returns the decision function of the sample for each class\n",
      "     |          in the model.\n",
      "     |          If decision_function_shape='ovr', the shape is (n_samples,\n",
      "     |          n_classes)\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Perform classification on samples in X.\n",
      "     |      \n",
      "     |      For an one-class model, +1 or -1 is returned.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          [n_samples_test, n_samples_train]\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : array, shape (n_samples,)\n",
      "     |          Class labels for samples in X.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.svm.base.BaseSVC:\n",
      "     |  \n",
      "     |  predict_log_proba\n",
      "     |      Compute log probabilities of possible outcomes for samples in X.\n",
      "     |      \n",
      "     |      The model need to have probability information computed at training\n",
      "     |      time: fit with attribute `probability` set to True.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          [n_samples_test, n_samples_train]\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : array-like, shape (n_samples, n_classes)\n",
      "     |          Returns the log-probabilities of the sample for each class in\n",
      "     |          the model. The columns correspond to the classes in sorted\n",
      "     |          order, as they appear in the attribute `classes_`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The probability model is created using cross validation, so\n",
      "     |      the results can be slightly different than those obtained by\n",
      "     |      predict. Also, it will produce meaningless results on very small\n",
      "     |      datasets.\n",
      "     |  \n",
      "     |  predict_proba\n",
      "     |      Compute probabilities of possible outcomes for samples in X.\n",
      "     |      \n",
      "     |      The model need to have probability information computed at training\n",
      "     |      time: fit with attribute `probability` set to True.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          [n_samples_test, n_samples_train]\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : array-like, shape (n_samples, n_classes)\n",
      "     |          Returns the probability of the sample for each class in\n",
      "     |          the model. The columns correspond to the classes in sorted\n",
      "     |          order, as they appear in the attribute `classes_`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The probability model is created using cross validation, so\n",
      "     |      the results can be slightly different than those obtained by\n",
      "     |      predict. Also, it will produce meaningless results on very small\n",
      "     |      datasets.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.svm.base.BaseLibSVM:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the SVM model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training vectors, where n_samples is the number of samples\n",
      "     |          and n_features is the number of features.\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          (n_samples, n_samples).\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,)\n",
      "     |          Target values (class labels in classification, real numbers in\n",
      "     |          regression)\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,)\n",
      "     |          Per-sample weights. Rescale C per sample. Higher weights\n",
      "     |          force the classifier to put more emphasis on these points.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      ------\n",
      "     |      If X and y are not C-ordered and contiguous arrays of np.float64 and\n",
      "     |      X is not a scipy.sparse.csr_matrix, X and/or y may be copied.\n",
      "     |      \n",
      "     |      If X is a dense array, then the other methods will not support sparse\n",
      "     |      matrices as input.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.svm.base.BaseLibSVM:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "    \n",
      "    class SVR(sklearn.svm.base.BaseLibSVM, sklearn.base.RegressorMixin)\n",
      "     |  Epsilon-Support Vector Regression.\n",
      "     |  \n",
      "     |  The free parameters in the model are C and epsilon.\n",
      "     |  \n",
      "     |  The implementation is based on libsvm.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <svm_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  C : float, optional (default=1.0)\n",
      "     |      Penalty parameter C of the error term.\n",
      "     |  \n",
      "     |  epsilon : float, optional (default=0.1)\n",
      "     |       Epsilon in the epsilon-SVR model. It specifies the epsilon-tube\n",
      "     |       within which no penalty is associated in the training loss function\n",
      "     |       with points predicted within a distance epsilon from the actual\n",
      "     |       value.\n",
      "     |  \n",
      "     |  kernel : string, optional (default='rbf')\n",
      "     |       Specifies the kernel type to be used in the algorithm.\n",
      "     |       It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n",
      "     |       a callable.\n",
      "     |       If none is given, 'rbf' will be used. If a callable is given it is\n",
      "     |       used to precompute the kernel matrix.\n",
      "     |  \n",
      "     |  degree : int, optional (default=3)\n",
      "     |      Degree of the polynomial kernel function ('poly').\n",
      "     |      Ignored by all other kernels.\n",
      "     |  \n",
      "     |  gamma : float, optional (default='auto')\n",
      "     |      Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n",
      "     |      If gamma is 'auto' then 1/n_features will be used instead.\n",
      "     |  \n",
      "     |  coef0 : float, optional (default=0.0)\n",
      "     |      Independent term in kernel function.\n",
      "     |      It is only significant in 'poly' and 'sigmoid'.\n",
      "     |  \n",
      "     |  shrinking : boolean, optional (default=True)\n",
      "     |      Whether to use the shrinking heuristic.\n",
      "     |  \n",
      "     |  tol : float, optional (default=1e-3)\n",
      "     |      Tolerance for stopping criterion.\n",
      "     |  \n",
      "     |  cache_size : float, optional\n",
      "     |      Specify the size of the kernel cache (in MB).\n",
      "     |  \n",
      "     |  verbose : bool, default: False\n",
      "     |      Enable verbose output. Note that this setting takes advantage of a\n",
      "     |      per-process runtime setting in libsvm that, if enabled, may not work\n",
      "     |      properly in a multithreaded context.\n",
      "     |  \n",
      "     |  max_iter : int, optional (default=-1)\n",
      "     |      Hard limit on iterations within solver, or -1 for no limit.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  support_ : array-like, shape = [n_SV]\n",
      "     |      Indices of support vectors.\n",
      "     |  \n",
      "     |  support_vectors_ : array-like, shape = [nSV, n_features]\n",
      "     |      Support vectors.\n",
      "     |  \n",
      "     |  dual_coef_ : array, shape = [1, n_SV]\n",
      "     |      Coefficients of the support vector in the decision function.\n",
      "     |  \n",
      "     |  coef_ : array, shape = [1, n_features]\n",
      "     |      Weights assigned to the features (coefficients in the primal\n",
      "     |      problem). This is only available in the case of a linear kernel.\n",
      "     |  \n",
      "     |      `coef_` is readonly property derived from `dual_coef_` and\n",
      "     |      `support_vectors_`.\n",
      "     |  \n",
      "     |  intercept_ : array, shape = [1]\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  sample_weight : array-like, shape = [n_samples]\n",
      "     |          Individual weights for each sample\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.svm import SVR\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> n_samples, n_features = 10, 5\n",
      "     |  >>> np.random.seed(0)\n",
      "     |  >>> y = np.random.randn(n_samples)\n",
      "     |  >>> X = np.random.randn(n_samples, n_features)\n",
      "     |  >>> clf = SVR(C=1.0, epsilon=0.2)\n",
      "     |  >>> clf.fit(X, y) #doctest: +NORMALIZE_WHITESPACE\n",
      "     |  SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.2, gamma='auto',\n",
      "     |      kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  NuSVR\n",
      "     |      Support Vector Machine for regression implemented using libsvm\n",
      "     |      using a parameter to control the number of support vectors.\n",
      "     |  \n",
      "     |  LinearSVR\n",
      "     |      Scalable Linear Support Vector Machine for regression\n",
      "     |      implemented using liblinear.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SVR\n",
      "     |      sklearn.svm.base.BaseLibSVM\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, kernel='rbf', degree=3, gamma='auto', coef0=0.0, tol=0.001, C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.svm.base.BaseLibSVM:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the SVM model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training vectors, where n_samples is the number of samples\n",
      "     |          and n_features is the number of features.\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          (n_samples, n_samples).\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,)\n",
      "     |          Target values (class labels in classification, real numbers in\n",
      "     |          regression)\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,)\n",
      "     |          Per-sample weights. Rescale C per sample. Higher weights\n",
      "     |          force the classifier to put more emphasis on these points.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      ------\n",
      "     |      If X and y are not C-ordered and contiguous arrays of np.float64 and\n",
      "     |      X is not a scipy.sparse.csr_matrix, X and/or y may be copied.\n",
      "     |      \n",
      "     |      If X is a dense array, then the other methods will not support sparse\n",
      "     |      matrices as input.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Perform regression on samples in X.\n",
      "     |      \n",
      "     |      For an one-class model, +1 (inlier) or -1 (outlier) is returned.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          For kernel=\"precomputed\", the expected shape of X is\n",
      "     |          (n_samples_test, n_samples_train).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : array, shape (n_samples,)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.svm.base.BaseLibSVM:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "\n",
      "FUNCTIONS\n",
      "    l1_min_c(X, y, loss='squared_hinge', fit_intercept=True, intercept_scaling=1.0)\n",
      "        Return the lowest bound for C such that for C in (l1_min_C, infinity)\n",
      "        the model is guaranteed not to be empty. This applies to l1 penalized\n",
      "        classifiers, such as LinearSVC with penalty='l1' and\n",
      "        linear_model.LogisticRegression with penalty='l1'.\n",
      "        \n",
      "        This value is valid if class_weight parameter in fit() is not set.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      "            Training vector, where n_samples in the number of samples and\n",
      "            n_features is the number of features.\n",
      "        \n",
      "        y : array, shape = [n_samples]\n",
      "            Target vector relative to X\n",
      "        \n",
      "        loss : {'squared_hinge', 'log'}, default 'squared_hinge'\n",
      "            Specifies the loss function.\n",
      "            With 'squared_hinge' it is the squared hinge loss (a.k.a. L2 loss).\n",
      "            With 'log' it is the loss of logistic regression models.\n",
      "            'l2' is accepted as an alias for 'squared_hinge', for backward\n",
      "            compatibility reasons, but should not be used in new code.\n",
      "        \n",
      "        fit_intercept : bool, default: True\n",
      "            Specifies if the intercept should be fitted by the model.\n",
      "            It must match the fit() method parameter.\n",
      "        \n",
      "        intercept_scaling : float, default: 1\n",
      "            when fit_intercept is True, instance vector x becomes\n",
      "            [x, intercept_scaling],\n",
      "            i.e. a \"synthetic\" feature with constant value equals to\n",
      "            intercept_scaling is appended to the instance vector.\n",
      "            It must match the fit() method parameter.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        l1_min_c : float\n",
      "            minimum value for C\n",
      "\n",
      "DATA\n",
      "    __all__ = ['LinearSVC', 'LinearSVR', 'NuSVC', 'NuSVR', 'OneClassSVM', ...\n",
      "\n",
      "FILE\n",
      "    /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sl.svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115e6f438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "svc=sl.svm.SVC()\n",
    "svc.fit(X_train_std,y_train)\n",
    "plot_decision_regions(X=X_combined_std,\n",
    "                      y=y_combined,\n",
    "                      classifier=svc,\n",
    "                      test_idx=range(105,150))\n",
    "plt.xlabel('petal length(standardized)')\n",
    "plt.ylabel('petal width(standardized)')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "y_pred=svc.predict(X_test_std)\n",
    "print((y_pred-y_test!=0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SVC in module sklearn.svm.classes:\n",
      "\n",
      "class SVC(sklearn.svm.base.BaseSVC)\n",
      " |  C-Support Vector Classification.\n",
      " |  \n",
      " |  The implementation is based on libsvm. The fit time complexity\n",
      " |  is more than quadratic with the number of samples which makes it hard\n",
      " |  to scale to dataset with more than a couple of 10000 samples.\n",
      " |  \n",
      " |  The multiclass support is handled according to a one-vs-one scheme.\n",
      " |  \n",
      " |  For details on the precise mathematical formulation of the provided\n",
      " |  kernel functions and how `gamma`, `coef0` and `degree` affect each\n",
      " |  other, see the corresponding section in the narrative documentation:\n",
      " |  :ref:`svm_kernels`.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <svm_classification>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  C : float, optional (default=1.0)\n",
      " |      Penalty parameter C of the error term.\n",
      " |  \n",
      " |  kernel : string, optional (default='rbf')\n",
      " |       Specifies the kernel type to be used in the algorithm.\n",
      " |       It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n",
      " |       a callable.\n",
      " |       If none is given, 'rbf' will be used. If a callable is given it is\n",
      " |       used to pre-compute the kernel matrix from data matrices; that matrix\n",
      " |       should be an array of shape ``(n_samples, n_samples)``.\n",
      " |  \n",
      " |  degree : int, optional (default=3)\n",
      " |      Degree of the polynomial kernel function ('poly').\n",
      " |      Ignored by all other kernels.\n",
      " |  \n",
      " |  gamma : float, optional (default='auto')\n",
      " |      Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n",
      " |      If gamma is 'auto' then 1/n_features will be used instead.\n",
      " |  \n",
      " |  coef0 : float, optional (default=0.0)\n",
      " |      Independent term in kernel function.\n",
      " |      It is only significant in 'poly' and 'sigmoid'.\n",
      " |  \n",
      " |  probability : boolean, optional (default=False)\n",
      " |      Whether to enable probability estimates. This must be enabled prior\n",
      " |      to calling `fit`, and will slow down that method.\n",
      " |  \n",
      " |  shrinking : boolean, optional (default=True)\n",
      " |      Whether to use the shrinking heuristic.\n",
      " |  \n",
      " |  tol : float, optional (default=1e-3)\n",
      " |      Tolerance for stopping criterion.\n",
      " |  \n",
      " |  cache_size : float, optional\n",
      " |      Specify the size of the kernel cache (in MB).\n",
      " |  \n",
      " |  class_weight : {dict, 'balanced'}, optional\n",
      " |      Set the parameter C of class i to class_weight[i]*C for\n",
      " |      SVC. If not given, all classes are supposed to have\n",
      " |      weight one.\n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      " |  \n",
      " |  verbose : bool, default: False\n",
      " |      Enable verbose output. Note that this setting takes advantage of a\n",
      " |      per-process runtime setting in libsvm that, if enabled, may not work\n",
      " |      properly in a multithreaded context.\n",
      " |  \n",
      " |  max_iter : int, optional (default=-1)\n",
      " |      Hard limit on iterations within solver, or -1 for no limit.\n",
      " |  \n",
      " |  decision_function_shape : 'ovo', 'ovr', default='ovr'\n",
      " |      Whether to return a one-vs-rest ('ovr') decision function of shape\n",
      " |      (n_samples, n_classes) as all other classifiers, or the original\n",
      " |      one-vs-one ('ovo') decision function of libsvm which has shape\n",
      " |      (n_samples, n_classes * (n_classes - 1) / 2).\n",
      " |  \n",
      " |      .. versionchanged:: 0.19\n",
      " |          decision_function_shape is 'ovr' by default.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *decision_function_shape='ovr'* is recommended.\n",
      " |  \n",
      " |      .. versionchanged:: 0.17\n",
      " |         Deprecated *decision_function_shape='ovo' and None*.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional (default=None)\n",
      " |      The seed of the pseudo random number generator to use when shuffling\n",
      " |      the data.  If int, random_state is the seed used by the random number\n",
      " |      generator; If RandomState instance, random_state is the random number\n",
      " |      generator; If None, the random number generator is the RandomState\n",
      " |      instance used by `np.random`.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  support_ : array-like, shape = [n_SV]\n",
      " |      Indices of support vectors.\n",
      " |  \n",
      " |  support_vectors_ : array-like, shape = [n_SV, n_features]\n",
      " |      Support vectors.\n",
      " |  \n",
      " |  n_support_ : array-like, dtype=int32, shape = [n_class]\n",
      " |      Number of support vectors for each class.\n",
      " |  \n",
      " |  dual_coef_ : array, shape = [n_class-1, n_SV]\n",
      " |      Coefficients of the support vector in the decision function.\n",
      " |      For multiclass, coefficient for all 1-vs-1 classifiers.\n",
      " |      The layout of the coefficients in the multiclass case is somewhat\n",
      " |      non-trivial. See the section about multi-class classification in the\n",
      " |      SVM section of the User Guide for details.\n",
      " |  \n",
      " |  coef_ : array, shape = [n_class-1, n_features]\n",
      " |      Weights assigned to the features (coefficients in the primal\n",
      " |      problem). This is only available in the case of a linear kernel.\n",
      " |  \n",
      " |      `coef_` is a readonly property derived from `dual_coef_` and\n",
      " |      `support_vectors_`.\n",
      " |  \n",
      " |  intercept_ : array, shape = [n_class * (n_class-1) / 2]\n",
      " |      Constants in decision function.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
      " |  >>> y = np.array([1, 1, 2, 2])\n",
      " |  >>> from sklearn.svm import SVC\n",
      " |  >>> clf = SVC()\n",
      " |  >>> clf.fit(X, y) #doctest: +NORMALIZE_WHITESPACE\n",
      " |  SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      " |      decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      " |      max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      " |      tol=0.001, verbose=False)\n",
      " |  >>> print(clf.predict([[-0.8, -1]]))\n",
      " |  [1]\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  SVR\n",
      " |      Support Vector Machine for Regression implemented using libsvm.\n",
      " |  \n",
      " |  LinearSVC\n",
      " |      Scalable Linear Support Vector Machine for classification\n",
      " |      implemented using liblinear. Check the See also section of\n",
      " |      LinearSVC for more comparison element.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SVC\n",
      " |      sklearn.svm.base.BaseSVC\n",
      " |      abc.NewBase\n",
      " |      sklearn.svm.base.BaseLibSVM\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, C=1.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', random_state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.svm.base.BaseSVC:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Distance of the samples X to the separating hyperplane.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : array-like, shape (n_samples, n_classes * (n_classes-1) / 2)\n",
      " |          Returns the decision function of the sample for each class\n",
      " |          in the model.\n",
      " |          If decision_function_shape='ovr', the shape is (n_samples,\n",
      " |          n_classes)\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Perform classification on samples in X.\n",
      " |      \n",
      " |      For an one-class model, +1 or -1 is returned.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          For kernel=\"precomputed\", the expected shape of X is\n",
      " |          [n_samples_test, n_samples_train]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_pred : array, shape (n_samples,)\n",
      " |          Class labels for samples in X.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.svm.base.BaseSVC:\n",
      " |  \n",
      " |  predict_log_proba\n",
      " |      Compute log probabilities of possible outcomes for samples in X.\n",
      " |      \n",
      " |      The model need to have probability information computed at training\n",
      " |      time: fit with attribute `probability` set to True.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          For kernel=\"precomputed\", the expected shape of X is\n",
      " |          [n_samples_test, n_samples_train]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like, shape (n_samples, n_classes)\n",
      " |          Returns the log-probabilities of the sample for each class in\n",
      " |          the model. The columns correspond to the classes in sorted\n",
      " |          order, as they appear in the attribute `classes_`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The probability model is created using cross validation, so\n",
      " |      the results can be slightly different than those obtained by\n",
      " |      predict. Also, it will produce meaningless results on very small\n",
      " |      datasets.\n",
      " |  \n",
      " |  predict_proba\n",
      " |      Compute probabilities of possible outcomes for samples in X.\n",
      " |      \n",
      " |      The model need to have probability information computed at training\n",
      " |      time: fit with attribute `probability` set to True.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          For kernel=\"precomputed\", the expected shape of X is\n",
      " |          [n_samples_test, n_samples_train]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like, shape (n_samples, n_classes)\n",
      " |          Returns the probability of the sample for each class in\n",
      " |          the model. The columns correspond to the classes in sorted\n",
      " |          order, as they appear in the attribute `classes_`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The probability model is created using cross validation, so\n",
      " |      the results can be slightly different than those obtained by\n",
      " |      predict. Also, it will produce meaningless results on very small\n",
      " |      datasets.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.svm.base.BaseLibSVM:\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit the SVM model according to the given training data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          Training vectors, where n_samples is the number of samples\n",
      " |          and n_features is the number of features.\n",
      " |          For kernel=\"precomputed\", the expected shape of X is\n",
      " |          (n_samples, n_samples).\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,)\n",
      " |          Target values (class labels in classification, real numbers in\n",
      " |          regression)\n",
      " |      \n",
      " |      sample_weight : array-like, shape (n_samples,)\n",
      " |          Per-sample weights. Rescale C per sample. Higher weights\n",
      " |          force the classifier to put more emphasis on these points.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns self.\n",
      " |      \n",
      " |      Notes\n",
      " |      ------\n",
      " |      If X and y are not C-ordered and contiguous arrays of np.float64 and\n",
      " |      X is not a scipy.sparse.csr_matrix, X and/or y may be copied.\n",
      " |      \n",
      " |      If X is a dense array, then the other methods will not support sparse\n",
      " |      matrices as input.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.svm.base.BaseLibSVM:\n",
      " |  \n",
      " |  coef_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sl.svm.SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118bcbc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Miss predicted number:  1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115efccf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Miss predicted number:  11\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118c4be80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Miss predicted number:  1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1162896a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Miss predicted number:  1\n"
     ]
    }
   ],
   "source": [
    "kernels=['linear', 'poly', 'rbf', 'sigmoid', ]#'precomputed'\n",
    "for kn in kernels:\n",
    "    svc=sl.svm.SVC(kernel=kn,degree=10)\n",
    "    svc.fit(X_train_std,y_train)\n",
    "    plot_decision_regions(X=X_combined_std,\n",
    "                          y=y_combined,\n",
    "                          classifier=svc,\n",
    "                          test_idx=range(105,150))\n",
    "    plt.xlabel('petal length(standardized)')\n",
    "    plt.ylabel('petal width(standardized)')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(\"SVM result when kernel is %s\"%kn)\n",
    "    plt.show()\n",
    "    y_pred=svc.predict(X_test_std)\n",
    "    print(\"Miss predicted number: \", (y_pred-y_test!=0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 24,  37,  65,  66,  75, 100,   0,   7,  13,  15,  21,  29,  32,\n",
       "        34,  63,  64,  70,  72,  84,  87,  88,  90,  94,  96,  97,   1,\n",
       "         3,   4,   9,  10,  11,  12,  17,  25,  31,  53,  74,  92, 101], dtype=int32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.support_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
